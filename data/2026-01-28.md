<div id=toc></div>

# Table of Contents

- [physics.flu-dyn](#physics.flu-dyn) [Total: 8]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.LG](#cs.LG) [Total: 73]
- [math.CV](#math.CV) [Total: 1]


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [1] [A theoretical model for oceanic submesoscales under next-order effects of strain and turbulence](https://arxiv.org/abs/2601.18962)
*Shirui Peng,Abigail Bodner*

Main category: physics.flu-dyn

TL;DR: 该研究通过微扰分析探讨了中尺度应变和边界层湍流共同作用下，有限罗斯贝数对亚中尺度锋面和丝状结构动力学的影响，揭示了湍流通量如何调节应变诱导的前锋生成过程。


<details>
  <summary>Details</summary>
Motivation: 亚中尺度流在海洋混合层中具有重要作用，但其在有限罗斯贝数条件下，受中尺度应变和边界层湍流共同作用的动力学修正机制尚未解决。需要理解这些相互作用如何影响锋面演化，以改进混合层参数化方案。

Method: 采用二维、地转调整的海洋锋面和丝状结构的微扰分析方法，基于Shakespeare和Bodner的解析模型框架。系统探索了罗斯贝数、埃克曼数和应变参数的广泛范围，分别分析了纯中尺度应变、边界层湍流扰动以及两者共同作用的情况。

Result: 纯中尺度应变的一阶解显示明显的前锋生成，与早期惯性周期内的完整模型动力学高度一致。边界层湍流扰动下，涡粘性和涡扩散性分别对应前锋生成和前消解趋势，但在参数空间内未观察到机制转换。两者共同作用时，湍流通量可增强、减弱甚至逆转应变诱导的前锋生成，具体取决于参数状态。

Conclusion: 混合层参数化必须仔细考虑边界层湍流在亚中尺度流中的空间变异性，才能准确捕捉中尺度应变下的锋面演化。湍流通量对前锋生成的调节作用具有参数依赖性，这对海洋模型参数化具有重要意义。

Abstract: Submesoscale currents in the oceanic mixed layer, comprising fronts, eddies, and filaments, are characterized by $\textit{O}(1)$ Rossby numbers (Ro). These features, which constantly interact with background mesoscale flows and boundary layer turbulence (BLT), are critical for mediating vertical exchange between the surface and the ocean interior. Despite growing insight into their generation and evolution, the modification of initially balanced submesoscale dynamics by finite-Ro effects under the combined influence of mesoscale strain and BLT remains unresolved. In this study, we address this question through a perturbation analysis of two-dimensional, geostrophically adjusted oceanic fronts and filaments, adapting the analytical models of \citet{shakespeare_generalized_2013} and \citet{bodner_breakdown_2020}. This framework allows for a systematic exploration across a broad range of Rossby numbers Ro, Ekman numbers Ek, and strain parameters. The first-order solution under pure mesoscale strain exhibits clear frontogenesis and closely mirrors the full model dynamics during early inertial periods, despite the absence of an exponential collapse. Under BLT perturbation, the first-order solution confirms the distinct frontogenetic and frontolytic tendencies associated with eddy viscosity and diffusivity, respectively; however, no transition between these regimes is observed across the explored Ro and Ek parameter space for vertical mixing. When both strain and BLT perturbations are present, turbulent fluxes can strengthen, weaken, or even reverse strain-induced frontogenesis depending on the parameter regime. These results suggest that mixed-layer parameterizations must carefully account for the spatial variability of BLT within submesoscale currents to accurately capture frontal evolution under mesoscale strain.

</details>


### [2] [A new form of mixing in turbulent sedimentation](https://arxiv.org/abs/2601.19212)
*Simone Tandurella,Marco Edoardo Rosti,Stefano Musacchio,Guido Boffetta*

Main category: physics.flu-dyn

TL;DR: 通过直接数值模拟研究有限尺寸惯性粒子在类Rayleigh-Taylor设置中的沉降过程，发现粒子沉降产生两个不同区域：前端线性浓度分布的混合层和后方均匀密度的主体区域，混合层扩展呈现异常加速指数。


<details>
  <summary>Details</summary>
Motivation: 研究粒子悬浮液在沉降过程中前端的对流混合新机制，探索与传统Rayleigh-Taylor湍流不同的沉降动力学行为。

Method: 采用最先进的直接数值模拟方法，研究有限尺寸惯性粒子在类Rayleigh-Taylor设置中的沉降过程，并建立基于局部阻滞沉降定律的一维模型。

Result: 观察到沉降粒子产生两个不同区域：前端具有线性浓度分布的混合层和后方均匀密度的主体区域。混合层扩展呈现异常的非整数指数加速，而主体区域以恒定速度运动。一维模型能准确捕捉观测到的动力学及其对粒子-流体密度比的依赖性。

Conclusion: 本研究识别出粒子悬浮液在沉降过程中前端发展的对流混合新机制，为理解有限尺寸粒子沉降动力学提供了新见解。

Abstract: We study the sedimentation of finite-size inertial particles in a Rayleigh-Taylor-like setup using state-of-the-art direct numerical simulations. The falling particles are observed to produce two distinct regions: a leading mixing layer with a linear concentration profile followed by a bulk region of uniform density. Unlike classical RT turbulence, the mixing layer extension accelerates with an anomalous, non-integer exponent, while the bulk region moves at a constant velocity. A one-dimensional model based on a local hindered settling law accurately captures the observed dynamics and its dependence on the particle-to-fluid density ratio. The present work identifies a new regime of convective mixing which develops at the front of particle suspensions in sedimentation processes.

</details>


### [3] [Physics-Informed Transformer operator for the prediction of three-dimensional turbulence](https://arxiv.org/abs/2601.19351)
*Zhihong Guo*

Main category: physics.flu-dyn

TL;DR: 提出基于视觉Transformer的物理信息Transformer算子(PITO)及其隐式变体(PIITO)，用于三维湍流预测，无需标签数据，能自动学习亚格子尺度系数，在长期外推预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的湍流预测方法面临数据依赖性强和缺乏物理解释性的挑战，需要开发既能保持物理一致性又减少数据依赖的预测方法。

Method: 基于视觉Transformer架构开发PITO和PIITO，通过将大涡模拟方程嵌入损失函数实现无标签学习，能自动学习亚格子尺度系数，使用适当补丁尺寸处理三维湍流场。

Result: 在衰减均匀各向同性湍流中，PITO和PIITO在超过训练时域25倍的长期外推预测中表现出优异稳定性和准确性，优于物理信息傅里叶神经算子；在强制湍流中PITO表现突出而PIFNO失败；相比PIFNO，GPU内存消耗分别减少79.5%和91.3%，参数仅需31.5%和3.1%；比传统LES方法快得多。

Conclusion: PITO和PIITO是高效、准确且物理一致的湍流预测方法，显著减少计算资源需求，在长期外推预测中表现优异，为数据驱动的湍流建模提供了新方向。

Abstract: Data-driven turbulence prediction methods often face challenges related to data dependency and lack of physical interpretability. In this paper, we propose a physics-informed Transformer operator (PITO) and its implicit variant (PIITO) for predicting three-dimensional (3D) turbulence, which are developed based on the vision Transformer (ViT) architecture with an appropriate patch size. Given the current flow field, the Transformer operator computes its prediction for the next time step. By embedding the large-eddy simulation (LES) equations into the loss function, PITO and PIITO can learn solution operators without using labeled data. Furthermore, PITO can automatically learn the subgrid scale (SGS) coefficient using a single set of flow data during training. Both PITO and PIITO exhibit excellent stability and accuracy on the predictions of various statistical properties and flow structures for the situation of long-term extrapolation exceeding 25 times the training horizon in decaying homogeneous isotropic turbulence (HIT), and outperform the physics-informed Fourier neural operator (PIFNO). Furthermore, PITO exhibits a remarkable accuracy on the predictions of forced HIT where PIFNO fails. Notably, PITO and PIITO reduce GPU memory consumption by 79.5\% and 91.3\% while requiring only 31.5\% and 3.1\% of the parameters, respectively, compared to PIFNO. Moreover, both PITO and PIITO models are much faster compared to traditional LES method.

</details>


### [4] [Inertial effects on the interphase drag force and rheology of dilute suspensions of buoyant droplets at low Reynolds number](https://arxiv.org/abs/2601.19377)
*Nicolas Fintzi,Jean-Lou Pierson*

Main category: physics.flu-dyn

TL;DR: 使用互易定理计算低雷诺数下球形液滴在均匀流中的流体动力、力的一阶和二阶矩，分析惯性修正和速度方差对有效应力的影响


<details>
  <summary>Details</summary>
Motivation: 研究低雷诺数稀乳液系统中，惯性效应和液滴速度分布对流体动力和有效应力的影响，为多相流建模提供理论依据

Method: 采用互易定理计算球形液滴在均匀流中的流体动力，考虑低但有限雷诺数（Re）和低体积分数（φ）条件，分析力的一阶和二阶矩的惯性修正

Result: 力的一阶矩惯性修正为O(ρ_f φU²)，二阶矩修正为O(aρ_fφU²)；液滴速度分布的系综平均引入与分散相速度方差成正比的附加项，影响相间动量交换和连续相有效应力

Conclusion: 在稀浮力液滴乳液中，有效应力与相间相对速度、分散相速度方差及其空间梯度的平方相关，为多相流本构关系提供新的理论框架

Abstract: In this work, we compute the hydrodynamic force and the first and second moments of force acting on a translating spherical droplet immersed in a uniform flow using the reciprocal theorem. We consider the low but finite Reynolds number regime, $Re = a U ρ_f / μ_f$, and the dilute limit of small droplet volume fraction $φ$. Here, $U$ denotes the magnitude of the relative velocity between the phases, $a$ the droplet radius, and $ρ_f$ and $μ_f$ the density and viscosity of the continuous phase, respectively. We show that the $O(Re)$ inertial corrections to the first and second moments of force scale as $O(ρ_f φU^2)$ and $O(aρ_fφU^2)$, respectively. Moreover, the ensemble average of the drag force and the higher-order force moments over the distribution of droplet velocities introduces additional contributions proportional to the velocity variance of the dispersed phase, both in the interphase momentum exchange and in the effective stress of the continuous phase. As a consequence, in dilute emulsions of buoyant droplets, the effective stress depends quadratically on the relative velocity between the phases, on the velocity variance of the dispersed phase, and on the spatial gradients of these quantities.

</details>


### [5] [Development of strongly nonlinear structures at the charged boundary of a non-conducting liquid in an electric field](https://arxiv.org/abs/2601.19468)
*N. M. Zubarev,E. A. Kochurin*

Main category: physics.flu-dyn

TL;DR: 对正常电场中带电荷自由表面的非导电液体进行直接数值模拟，研究强非线性阶段的不稳定性发展，发现两个主要阶段：初始阶段形成凹陷，发展阶段凹陷转变为膨胀气泡。


<details>
  <summary>Details</summary>
Motivation: 研究电场作用下带电自由表面液体的非线性不稳定性发展机制，特别是强非线性阶段的演化特征。

Method: 采用直接数值模拟方法，分析正常电场中非导电液体带电自由表面的强非线性不稳定性发展阶段。

Result: 识别出两个主要阶段：初始阶段表面出现凹陷，发展阶段凹陷转变为膨胀气泡；气泡尺寸随外加电场增加而增大，尽管不稳定性主导模式的尺度减小。

Conclusion: 电场作用下带电液体表面的不稳定性发展呈现两个明显阶段，气泡尺寸与电场强度的关系与主导模式尺度变化趋势相反，揭示了非线性效应的重要作用。

Abstract: Direct numerical simulation of the strongly nonlinear stages of instability development for a non-conducting liquid with a charged free surface in a normal electric field is performed. It is demonstrated that two main stages of the instability can be distinguished: an initial stage, during which dimples appear on the surface, and a developed stage, during which these dimples transform into expanding bubbles. The bubble size increases with increasing applied field, despite the fact that the scale corresponding to the dominant mode of the instability decreases.

</details>


### [6] [Investigating the mechanism by which finite-size heavy particles are entrained in turbulent open channel flow over a smooth surface](https://arxiv.org/abs/2601.19600)
*Tatia Bzikadze,Markus Weyrauch,Markus Uhlmann*

Main category: physics.flu-dyn

TL;DR: 该研究通过粒子解析直接数值模拟，分析了光滑表面湍流明渠流中有限尺寸重颗粒的夹带动力学，揭示了准流向涡和高壁面法向剪切率对颗粒夹带的关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究湍流明渠流中有限尺寸重颗粒的夹带机制，特别关注颗粒旋转和横向运动约束对夹带过程的影响，以及流动结构在颗粒夹带中的作用。

Method: 采用粒子解析直接数值模拟，进行三种类型的模拟：自由运动颗粒、旋转约束颗粒和展向运动约束颗粒。通过定义颗粒附近的相对速度，将水动力分解为阻力和升力贡献，评估颗粒周围的局部壁面法向剪切率。使用相干结构识别技术分析夹带事件前后的流动结构。

Result: 旋转约束模拟显示颗粒旋转对夹带机制影响不显著；展向运动约束模拟揭示了颗粒相对于流动结构位置的重要性，夹带频率、持续时间、壁面法向剪切和夹带时与最近涡结构的距离均发生变化。升力对壁面法向力的贡献是颗粒夹带起始的原因，由与快速流体相关的高剪切事件诱导。准流向涡的存在是颗粒夹带到主流的重要条件。

Conclusion: 在临界Shields数条件下，高壁面法向剪切率和强准流向涡的邻近性是颗粒夹带机制的关键要素。升力负责夹带起始，而准流向涡促进颗粒进入主流流动。

Abstract: The dynamics of entrainment of finite-size heavy particles in a turbulent open channel flow over a smooth surface are analyzed. Three types of simulations, namely with freely moving, rotation-constrained, and spanwise-motion-constrained particles, were conducted using particle-resolved direct numerical simulations. With the aid of a relative velocity suitably defined in the vicinity of the finite-size particle, we decompose the hydrodynamic force into drag and lift contributions and evaluate the local wall-normal shear rate around the particles. By means of coherent structure eduction techniques, we investigate flow structures before and during lift-off events. Rotation-constrained simulations revealed the insignificance of particle rotation in the entrainment mechanism. Spanwise-motion-constrained simulations revealed the importance of particle location with respect to flow structures with apparent changes in entrainment frequency, duration of the entrainment process, wall-normal shear around the particles, and distance to the nearest vortical structures during lift-off. The contribution of lift to the wall-normal force is found to be responsible for the initiation of particle entrainment, which is induced by a high-shear event associated with fast-moving fluid. The presence of quasi-streamwise vortices is shown to be an important ingredient for the entrainment of particles into the bulk flow. The results show that, at marginal Shields number values, a high wall-normal shear rate and the proximity of an intense quasi-streamwise vortex are essential elements of the entrainment mechanism.

</details>


### [7] [Unveiling crown-finger instability of a non-spherical drop impacting a liquid surface](https://arxiv.org/abs/2601.19621)
*Nagula Venkata Anirudh,Sachidananda Behera,Kirti Chandra Sahu*

Main category: physics.flu-dyn

TL;DR: 三维数值研究非球形液滴撞击静止液膜的飞溅动力学，揭示了不同长宽比和韦伯数下的撞击模式，发现液滴形态显著影响冠状结构演化和飞溅起始，建立了理论模型预测冠状边缘指状结构形成。


<details>
  <summary>Details</summary>
Motivation: 研究非球形液滴撞击液膜的飞溅动力学，探索液滴形态（长宽比）和撞击能量（韦伯数）对撞击过程的影响机制，填补传统球形液滴研究的空白，为相关应用提供理论基础。

Method: 采用三维数值模拟方法，覆盖宽范围的长宽比（Ar）和韦伯数（We）参数空间，通过线性稳定性分析结合数值模拟结果，考虑Rayleigh-Plateau和Rayleigh-Taylor不稳定性，预测冠状边缘指状结构形成。

Result: 建立了Ar-We参数空间的撞击模式图，识别出铺展、飞溅类型1、飞溅类型2和冠层形成四种不同动力学模式；发现扁球状液滴促进指状生长和破碎，长球状液滴倾向于形成冠层；孔洞不稳定性源于冠状边缘下方最薄液膜区域的破裂；理论分析成功预测冠状边缘指状数量。

Conclusion: 液滴形态在飞溅动力学中起关键作用，Rayleigh-Plateau不稳定性主导早期波动数量和波长，Rayleigh-Taylor不稳定性放大扰动增长率，研究结果为涉及液滴撞击的应用提供了重要见解。

Abstract: We present a three-dimensional numerical study of the splashing dynamics of non-spherical droplets impacting a quiescent liquid film, covering a wide range of aspect ratios (Ar) and Weber numbers (We). The simulations reveal distinct impact dynamics, such as spreading, splashing type-1, splashing type-2, and canopy formation, which are delineated in a regime map constructed in the Ar-We parameter space. Our results demonstrate that droplet morphology during the impact significantly influences crown evolution and splash initiation, with oblate drops promoting finger growth and fragmentation due to enhanced rim deceleration, while prolate drops tend to form canopies. We observe that the hole instability, which becomes more prominent at higher Weber numbers, arises from lamella rupture in the thinnest region of the film, located just beneath the crown rim. A linear stability analysis, supplemented by the temporal evolution of the crown obtained from the numerical simulations, adequately predicts the number of fingers formed along the crown rim by accounting for both Rayleigh-Plateau (RP) and Rayleigh-Taylor (RT) instabilities. The theoretical analysis demonstrates the dominant role of the Rayleigh-Plateau instability in determining the number and wavelength of early undulations, with the Rayleigh-Taylor instability serving to amplify the growth rate of the disturbances. Our findings highlight the critical role of the droplet shape in splash dynamics, which is relevant to a range of applications involving droplet impact.

</details>


### [8] [Numerical simulations of simultaneous pair-drop impacts and their energetics](https://arxiv.org/abs/2601.19835)
*Ziyao Zhang,Alfonso A. Castrejon-Pita,Wouter Mostert*

Main category: physics.flu-dyn

TL;DR: 通过三维直接数值模拟研究两个相同液滴同时撞击疏水基底的现象，分析毛细作用和粘性效应（通过韦伯数和雷诺数表征）对中央液膜形成和演化的影响。


<details>
  <summary>Details</summary>
Motivation: 研究两个液滴同时撞击基底时，其扩散边缘碰撞产生的中央液膜的形成机制和动力学特性，填补对多液滴撞击相互作用理解的空白。

Method: 采用三维直接数值模拟方法，通过改变韦伯数（We）和雷诺数（Re）来调节毛细作用和粘性效应的相对强度，分析中央液膜的宽度、高度和形态演化，并与实验数据进行验证。

Result: 中央液膜的上升行为与单液滴撞击的扩散特性相似，特别是在高韦伯数和雷诺数条件下；建立了中央液膜最大高度的能量模型；提出了毛细主导和粘性主导两种机制下的标度关系，可用于轨迹数据的归一化。

Conclusion: 研究揭示了双液滴撞击中中央液膜的形成机制，建立了预测其最大高度的理论模型，为深入理解中央上升液膜的动力学特性及其破碎特征的研究奠定了基础。

Abstract: We present three-dimensional direct numerical simulations of the simultaneous impact of two identical drops on an hydrophobic substrate, varying the relative strength of capillary and viscous effects respectively through Weber and Reynolds numbers of impact. The interaction between the two drops is characterized by the appearance of a lamella arising from the collision of the two droplets' spreading rims. We examine the width, the height, and the general morphological evolution of the central sheet; the numerical data is validated against prior experiments and used to guide the development of an energetic model for the maximum elevation of the central sheet. In particular, the rise of the central sheet resembles the spreading behaviour single-drop impacts, especially at high Weber and Reynolds numbers. This fact can be used to estimate scalings in the capillary- and viscous-dominated regimes, which can be used to collapse the trajectories. These insights provide a route for a more complete understanding of the dynamics for the central rising sheet, and anticipate the detailed study of its fragmentation characteristics

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: 使用LLaMEA框架，通过LLM从自然语言描述生成具有特定景观特征的优化问题，利用ELA属性预测器评分，引入ELA空间适应度共享机制增加多样性，生成的问题扩展了BBOB实例空间。


<details>
  <summary>Details</summary>
Motivation: 现有BBOB等测试套件在连续黑盒优化基准测试中存在结构多样性有限的问题，需要能够生成具有明确高层景观特征的新优化问题。

Method: 使用LLaMEA框架，将LLM嵌入进化循环中，从目标属性的自然语言描述生成问题代码；在循环内通过基于ELA的属性预测器对候选问题进行评分；引入ELA空间适应度共享机制增加种群多样性并避免冗余景观。

Result: 生成的许多函数确实表现出预期的结构特征（通过吸引盆分析、统计测试和视觉检查验证）；t-SNE嵌入显示这些函数扩展了BBOB实例空间而非形成无关集群；最终库提供了广泛、可解释且可复现的基准问题集。

Conclusion: 该方法能够生成具有明确景观特征的多样化优化问题，扩展了现有基准测试套件，为景观分析和自动算法选择等下游任务提供了有价值的基准问题库。

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [10] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: 开发IT2-ANFIS模型用于污水处理厂能耗预测，提供可解释的不确定性量化，优于传统点预测方法


<details>
  <summary>Details</summary>
Motivation: 污水处理厂消耗全球1-3%电力，需要准确能耗预测进行运营优化。现有机器学习模型仅提供点预测，缺乏对安全关键基础设施风险决策至关重要的可解释不确定性量化。

Method: 开发区间二型自适应神经模糊推理系统（IT2-ANFIS），通过模糊规则结构生成可解释的预测区间。框架将不确定性分解为三个层次：特征级（识别引入模糊性的变量）、规则级（揭示局部模型置信度）和实例级（量化整体预测不确定性）。

Result: 在墨尔本水务东部处理厂数据集上验证，IT2-ANFIS达到与一阶ANFIS相当的预测性能，同时显著降低训练运行间的方差，并提供可解释的不确定性估计，将预测置信度直接与运营条件和输入变量关联。

Conclusion: IT2-ANFIS为污水处理厂能耗预测提供了可解释的不确定性量化框架，支持风险感知决策，有助于优化能源密集型基础设施的运营可持续性。

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [11] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: RIFT基准测试揭示大型语言模型在非顺序指令结构下性能显著下降，表明当前架构过度依赖位置连续性而非推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将任务复杂性与结构顺序混为一谈，难以分离提示拓扑结构对性能的影响。需要评估LLMs在复杂工作流中保持指令流的能力，特别是非顺序控制流场景

Method: 引入RIFT（重排序指令跟随测试平台），使用重新表述的Jeopardy!问答对，测试两种提示结构：线性提示（顺序进行）和跳跃提示（内容相同但需要非顺序遍历）。在6个最先进的开源LLMs上进行10,000次评估

Result: 在跳跃条件下，准确率相比基线下降高达72%，显示对位置连续性的强烈依赖。约50%的失败源于指令顺序违反和语义漂移，表明当前架构将指令跟随内化为顺序模式而非推理技能

Conclusion: 结构敏感性是当前架构的基本限制，对需要非顺序控制流的应用（如工作流自动化和多智能体系统）有直接影响。需要开发能够处理复杂拓扑结构的模型

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [12] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: NTP4VC：首个针对程序验证中验证条件自动证明的真实世界多语言基准测试，从Linux和Contiki-OS等实际项目中生成测试用例，评估LLM在VC证明中的表现。


<details>
  <summary>Details</summary>
Motivation: 程序验证中的验证条件自动证明是主要瓶颈，现有自动定理证明器无法处理困难VC，导致需要大量手动证明。虽然神经定理证明在数学竞赛中取得成功，但在程序验证特别是VC证明中的应用尚未充分探索，缺乏专门针对此任务的基准测试。

Method: 从Linux和Contiki-OS等真实项目收集数据，利用工业级工具链（Why3和Frama-C）生成语义等价的测试用例，涵盖Isabelle、Lean和Rocq三种形式化语言。评估通用大语言模型和针对定理证明微调的模型在VC证明任务上的表现。

Result: 大语言模型在VC证明中显示出潜力，但程序验证仍面临重大挑战，存在明显差距，为未来研究提供了重要机会。

Conclusion: NTP4VC填补了程序验证中VC自动证明基准测试的空白，虽然LLM显示出前景，但当前能力仍有限，需要进一步研究来提升自动定理证明在程序验证中的应用。

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [13] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: 研究LLM在重复社会困境中的战略行为，分析收益大小和语言背景如何影响其策略选择，揭示模型和语言依赖的行为模式


<details>
  <summary>Details</summary>
Motivation: 随着LLM在交互和多智能体环境中作为自主智能体运行，理解其战略行为对安全性、协调性以及AI驱动的社会和经济系统至关重要

Method: 使用收益缩放的囚徒困境来隔离对激励强度的敏感性，通过监督分类器训练经典重复博弈策略并应用于LLM决策，分析模型和语言依赖的行为意图

Result: 观察到一致的行为模式，包括激励敏感的调节策略和跨语言分歧，语言框架有时匹配或超过架构效应，揭示了系统性的模型和语言依赖行为意图

Conclusion: 为审计LLM作为战略智能体提供了统一框架，突出了合作偏见对AI治理和多智能体系统设计的直接意义

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [14] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk提出了一种不确定性感知的3D情感说话人脸合成方法，通过情感先验蒸馏和自适应多视角融合解决现有方法在情感对齐和渲染质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D情感说话人脸合成方法存在两个关键挑战：1) 音频-视觉情感对齐不佳，表现为音频情感提取困难和对情感微表情控制不足；2) 采用一刀切的多视角融合策略，忽略了不确定性和特征质量差异，损害了渲染质量。

Method: 提出UA-3DTalk框架，包含三个核心模块：1) 先验提取模块，将音频解耦为内容同步特征和个性化补充特征；2) 情感蒸馏模块，采用多模态注意力加权融合机制和4D高斯编码，实现细粒度音频情感提取和情感微表情精确控制；3) 基于不确定性的形变模块，使用不确定性块估计视角特定的不确定性，实现自适应多视角融合，并采用多头解码器优化高斯基元。

Result: 在常规和情感数据集上的广泛实验表明，UA-3DTalk在情感对齐（E-FID提升5.2%）、唇部同步（SyncC提升3.1%）和渲染质量（LPIPS提升0.015）方面均优于DEGSTalk和EDTalk等最先进方法。

Conclusion: UA-3DTalk通过情感先验蒸馏和不确定性感知的多视角融合，有效解决了3D情感说话人脸合成中的情感对齐和渲染质量问题，在多个评估指标上实现了显著提升。

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [15] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: 提出一种基于强化学习的对抗性数据增强方法，通过训练查询模型生成对抗性查询来挑战函数调用模型，提升LLM函数调用能力的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM函数调用能力的方法依赖人工标注或模型自动生成的数据进行微调，缺乏针对性设计，受限于固定模式和数据分布，限制了函数调用LLM的泛化性和鲁棒性。

Method: 提出新颖的对抗性数据增强方法，使用强化学习系统性地识别和针对函数调用LLM的弱点。训练框架引入基于强化学习的查询模型，专门生成挑战函数调用模型的对抗性查询。采用零和博弈形式，查询模型和函数调用模型进行迭代交替训练。

Result: 该方法推动了更鲁棒的函数调用模型的发展，为系统性地识别和纠正LLM与外部工具交互能力的弱点提供了系统化方法。

Conclusion: 通过强化学习的对抗性数据增强方法，能够有效提升LLM函数调用能力的泛化性和鲁棒性，克服现有方法的局限性。

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [16] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: TS-Debate：一种用于零样本时间序列推理的模态专业化协作多智能体辩论框架，通过专门智能体处理文本、视觉和数值信号，结合验证-冲突-校准机制，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列分析中表现出潜力但也存在脆弱性，特别是在数值保真度、模态干扰和跨模态集成方面存在困难，需要一种无需任务特定微调就能解决这些问题的框架

Method: TS-Debate采用模态专业化多智能体辩论框架，包含专门处理文本上下文、视觉模式和数值信号的专家智能体，通过结构化辩论协议协调交互，使用验证-冲突-校准机制评估智能体主张，支持轻量级代码执行和数值查找进行程序化验证

Result: 在涵盖三个公共基准测试的20个任务中，TS-Debate相比强基线（包括所有智能体观察所有输入的标准多模态辩论）实现了持续且显著的性能提升

Conclusion: TS-Debate通过模态专业化协作辩论框架有效解决了LLMs在时间序列分析中的数值保真度、模态干扰和跨模态集成问题，无需任务特定微调即可减少数值幻觉，提升推理性能

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


### [17] [LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge](https://arxiv.org/abs/2601.19155)
*Qiujun Li,Zijin Xiao,Xulin Wang,Zhidan Ma,Cheng Yang,Haifeng Li*

Main category: cs.AI

TL;DR: LocationAgent：基于RER架构的分层定位智能体，将地理证据验证外包给外部工具，显著提升零样本定位性能


<details>
  <summary>Details</summary>
Motivation: 现有图像地理定位方法通过监督训练或强化微调将位置知识和推理模式内化为静态记忆，在开放世界或需要动态知识的场景中容易出现事实幻觉和泛化瓶颈

Method: 提出分层定位智能体LocationAgent，采用RER架构（推理器-执行器-记录器）实现分层推理，通过角色分离和上下文压缩防止多步推理中的漂移问题；构建线索探索工具集进行证据验证；创建CCL-Bench中文城市定位基准数据集

Result: LocationAgent在零样本设置下显著优于现有方法至少30%；CCL-Bench数据集涵盖多种场景粒度和难度级别，解决了现有数据集中数据泄露和中文数据稀缺问题

Conclusion: 通过将分层推理逻辑保留在模型内部，同时将地理证据验证外包给外部工具，LocationAgent有效解决了现有方法的局限性，在开放世界场景中展现出优越的定位性能

Abstract: Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\% in zero-shot settings.

</details>


### [18] [Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement](https://arxiv.org/abs/2601.19170)
*Wangyang Ying,Yanchi Liu,Xujiang Zhao,Wei Cheng,Zhengzhang Chen,Wenchao Yu,Yanjie Fu,Haifeng Chen*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，通过结构化和逻辑反馈迭代优化从自然语言中提取工作流程图


<details>
  <summary>Details</summary>
Motivation: 当前LLM在从自然语言提取工作流程图时存在结构不完整和逻辑不一致的问题，需要同时保证结构有效性和逻辑对齐

Method: 采用多智能体框架，包含三个阶段：1) 图构建智能体提取初始图；2) 模拟智能体诊断结构缺陷；3) 语义智能体对齐流程逻辑与文本语义。通过自然语言反馈进行迭代优化

Result: 实验表明该方法在结构正确性和逻辑一致性方面显著优于现有基线方法

Conclusion: 提出的多智能体框架通过模块化设计和迭代反馈机制，有效解决了工作流程图提取中的结构和逻辑问题，无需监督或参数更新

Abstract: Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.

</details>


### [19] [CollectiveKV: Decoupling and Sharing Collaborative Information in Sequential Recommendation](https://arxiv.org/abs/2601.19178)
*Jingyu Li,Zhaocheng Du,Qianhui Zhu,kaiyuan Li,Zhicheng Zhang,Song-Li Wu,Chaolang Li,Pengwen Dai*

Main category: cs.AI

TL;DR: CollectiveKV是一种跨用户KV共享机制，通过可学习的全局KV池捕获用户间共享信息，将KV缓存压缩至原始大小的0.8%，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 序列推荐系统中Transformer注意力机制的计算复杂度随序列长度增长，KV缓存技术虽能降低推理延迟，但会引入大量存储开销，特别是对于拥有大量用户和长历史序列的系统。研究发现不同用户的KV序列存在显著相似性，表明KV中存在协作信号。

Method: 通过奇异值分解分析KV，发现信息可分为两部分：大部分信息可在用户间共享，小部分为用户特定。提出CollectiveKV机制，使用可学习的全局KV池捕获跨用户共享信息。推理时，每个用户从池中检索高维共享KV，并与低维用户特定KV拼接得到最终KV。

Result: 在五个序列推荐模型和三个数据集上的实验表明，该方法能将KV缓存压缩至原始大小的0.8%，同时保持甚至提升模型性能。

Conclusion: CollectiveKV通过跨用户KV共享机制有效解决了序列推荐系统中KV缓存存储开销大的问题，在显著压缩缓存的同时维持了模型性能，为实际部署提供了可行的解决方案。

Abstract: Sequential recommendation models are widely used in applications, yet they face stringent latency requirements. Mainstream models leverage the Transformer attention mechanism to improve performance, but its computational complexity grows with the sequence length, leading to a latency challenge for long sequences. Consequently, KV cache technology has recently been explored in sequential recommendation systems to reduce inference latency. However, KV cache introduces substantial storage overhead in sequential recommendation systems, which often have a large user base with potentially very long user history sequences. In this work, we observe that KV sequences across different users exhibit significant similarities, indicating the existence of collaborative signals in KV. Furthermore, we analyze the KV using singular value decomposition (SVD) and find that the information in KV can be divided into two parts: the majority of the information is shareable across users, while a small portion is user-specific. Motivated by this, we propose CollectiveKV, a cross-user KV sharing mechanism. It captures the information shared across users through a learnable global KV pool. During inference, each user retrieves high-dimensional shared KV from the pool and concatenates them with low-dimensional user-specific KV to obtain the final KV. Experiments on five sequential recommendation models and three datasets show that our method can compress the KV cache to only 0.8% of its original size, while maintaining or even enhancing model performance.

</details>


### [20] [CoReTab: Improving Multimodal Table Understanding with Code-driven Reasoning](https://arxiv.org/abs/2601.19193)
*Van-Quang Nguyen,Takayuki Okatani*

Main category: cs.AI

TL;DR: CoReTab：一个代码驱动的推理框架，通过将多步推理与可执行Python代码结合，为多模态表格理解提供可扩展、可解释且可自动验证的标注，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表格理解数据集（如MMTab）主要提供简短的事实性答案，缺乏明确的多步推理监督。在这些数据集上训练的模型通常生成简短回答，准确率不足且可解释性有限，无法清晰展示模型如何得出最终答案。

Method: 引入CoReTab代码驱动推理框架，通过将多步推理与可执行Python代码耦合，生成可扩展、可解释且可自动验证的标注。使用该框架构建了包含115K已验证样本的数据集（平均每个响应529个token），并通过三阶段流水线微调开源多模态大语言模型。

Result: 在17个MMTab基准测试（涵盖表格问答、事实验证和表格结构理解）上评估CoReTab训练的模型。相比MMTab训练的基线模型，在三个任务上分别获得+6.2%、+5.7%和+25.6%的显著提升，同时产生透明且可验证的推理轨迹。

Conclusion: CoReTab作为一个稳健且可泛化的监督框架，有效改进了多模态表格理解中的多步推理能力，为模型提供了透明和可验证的推理过程。

Abstract: Existing datasets for multimodal table understanding, such as MMTab, primarily provide short factual answers without explicit multi-step reasoning supervision. Models trained on these datasets often generate brief responses that offers insufficient accuracy and limited interpretability into how these models arrive at the final answer. We introduce CoReTab, a code-driven reasoning framework that produces scalable, interpretable, and automatically verifiable annotations by coupling multi-step reasoning with executable Python code. Using the CoReTab framework, we curate a dataset of 115K verified samples averaging 529 tokens per response and fine-tune open-source MLLMs through a three-stage pipeline. We evaluate the resulting model trained on CoReTab across 17 MMTab benchmarks spanning table question answering, fact verification, and table structure understanding. Our model achieves significant gains of +6.2%, +5.7%, and +25.6%, respectively, over MMTab-trained baselines, while producing transparent and verifiable reasoning traces. These results establish CoReTab as a robust and generalizable supervision framework for improving multi-step reasoning in multimodal table understanding.

</details>


### [21] [MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning](https://arxiv.org/abs/2601.19204)
*Zhixi Cai,Fucai Ke,Kevin Leo,Sukai Huang,Maria Garcia de la Banda,Peter J. Stuckey,Hamid Rezatofighi*

Main category: cs.AI

TL;DR: MATA是一个基于分层有限状态自动机的多智能体视觉推理系统，通过可训练的超级智能体选择顶层状态转移，每个智能体运行基于规则的子自动机，共享内存实现透明执行历史，在多个视觉推理基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂查询上存在隐式推理难以解释、容易产生幻觉的问题。组合方法虽然提高了可解释性，但大多依赖单一智能体或手工设计的流水线，无法在互补智能体之间决定何时协作或在重叠智能体之间决定何时竞争。

Method: 提出MATA系统：1) 作为分层有限状态自动机，顶层转移由可训练的超级智能体选择；2) 每个智能体对应超自动机中的一个状态，运行基于规则的子自动机进行可靠微控制；3) 所有智能体读写共享内存，形成透明执行历史；4) 构建转移轨迹树并转换为内存-下一状态对，创建MATA-SFT-90K数据集用于监督微调。

Result: 在多个视觉推理基准测试中，MATA相比单体模型和组合基线方法取得了最先进的结果。微调后的LLM作为转移策略能够理解查询和智能体能力，有效选择最优智能体解决任务。

Conclusion: MATA通过分层多智能体自动机架构实现了可解释、可靠的视觉推理，解决了现有模型在复杂查询上的幻觉问题，同时保持了高性能。系统代码和数据集已开源。

Abstract: Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.

</details>


### [22] [Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection](https://arxiv.org/abs/2601.19245)
*Yongxin Deng,Zhen Fang,Yixuan Li,Ling Chen*

Main category: cs.AI

TL;DR: 提出SpikeScore方法用于可泛化的幻觉检测，通过量化多轮对话中的不确定性突变来区分幻觉和非幻觉响应，在跨域场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在同域数据上表现良好，但在跨域泛化方面表现不佳。研究可泛化幻觉检测问题，旨在使用单域数据训练检测器，同时确保在多样相关域上的鲁棒性能。

Method: 提出SpikeScore方法，基于观察到的重要现象：幻觉引发的多轮对话在不同域中普遍表现出比事实性对话更大的不确定性波动。通过理论分析和实证验证，SpikeScore量化多轮对话中的突发波动，实现幻觉和非幻觉响应的跨域可分离性。

Result: 在多个LLM和基准测试上的实验表明，基于SpikeScore的检测方法在跨域泛化方面优于代表性基线方法，并超越了先进的面向泛化的方法，验证了该方法在跨域幻觉检测中的有效性。

Conclusion: SpikeScore通过量化多轮对话中的不确定性波动，为可泛化幻觉检测提供了有效的解决方案，在跨域场景中表现出强大的检测能力。

Abstract: Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.

</details>


### [23] [GLOVE: Global Verifier for LLM Memory-Environment Realignment](https://arxiv.org/abs/2601.19249)
*Xingkun Yin,Hongyang Du*

Main category: cs.AI

TL;DR: GLOVE框架为LLM记忆系统引入相对真理概念，通过主动探测记忆与观察的不一致性，在动态漂移环境中实现记忆-环境对齐，无需真实监督或强模型内省。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆增强方法假设记忆有效性可通过外部评估器或模型内省建立，但这些假设在动态漂移的实际环境中经常失效，需要更鲁棒的验证机制。

Method: 提出全局验证器(GLOVE)框架，通过主动探测检索记忆与新鲜观察之间的不一致性，建立相对真理概念，实现记忆验证和更新，无需真实监督或强模型内省依赖。

Result: 在增强环境漂移的网页导航、规划和控制基准测试中，GLOVE显著提高了智能体成功率，展示了在非平稳环境中的鲁棒性。

Conclusion: GLOVE为LLM记忆系统提供了新的设计维度，通过相对真理概念和主动不一致性检测，为构建能够自我进化的认知智能体提供了鲁棒路径。

Abstract: Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.

</details>


### [24] [Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems](https://arxiv.org/abs/2601.19311)
*Anh Khoa Ngo Ho,Martin Chauvin,Simon Gosset,Philippe Cordier,Boris Gamazaychikov*

Main category: cs.AI

TL;DR: 研究表明，在现实多智能体环境中，部署较小规模的开源语言模型可以在保持任务质量的同时显著降低能耗，为可持续AI设计提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型成为智能体AI系统的核心组件，其推理过程中的能源需求可能带来显著的可持续性挑战。本研究旨在探索部署较小规模语言模型是否能在多智能体现实环境中，在不损害响应性和输出质量的前提下降低能耗。

Method: 对不同规模的语言模型进行对比分析，量化效率与性能之间的权衡关系。研究包括评估各种规模模型在现实多智能体环境中的表现。

Result: 结果显示，较小的开源模型能够降低能源使用，同时保持任务质量。研究发现，通过优化配置可以实现效率与性能的良好平衡。

Conclusion: 基于研究结果，提出了可持续人工智能设计的实用指南，包括最优批量大小配置和计算资源分配策略。这些见解为开发可扩展、环境友好的AI系统提供了可行的策略。

Abstract: As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.

</details>


### [25] [RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization](https://arxiv.org/abs/2601.19404)
*Hongzhu Yi,Xinming Wang,Zhenghao zhang,Tianyu Zong,Yuanxiang Wang,Jun Xie,Tao Yu,Haopeng Jin,Zhepeng Wang,Kaixin Xu,Feng Chen,Jiahuan Chen,Yujia Yang,Zhenyu Guan,Bingkang Shi,Jungang Xu*

Main category: cs.AI

TL;DR: RPO是一种基于部分推理优化的强化微调算法，通过仅生成推理路径的后缀来减少约95%的rollout阶段token生成，显著降低训练时间，同时保持与完整路径算法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调算法需要从输入查询开始生成完整的推理轨迹，这在训练rollout阶段产生大量计算开销。为了解决这一问题，作者分析了推理路径不同部分对最终结果正确性的影响，并基于此提出了更高效的优化方法。

Method: 提出RPO（Reinforcement Fine-Tuning with Partial Reasoning Optimization）算法，这是一种即插即用的强化微调方法。与传统方法生成完整推理路径不同，RPO利用经验缓存仅生成推理路径的后缀进行训练。该方法可与GRPO、DAPO等典型算法集成，实现训练加速。

Result: RPO在rollout阶段减少了约95%的token生成，大幅降低了理论时间开销。对于1.5B模型，训练时间减少90%；对于7B模型，训练时间减少72%。同时，RPO能够与现有算法集成，在保持与原始算法相当性能的前提下实现训练加速。

Conclusion: RPO是一种高效的强化微调算法，通过部分推理优化显著减少了训练时间开销，同时保持了模型性能。该方法具有即插即用的特性，可与现有算法集成，为大规模语言模型的强化微调提供了实用的加速方案。

Abstract: Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.

</details>


### [26] [Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach](https://arxiv.org/abs/2601.19527)
*Temirbolat Maratuly,Pakizar Shamoi,Timur Samigulin*

Main category: cs.AI

TL;DR: 本文提出了一种结合模糊专家系统和数字孪生的控制策略，用于酸性水净化过程，通过模拟人类推理来维持关键参数在期望水平，并开发了基于Web的仿真界面进行测试和评估。


<details>
  <summary>Details</summary>
Motivation: 酸性水净化对于减少排放、降低腐蚀风险、实现处理水在工业或家庭应用中的再利用以及最终降低运营成本至关重要。自动化净化过程有助于减少工人伤害风险。原油中的酸性成分（如硫化氢、二氧化碳等）在处理过程中会释放到酸性水中，若不妥善处理，会对环境造成严重威胁并加速管道和设备腐蚀。

Method: 开发了模糊专家系统与定制生成的数字孪生相结合的控制策略。数字孪生使用Honeywell UniSim Design R492开发以准确模拟真实工业行为，阀门动力学通过MATLAB系统辨识建模，使用OPC DA实现仿真器与控制器之间的实时数据交换。模糊控制器采用分程控制策略控制两个阀门，在21种不同初始压力条件下使用5种不同的去模糊化策略进行了测试（共105个测试场景）。

Result: 系统性能使用误差指标（MSE、RMSE、MAE、IAE、ISE、ITAE）和动态响应指标（超调量、欠调量、上升时间、下降时间、调节时间、稳态误差）进行评估。开发了基于Python Streamlit框架的Web仿真界面。

Conclusion: 虽然本文针对酸性水处理进行演示，但所提出的模糊专家系统是通用型的，控制策略设计简单直观，允许初级或非专业人员有效与系统交互。

Abstract: Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.

</details>


### [27] [Benchmarks Saturate When The Model Gets Smarter Than The Judge](https://arxiv.org/abs/2601.19532)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.AI

TL;DR: Omni-MATH-2是一个经过人工修订的数学基准数据集，包含4181个精确答案问题和247个带标签的非标准问题，通过审计确保LaTeX可编译性、可解性和可验证性，显著减少数据集噪声，并提供更精确的模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型基准测试中数据集和评估方法的不准确性持续削弱其有效性，需要更高质量的数据集和更可靠的评估方法来准确衡量模型性能。

Method: 创建Omni-MATH-2数据集，对原始Omni-MATH进行人工修订：1) 审计每个问题确保LaTeX可编译性、可解性和可验证性；2) 添加缺失的图形或信息；3) 标记需要证明、估计或图像的问题；4) 移除杂乱内容；5) 使用GPT-5 mini和原始Omni-Judge评估法官引起的噪声。

Result: 1) 数据集修订显著减少了数据集引起的噪声；2) 比较GPT-5 mini和Omni-Judge发现法官之间存在显著差异；3) 专家标注显示在法官分歧中，Omni-Judge在96.4%的情况下是错误的；4) 随着问题难度增加，需要更胜任的法官来防止法官错误掩盖模型间的真实差异；5) 两种法官都无法识别带标签问题子集的当前失效模式。

Conclusion: 数据集质量和法官可靠性对于开发准确的模型性能基准测试都至关重要，两者都需要改进以确保基准测试能够有效跟踪大型语言模型的进展。

Abstract: Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.

</details>


### [28] [Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search](https://arxiv.org/abs/2601.19622)
*Thomas Bömer,Nico Koltermann,Max Disselnmeyer,Bastian Amberg,Anne Meyer*

Main category: cs.AI

TL;DR: 本文提出A-CEoH框架，通过将A*算法代码融入提示词来增强LLM的上下文学习能力，自动生成A*搜索的启发式函数，在UPMP和SPP问题上超越人工设计的启发式函数。


<details>
  <summary>Details</summary>
Motivation: 传统启发式函数需要专家手工设计，耗时耗力。LLM和进化框架的发展为自动化启发式设计提供了可能。本文旨在扩展EoH框架，探索A*搜索引导启发式的自动生成。

Method: 提出A-CEoH（算法上下文EoH）框架，采用领域无关的提示增强策略，将A*算法代码融入提示词以利用上下文学习能力。在单元装载预整理问题（UPMP）和经典滑块拼图问题（SPP）两个领域进行验证。

Result: 计算实验表明，A-CEoH能显著提升生成启发式的质量，在测试问题上甚至超越专家设计的启发式函数。

Conclusion: A-CEoH框架成功实现了A*搜索启发式函数的自动化生成，证明了将算法代码融入提示词的上下文学习策略的有效性，为自动化启发式设计提供了新方法。

Abstract: Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.

</details>


### [29] [Agentic Design Patterns: A System-Theoretic Framework](https://arxiv.org/abs/2601.19752)
*Minh-Dung Dao,Quy Minh Le,Hoang Thanh Lam,Duc-Trong Le,Quoc-Viet Pham,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文提出了一种基于系统理论的AI智能体工程化方法，包括一个五子系统框架和12种设计模式，旨在解决现有智能体系统设计缺乏理论基础、不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型驱动的智能体AI系统存在幻觉、推理能力差等问题，且设计往往缺乏系统性理论基础，导致不可靠和脆弱的应用。现有的智能体设计模式描述缺乏严格的系统理论支撑，难以实际实施。

Method: 提出了两个主要贡献：1）基于系统理论的框架，将智能体AI系统解构为五个核心交互功能子系统：推理与世界模型、感知与接地、动作执行、学习与适应、智能体间通信；2）基于该架构，映射到全面的智能体挑战分类，提出了12种智能体设计模式，分为基础类、认知与决策类、执行与交互类、适应与学习类。

Result: 通过ReAct框架的案例研究展示了该框架的实用性，表明所提出的设计模式能够纠正系统性架构缺陷。该工作为研究人员和工程师提供了标准化的智能体设计交流语言和结构化方法。

Conclusion: 该研究为智能体AI系统设计提供了理论基础和工程化方法，能够促进更模块化、可理解和可靠的自主系统开发，标准化了智能体设计交流。

Abstract: With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.

</details>


### [30] [GAVEL: Towards rule-based safety through activation monitoring](https://arxiv.org/abs/2601.19768)
*Shir Rozenfeld,Rahul Pankajakshan,Itay Zloczower,Eyal Lenga,Gilad Gressel,Yisroel Mirsky*

Main category: cs.AI

TL;DR: 论文提出基于规则的激活安全新范式，将LLM激活建模为可组合的认知元素，通过谓词规则实时检测违规行为，提高精确度并支持领域定制。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的安全监控方法存在精确度低、灵活性有限和可解释性不足的问题，需要一种更精确、可定制且可解释的AI安全框架。

Method: 提出基于规则的激活安全范式：1) 将激活建模为认知元素(CEs) - 细粒度可解释因子；2) 在CEs上定义谓词规则；3) 实时检测规则违规；4) 提供开源框架GAVEL和自动规则创建工具。

Result: 组合式基于规则的激活安全方法提高了精确度，支持领域定制，为可扩展、可解释和可审计的AI治理奠定了基础。

Conclusion: 基于规则的激活安全为LLM安全监控提供了更精确、灵活和可解释的解决方案，支持无需重新训练模型的安全策略配置和更新，促进AI治理的透明度和可审计性。

Abstract: Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.

</details>


### [31] [An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care](https://arxiv.org/abs/2601.19824)
*Andre Paulino de Lima,Paula Castro,Suzana Carvalho Vaz de Andrade,Rosa Maria Marcucci,Ruth Caldeira de Melo,Marcelo Garcia Manzato*

Main category: cs.AI

TL;DR: 提出一种针对老年初级医疗保健的推荐系统，利用心理测量数据结构提供忠实于模型且可被护理专业人员理解的视觉解释，以解决医疗推荐系统中的数据稀缺、可解释性、风险和有效性不确定性等挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗推荐系统面临多重挑战：缺乏公开临床数据、用户难以理解推荐理由、遵循推荐存在风险、以及推荐效果不确定性。特别是在老年初级医疗保健领域，随着人口老龄化，对个性化护理计划的需求日益增长。

Method: 开发了一种推荐模型，利用心理测量数据结构生成视觉解释。模型专注于老年初级医疗保健领域，通过巴西研究合作伙伴收集的医疗数据集进行离线性能评估，并进行用户研究评估视觉解释的可解释性。

Result: 离线性能评估显示模型在医疗数据集上表现良好。用户研究表明模型生成的视觉解释具有可解释性，能够帮助护理专业人员理解推荐理由。结果表明该模型能够推进推荐系统在老年医疗保健领域的应用。

Conclusion: 提出的推荐模型能够解决医疗推荐系统的关键挑战，特别是在老年初级医疗保健领域。随着人口结构变化带来的需求增长，该模型有望在该领域获得更广泛应用，满足日益增长的信息技术需求。

Abstract: There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.

</details>


### [32] [Routing End User Queries to Enterprise Databases](https://arxiv.org/abs/2601.19825)
*Saikrishna Sudarshan,Tanay Kulkarni,Manasi Patwardhan,Lovekesh Vig,Ashwin Srinivasan,Tanmay Tulsidas Verlekar*

Main category: cs.AI

TL;DR: 该论文研究了在多数据库企业环境中路由自然语言查询的任务，通过扩展现有NL-to-SQL数据集构建了更现实的基准，提出了一种基于推理的模块化重排序策略，在各项指标上均优于嵌入方法和直接LLM提示方法。


<details>
  <summary>Details</summary>
Motivation: 在多数据库企业环境中，随着数据库仓库规模增大和领域重叠，以及查询的模糊性增加，路由自然语言查询变得越来越具有挑战性，需要更结构化、更鲁棒的基于推理的解决方案。

Method: 提出了一种模块化、推理驱动的重排序策略，通过显式建模模式覆盖、结构连接性和细粒度语义对齐来改进查询路由。

Result: 该方法在所有指标上均一致优于仅使用嵌入的方法和直接LLM提示的基线方法，特别是在大规模、领域重叠的数据库仓库和模糊查询场景下表现更优。

Conclusion: 在多数据库企业环境中，基于推理的模块化重排序策略比简单的嵌入方法或直接LLM提示更有效，能够更好地处理模式覆盖、结构连接性和语义对齐等复杂因素，为自然语言查询路由提供了更鲁棒的解决方案。

Abstract: We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.

</details>


### [33] [Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models](https://arxiv.org/abs/2601.19834)
*Jialong Wu,Xiaoying Zhang,Hongyi Yuan,Xiangcheng Zhang,Tianhao Huang,Changjing He,Chaoyi Deng,Renrui Zhang,Youbin Wu,Mingsheng Long*

Main category: cs.AI

TL;DR: 该论文首次系统研究了视觉生成何时及如何提升推理能力，提出视觉优势假说：在物理世界相关任务中，视觉生成能更自然地作为世界模型，而纯语言模型因表征限制或先验知识不足而遇到瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在数学、编程等抽象领域已达到专家水平，但在物理和空间智能等需要丰富表征和先验知识的领域仍远落后于人类。统一多模态模型的出现激发了人们对基于互补多模态通路的类人推理的兴趣，但其益处尚不明确。

Method: 从世界模型视角出发，理论层面将内部世界建模形式化为CoT推理的核心组件，分析不同形式世界模型的区别；实证层面识别需要交错视觉-语言CoT推理的任务，构建新的评估套件VisWorld-Eval，并在最先进的UMM上进行对照实验。

Result: 在有利于视觉世界建模的任务上，交错CoT显著优于纯语言CoT，但在其他任务上没有明显优势。这验证了视觉优势假说，阐明了多模态世界建模的潜力。

Conclusion: 该工作阐明了多模态世界建模对于构建更强大、更类人的多模态AI的潜力，为理解视觉生成在推理中的作用提供了首个原则性研究框架。

Abstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [34] [DeFM: Learning Foundation Representations from Depth for Robotics](https://arxiv.org/abs/2601.18923)
*Manthan Patel,Jonas Frey,Mayank Mittal,Fan Yang,Alexander Hansson,Amir Bar,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: DeFM是一个专门为机器人应用设计的自监督深度图像基础模型，通过60M深度图像数据集训练，在多种任务上实现SOTA性能并保持跨尺度度量感知


<details>
  <summary>Details</summary>
Motivation: 深度传感器在机器人平台广泛应用，但深度模态的表征学习相比RGB模态仍未被充分探索，缺乏专门针对深度图像的大规模基础模型

Method: 采用DINO风格的自蒸馏目标在60M深度图像数据集上训练；引入新颖的输入归一化策略保持跨尺度度量感知；将DeFM蒸馏为适合资源受限系统的紧凑模型

Result: 在深度分类、分割、导航、运动、操作等基准测试中达到最先进性能；展示从仿真到真实环境的强泛化能力；模型无需任务特定微调即可使用

Conclusion: DeFM填补了深度模态基础模型的空白，为机器人学习提供了可直接使用的预训练模型，支持跨环境、任务和传感器的泛化

Abstract: Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/

</details>


### [35] [Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot](https://arxiv.org/abs/2601.18963)
*Fauna Robotics,:,Diego Aldarondo,Ana Pervan,Daniel Corbalan,Dave Petrillo,Bolun Dai,Aadhithya Iyer,Nina Mortensen,Erik Pearson,Sridhar Pandian Arunachalam,Emma Reznick,David Weis,Jacob Davison,Samuel Patterson,Tess Carella,Michael Suguitan,David Ye,Oswaldo Ferro,Nilesh Suriyarachchi,Spencer Ling,Erik Su,Daniel Giebisch,Peter Traver,Sam Fonseca,Mack Mor,Rohan Singh,Sertac Guven,Kangni Liu,Yaswanth Kumar Orru,Ashiq Rahman Anwar Batcha,Shruthi Ravindranath,Silky Arora,Hugo Ponte,Dez Hernandez,Utsav Chaudhary,Zack Walker,Michael Kelberman,Ivan Veloz,Christina Santa Lucia,Kat Casale,Helen Han,Michael Gromis,Michael Mignatti,Jason Reisman,Kelleher Guerin,Dario Narvaez,Christopher Anderson,Anthony Moschella,Robert Cochran,Josh Merel*

Main category: cs.RO

TL;DR: Sprout是一个面向开发者的安全、可表达的人形机器人平台，专为在人类环境中长期部署而设计，通过轻量化设计、柔顺控制和软外壳确保安全，集成了全身控制、抓取操作和VR遥操作功能。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人领域缺乏适合在人类环境中安全、可表达、长期部署的平台。现有系统要么是封闭的工业系统，要么是难以在人类周围部署和操作的学术原型，这限制了机器人技术的进展。

Method: Sprout采用轻量化外形设计，结合柔顺控制、有限关节扭矩和软外壳以确保安全；集成了全身控制、内置夹持器操作和基于虚拟现实的遥操作功能；配备可表达头部以支持社交互动，形成一个统一的硬件-软件堆栈。

Result: Sprout平台降低了物理和技术部署障碍，扩展了功能强大的人形机器人平台的可访问性，为在真实人类环境中开发具身智能提供了实用基础。

Conclusion: Sprout通过强调安全性、可表达性和开发者可访问性，解决了当前人形机器人平台在人类环境中部署的限制，为机器人技术发展提供了更实用的平台。

Abstract: Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.

</details>


### [36] [A Switching Nonlinear Model Predictive Control Strategy for Safe Collision Handling by an Underwater Vehicle-Manipulator System](https://arxiv.org/abs/2601.18971)
*Ioannis G. Polyzos,Konstantinos J. Kyriakopoulos*

Main category: cs.RO

TL;DR: 提出一种切换非线性模型预测控制策略，用于水下车辆-机械臂系统的安全碰撞处理，包括避免碰撞和利用机械臂推离障碍物两种模式。


<details>
  <summary>Details</summary>
Motivation: 水下自主车辆在执行干预任务时可能面临碰撞风险，需要安全处理策略以避免损坏车辆敏感区域。

Method: 采用切换非线性模型预测控制策略，当无法避免碰撞时，利用机械臂推离障碍物以偏转碰撞方向。

Result: 虚拟实验表明算法能成功检测碰撞，并能避免碰撞或利用机械臂适当处理碰撞而不损坏车辆敏感区域。

Conclusion: 提出的切换NMPC策略能有效处理水下车辆-机械臂系统的碰撞情况，提高水下干预任务的安全性。

Abstract: For active intervention tasks in underwater environments, the use of autonomous vehicles is just now emerging as an active area of research. During operation, for various reasons, the robot might find itself on a collision course with an obstacle in its environment. In this paper, a switching Nonlinear Model Predictive Control (NMPC) strategy is proposed to safely handle collisions for an Underwater Vehicle-Manipulator System (UVMS). When avoiding the collision is impossible, the control algorithm takes advantage of the manipulator, using it to push against the obstacle, and deflect away from the collision. Virtual experiments are performed to demonstrate the algorithm's capability to successfully detect collisions and either avoid them, or use the manipulator to handle them appropriately without damaging sensitive areas of the vehicle.

</details>


### [37] [Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing](https://arxiv.org/abs/2601.19079)
*Naqash Afzal,Niklas Funk,Erik Helmut,Jan Peters,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: 提出基于神经形态事件触觉传感器Evetac的连续盲文识别系统，实现高精度实时识别，模拟人类手指滑动扫描策略


<details>
  <summary>Details</summary>
Motivation: 传统机器人盲文阅读器依赖离散字符扫描，限制阅读速度；基于视觉的方法计算量大、延迟高、在真实条件下性能下降。需要一种更自然、高效的连续盲文识别方法。

Method: 使用开源神经形态事件触觉传感器Evetac，结合时空分割和轻量级ResNet分类器处理稀疏事件流，模拟人类手指连续滑动扫描策略。

Result: 在标准深度下达到接近完美准确率(>=98%)，跨多种盲文板布局泛化能力强，快速扫描下保持良好性能。在实际盲文板上实现超过90%的词级准确率。

Conclusion: 神经形态触觉感知为机器人盲文阅读提供了可扩展、低延迟的解决方案，对辅助技术和机器人触觉感知有更广泛的应用意义。

Abstract: Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.

</details>


### [38] [SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers](https://arxiv.org/abs/2601.19098)
*Kurt Enkera,Josh Pinskier,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: SimTO框架通过从物理模拟器中自动提取载荷工况，实现针对特征丰富物体的高分辨率拓扑优化，生成高度定制化的软体夹持器。


<details>
  <summary>Details</summary>
Motivation: 现有软体夹持器难以抓取具有高拓扑变异性的特征丰富物体（如齿轮、珊瑚、西兰花等），这些物体缺乏明确的"最优"接触表面，既难以抓取又容易受损。传统拓扑优化方法需要预先定义载荷工况，但对于软体夹持器与特征丰富物体的交互，这些载荷来自抓取过程中数百种不可预测的接触力，无法事先确定。

Method: 提出SimTO框架，通过基于接触的物理模拟器自动提取载荷工况，无需手动载荷规范。该框架能够针对任意特征丰富物体进行高分辨率拓扑优化，生成具有精细形态特征、与物体几何形状相匹配的高度定制化软体夹持器。

Result: 数值结果表明，SimTO生成的设计不仅对特征丰富物体具有高度专一性，还能泛化到未见过的物体。该方法消除了传统拓扑优化对预定义载荷工况的依赖，实现了更有效的软体夹持器定制化设计。

Conclusion: SimTO框架通过自动从物理模拟中提取载荷工况，解决了传统拓扑优化在软体夹持器设计中的局限性，能够为特征丰富物体生成高度定制化且具有泛化能力的软体夹持器设计。

Abstract: Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear "optimal" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.

</details>


### [39] [Agree to Disagree: Consensus-Free Flocking under Constraints](https://arxiv.org/abs/2601.19119)
*Peter Travis Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 提出一种新的群体控制方法，允许智能体通过局部观测协商不同的期望距离，无需全局信息或通信，适用于目标部分对齐或冲突的半信任场景。


<details>
  <summary>Details</summary>
Motivation: 传统群体控制假设智能体具有统一的期望距离，但实际应用中智能体类型和配置多样，且经常在没有信任保证或安全通信的环境下运行，需要更灵活的方法处理部分对齐或冲突的目标。

Method: 通过新的约束集体势函数，允许智能体协商期望距离参数，仅通过局部观测实现，无需全局信息或智能体间通信，适用于半信任场景。

Result: 通过一系列仿真验证了方法的有效性，能够处理智能体追求冲突目标的情况。

Conclusion: 更新了传统群体控制框架，放松了共享距离和约束的假设，提供了一种灵活、分布式的解决方案，适用于多样化的多智能体系统应用。

Abstract: Robots sometimes have to work together with a mixture of partially-aligned or conflicting goals. Flocking - coordinated motion through cohesion, alignment, and separation - traditionally assumes uniform desired inter-agent distances. Many practical applications demand greater flexibility, as the diversity of types and configurations grows with the popularity of multi-agent systems in society. Moreover, agents often operate without guarantees of trust or secure communication. Motivated by these challenges we update well-established frameworks by relaxing this assumption of shared inter-agent distances and constraints. Through a new form of constrained collective potential function, we introduce a solution that permits negotiation of these parameters. In the spirit of the traditional flocking control canon, this negotiation is achieved purely through local observations and does not require any global information or inter-agent communication. The approach is robust to semi-trust scenarios, where neighbouring agents pursue conflicting goals. We validate the effectiveness of the approach through a series of simulations.

</details>


### [40] [Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity](https://arxiv.org/abs/2601.19144)
*Tzvika Geft,William Zhang,Jingjin Yu,Kostas Bekris*

Main category: cs.RO

TL;DR: 提出一个在不确定性下提高自动化仓储系统运营效率的框架，针对k有界扰动下的负载重定位最小化问题，证明Θ(k)网格宽度是消除重定位的必要充分条件，并提供高效求解器。


<details>
  <summary>Details</summary>
Motivation: 实际物流应用中，存储序列已知但检索序列可能在存储后发生变化，这种不确定性导致需要重新安排负载位置，增加重定位操作，影响系统效率。现有方法假设检索序列完全已知，无法应对实际中的不确定性。

Method: 研究k有界扰动模型，其中任意两个负载最多可偏离原始顺序k个位置。提出理论分析证明Θ(k)网格宽度是消除重定位的必要充分条件。开发高效求解器计算鲁棒的存储布局。针对扰动超过k的情况，引入最小化重定位的策略。

Result: 实验表明，当k不超过网格宽度一半时，所提框架基本消除重定位；当k达到整个网格宽度时，重定位减少50%以上。理论证明Θ(k)网格宽度是消除重定位的必要充分条件。

Conclusion: 该框架有效解决了自动化仓储系统在不确定性下的运营效率问题，通过理论分析和算法设计，在k有界扰动下显著减少重定位操作，为实际物流应用提供了实用的解决方案。

Abstract: This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty. It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid. The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards. The objective is to minimize the load relocations to ensure efficient retrieval. A previous result guarantees a zero-relocation solution for known storage and retrieval sequences, even for storage at full capacity, provided that the side of the grid through which loads are stored/retrieved is at least 3 cells wide. However, in practice, the retrieval sequence can change after the storage phase. To address such uncertainty, this work investigates \emph{$k$-bounded perturbations} during retrieval, under which any two loads may depart out of order if they are originally at most $k$ positions apart. We prove that a $Θ(k)$ grid width is necessary and sufficient for eliminating relocations at maximum capacity. We also provide an efficient solver for computing a storage arrangement that is robust to such perturbations. To address the higher-uncertainty case where perturbations exceed $k$, a strategy is introduced to effectively minimize relocations. Extensive experiments show that, for $k$ up to half the grid width, the proposed storage-retrieval framework essentially eliminates relocations. For $k$ values up to the full grid width, relocations are reduced by $50\%+$.

</details>


### [41] [Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist](https://arxiv.org/abs/2601.19275)
*Tatsuya Kamijo,Mai Nishimura,Cristian C. Beltran-Hernandez,Nodoka Shibasaki,Masashi Hamaya*

Main category: cs.RO

TL;DR: 提出TaMeSo-bot系统，结合软手腕和触觉记忆实现安全鲁棒操作，核心是MAT³模型通过掩码预测学习时空表征，在真实机器人孔轴装配任务中验证了优越性能


<details>
  <summary>Details</summary>
Motivation: 触觉记忆对于接触丰富的任务（如不确定条件下的钥匙插入）至关重要，需要复制这种能力以实现安全鲁棒的机器人操作

Method: 提出TaMeSo-bot系统：1) 软手腕实现安全接触探索；2) 触觉记忆通过检索重用过去演示；3) 核心MAT³模型通过掩码标记预测联合建模机器人动作、分布式触觉反馈、力扭矩测量和本体感觉信号的时空交互

Result: 在真实机器人孔轴装配任务中验证，MAT³在所有条件下比基线方法获得更高成功率，对未见过的轴和条件展现出卓越适应能力

Conclusion: TaMeSo-bot系统成功实现了触觉记忆能力，MAT³通过掩码预测学习丰富时空表征，无需显式子任务分割，为接触丰富的机器人操作任务提供了有效解决方案

Abstract: Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.

</details>


### [42] [Teaching Machine Learning Fundamentals with LEGO Robotics](https://arxiv.org/abs/2601.19376)
*Viacheslav Sydora,Guner Dilsad Er,Michael Muehlebach*

Main category: cs.RO

TL;DR: 基于网页的平台"Machine Learning with Bricks"通过无编程的乐高机器人活动向12-17岁学生教授机器学习概念，结合交互式可视化与机器人实践，显著提升了学生对ML算法的理解和学习兴趣。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习教育主要面向高等教育阶段，缺乏面向青少年的直观、互动式学习工具。需要开发无需编程基础、通过可视化与实体机器人交互的方式，让年轻学习者能够理解KNN、线性回归、Q-learning等核心算法概念。

Method: 开发开源网页平台Machine Learning with Bricks，结合乐高机器人进行无编程的机器学习教学。平台包含交互式可视化界面，学生通过收集数据、训练模型、与机器人交互来学习三种核心算法。配套为期两天的课程，并通过前后测问卷评估14名学生的学习效果。

Result: 14名学生的前后测结果显示：1) 对机器学习算法的概念理解显著提升；2) 对人工智能的认知态度正向转变；3) 平台可用性评价高；4) 继续学习的动机增强。平台成功将机器学习概念以直观、互动的方式呈现给年轻学习者。

Conclusion: 基于可视化与实体交互的教学方法能够有效向青少年传授机器学习概念，在保持技术深度的同时提高可访问性和参与度。开源平台为青少年ML教育提供了实用工具，证明了无编程、基于机器人的教学方法的有效性。

Abstract: This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at https://learning-and-dynamics.github.io/ml-with-bricks/, with video tutorials guiding students through the experiments at https://youtube.com/playlist?list=PLx1grFu4zAcwfKKJZ1Ux4LwRqaePCOA2J.

</details>


### [43] [Judgelight: Trajectory-Level Post-Optimization for Multi-Agent Path Finding via Closed-Subwalk Collapsing](https://arxiv.org/abs/2601.19388)
*Yimin Tang,Sven Koenig,Erdem Bıyık*

Main category: cs.RO

TL;DR: Judgelight是一种后优化方法，通过折叠智能体轨迹中的闭合子路径来减少冗余运动，提高多智能体路径规划解的质量


<details>
  <summary>Details</summary>
Motivation: 基于学习的MAPF求解器虽然快速且可扩展，但生成的轨迹常包含不必要的或振荡的运动，不适合实际部署，需要后优化方法来提高轨迹质量

Method: 提出Judgelight后优化方法，将MAPF-Collapse问题形式化为整数线性规划（ILP）问题，通过折叠智能体轨迹中的闭合子路径来移除冗余运动，同时保持所有可行性约束

Result: 实验结果显示Judgelight能持续将解决方案成本降低约20%，特别是对基于学习的求解器效果显著，生成的轨迹更适合实际部署

Conclusion: Judgelight作为一种有效的后优化方法，能显著提高MAPF解决方案的质量，特别是对基于学习的求解器，为实际应用提供了更优的轨迹

Abstract: Multi-Agent Path Finding (MAPF) is an NP-hard problem with applications in warehouse automation and multi-robot coordination. Learning-based MAPF solvers offer fast and scalable planning but often produce feasible trajectories that contain unnecessary or oscillatory movements. We propose Judgelight, a post-optimization method that improves trajectory quality after a MAPF solver generates a feasible schedule. Judgelight collapses closed subwalks in agents' trajectories to remove redundant movements while preserving all feasibility constraints. We formalize this process as MAPF-Collapse, prove that it is NP-hard, and present an exact optimization approach by formulating it as integer linear programming (ILP) problem. Experimental results show Judgelight consistently reduces solution cost by around 20%, particularly for learning-based solvers, producing trajectories that are better suited for real-world deployment.

</details>


### [44] [Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.19406)
*Kaipeng Fang,Weiqing Liang,Yuyang Li,Ji Zhang,Pengpeng Zeng,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.RO

TL;DR: SimHum：一个协同训练框架，通过结合模拟机器人动作的动力学先验和真实人类观测的视觉先验，解决模拟数据和人类数据各自的局限性，实现数据高效且泛化能力强的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 模拟数据和真实人类数据虽然能规避机器人数据收集的高成本，但分别存在模拟到真实的视觉差距和人类到机器人的具身差距。研究发现这两种数据源存在天然的互补性：模拟提供机器人动作（人类数据缺乏），人类数据提供真实世界观测（模拟难以渲染）。

Method: 提出SimHum协同训练框架，同时从模拟机器人动作中提取动力学先验，从真实人类观测中提取视觉先验。基于这两种互补先验，实现数据高效且泛化能力强的真实世界机器人操作。

Result: 在相同数据收集预算下，SimHum比基线方法性能提升高达40%；仅使用80个真实数据就能达到62.5%的OOD成功率，比仅使用真实数据的基线方法高出7.1倍。

Conclusion: 通过协同利用模拟数据和人类数据的互补优势，SimHum框架能够有效克服各自的数据局限性，实现数据高效且泛化能力强的机器人操作，为机器人学习提供了新的数据融合范式。

Abstract: Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\mathbf{40\%}$ under the same data collection budget, and achieves a $\mathbf{62.5\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\times$. Videos and additional information can be found at \href{https://kaipengfang.github.io/sim-and-human}{project website}.

</details>


### [45] [Task-Centric Policy Optimization from Misaligned Motion Priors](https://arxiv.org/abs/2601.19411)
*Ziang Zheng,Kai Feng,Yi Nie,Shentao Qin*

Main category: cs.RO

TL;DR: TCMP提出任务优先对抗模仿框架，将模仿作为条件正则化而非平等目标，在保持任务性能的同时获得自然运动风格


<details>
  <summary>Details</summary>
Motivation: 人类演示的运动先验常因具身差异、重定向误差和任务无关变化而次优或与机器人任务不对齐，导致单纯模仿降低任务性能；而纯任务强化学习会产生不自然或不稳定运动，线性奖励混合存在根本局限性

Method: TCMP（任务中心运动先验）框架将模仿作为条件正则化而非平等目标，仅在模仿信号与任务进展兼容时纳入，产生自适应、几何感知的更新，保持任务可行下降并抑制不对齐时的有害模仿

Result: 通过人形机器人控制实验验证，在噪声演示下实现稳健的任务性能并保持一致的运动风格

Conclusion: TCMP解决了对抗模仿学习中线性奖励混合的根本局限性，通过任务优先方法在保持任务性能的同时获得自然运动风格

Abstract: Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.

</details>


### [46] [Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots](https://arxiv.org/abs/2601.19496)
*Jie Gu,Hongrun Gao,Zhihao Xia,Yirun Sun,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: 提出一种保证稳定连接的四边形模块化自重构机器人自重构规划算法，通过虚拟图表示构建可行连接/断开动作，使用依赖反向树组织执行序列，证明7个以上模块（除线性拓扑外）任意配置对存在满足运动特性的重构序列。


<details>
  <summary>Details</summary>
Motivation: 对于晶格型模块化自重构机器人，在重构过程中保持稳定连接对于物理可行性和部署能力至关重要。现有方法在保证连接稳定性方面存在不足，需要开发能够确保稳定连接的重构规划算法。

Method: 1. 使用虚拟图表示构建可行的连接/断开动作；2. 通过依赖反向树（DRTree）组织这些动作，解决动作间的相互依赖关系，形成有效的执行序列；3. 证明对于7个以上模块（排除线性拓扑）的任意配置对，存在满足运动特性的重构序列。

Result: 1. 与改进的BiRRT算法相比，该方法在效率和稳定性方面表现更优；2. 在物理机器人平台上部署验证了该方法的实际可行性；3. 算法能够保证重构过程中的稳定连接。

Conclusion: 提出的自重构规划算法能够有效保证四边形模块化自重构机器人在重构过程中的稳定连接，具有高效性和实际可行性，为晶格型MSRR的可靠重构提供了解决方案。

Abstract: For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.

</details>


### [47] [Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots](https://arxiv.org/abs/2601.19499)
*Mehdi Heydari Shahna,Seyed Adel Alizadeh Kolagar,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出一种结合强化学习与李雅普诺夫稳定器的控制框架，为轮式移动机器人在非结构化环境中提供形式化的目标到达保证，将到达率从84.6%提升至99.0%。


<details>
  <summary>Details</summary>
Motivation: 强化学习在目标到达策略学习方面有效，但缺乏形式化保证。现有屏蔽机制虽然能提供安全约束，但容易导致学习过程过于保守，影响探索效率。需要一种既能提供形式化保证，又不损害RL探索能力的控制框架。

Method: 1. 设计实时RL策略，包含15个精心定义的奖励项，鼓励机器人到达静态和动态目标，同时生成平滑的控制信号并满足安全规范。2. 在基准RL框架中集成李雅普诺夫式稳定器作为策略监督器，在保持状态动作空间有意义探索的同时，形式化强化目标到达控制。

Result: 实验结果表明，提出的李雅普诺夫式稳定器持续改进基准RL策略：目标到达率从84.6%提升至99.0%，显著减少失败率，提高效率。框架适合在挑战性环境中实时部署，提供形式化的收敛保证。

Conclusion: 提出的RL控制框架成功结合了强化学习的灵活性与李雅普诺夫稳定器的形式化保证，为轮式移动机器人在非结构化环境中的目标到达问题提供了既有效又可靠的解决方案，显著提升了系统性能。

Abstract: Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.

</details>


### [48] [A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation](https://arxiv.org/abs/2601.19509)
*Jin Huang,Zichen Liu,Haoda Li,Zhikun Wang,Ying Chen*

Main category: cs.RO

TL;DR: 提出改进SINS/DVL松耦合导航系统的方法，通过姿态误差感知的DVL速度转换模型和协方差传播方法，减少姿态误差累积对速度投影和噪声建模的影响，显著提升长期导航精度。


<details>
  <summary>Details</summary>
Motivation: 传统SINS/DVL松耦合架构使用SINS推导的姿态将DVL速度从机体坐标系转换到导航坐标系，但姿态估计误差的累积会导致速度投影偏差，长期运行会降低导航性能。

Method: 提出两种互补改进：1) 车辆姿态误差感知的DVL速度转换模型，在观测方程中引入姿态误差项以减少投影引起的速度偏差；2) 基于协方差矩阵的方差传播方法，通过期望的姿态误差补偿项实现统计一致的噪声建模。

Result: 仿真和现场实验结果表明，两种改进均能单独提升导航精度，证实累积姿态误差同时影响投影速度测量及其不确定性。联合应用时能有效抑制长期误差发散。现场实验显示，相比基线IMU+DVL方法，3D位置RMSE提升78.3%，最大分量位置误差减少71.8%。

Conclusion: 所提方法通过姿态误差感知的速度转换和统计一致的噪声建模，显著改善了SINS/DVL导航系统的长期性能，为水下导航提供了鲁棒的解决方案。

Abstract: In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.

</details>


### [49] [PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment](https://arxiv.org/abs/2601.19514)
*Ruiyu Wang,Zheyu Zhuang,Danica Kragic,Florian T. Pokorny*

Main category: cs.RO

TL;DR: PALM通过利用局部动作分布在OOD和演示域之间的不变性，同时解决工作空间偏移、视角变化和跨具身转移等泛化挑战，无需额外模态、模型修改或数据收集。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对单个泛化轴（如工作空间偏移、视角变化、跨具身转移）进行独立开发，依赖复杂流程，难以同时处理多种OOD偏移。需要一种能同时应对多种泛化挑战的简洁方法。

Method: PALM将操作策略模块化为粗粒度全局组件和细粒度局部策略。通过强制局部视觉聚焦和一致的本体感觉表示，减少域内和OOD输入在局部策略层面的差异，使策略能在OOD条件下检索不变的局部动作。

Result: 实验表明，PALM将OOD性能下降限制在模拟环境中8%、真实世界24%，而基线方法分别为45%和77%，显著提升了跨域泛化能力。

Conclusion: PALM利用局部动作分布的不变性，提供了一种统一框架来同时处理多种OOD偏移，显著改善了图像行为克隆的泛化性能，且无需额外模态或数据收集。

Abstract: Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.

</details>


### [50] [Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion](https://arxiv.org/abs/2601.19529)
*Jie Gu,Yirui Sun,Zhihao Xia,Tin Lun Lam,Chunxu Tian,Dan Zhang*

Main category: cs.RO

TL;DR: Rhombot是一种新型可变形平面晶格模块化自重构机器人，采用菱形模块设计，通过单个中央执行器实现对角线折叠/展开，能够在不同环境中稳定重构。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够以最小控制复杂度实现基本MSRR功能（变形、对接、运动）的模块化自重构机器人，使其能够在不同环境中可靠地形成各种配置。

Method: 采用菱形模块设计，每个模块包含平行四边形骨架和单个中央执行器，实现沿对角线折叠/展开；引入morphpivoting运动原语进行重构，并提出连续执行策略。

Result: 物理实验验证了模块的稳定重构能力、位置精度和对接精度，展示了系统在不同环境中的可靠配置形成能力。

Conclusion: Rhombot通过简化设计和控制复杂度，实现了连续稳定的重构过程，为模块化自重构机器人提供了新的设计思路和运动原语。

Abstract: In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.

</details>


### [51] [Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation](https://arxiv.org/abs/2601.19536)
*Hongji Liu,Linwei Zheng,Yongjian Li,Mingkai Tang,Xiaoyang Yan,Ming Liu,Jun Ma*

Main category: cs.RO

TL;DR: 提出基于增强逆透视映射的低成本统一矢量道路建图框架，使用Catmull-Rom样条表征车道线，多边形表征地面标记，通过实例分割结果优化三维控制点和IPM单应矩阵，实现厘米级精度建图。


<details>
  <summary>Details</summary>
Motivation: 传统IPM方法在矢量道路建图中存在映射误差大、共面性假设限制、车辆位姿精度不足等问题，需要开发低成本高精度的统一建图框架。

Method: 1) 使用Catmull-Rom样条表征车道线，多边形统一表征其他地面标记；2) 利用实例分割结果优化样条控制点和多边形角点的三维位置；3) 同时优化IPM单应矩阵和车辆位姿；4) 通过联合优化减少IPM映射误差。

Result: 1) 显著降低IPM映射误差；2) 提高初始IPM单应矩阵和车辆位姿预测精度；3) 突破IPM共面性假设限制；4) 在两个实际场景测试中实现近厘米级精度自动建图；5) 优化的IPM矩阵达到人工校准精度水平。

Conclusion: 提出的增强IPM框架为矢量道路建图提供了低成本高精度解决方案，能够统一处理各种地面标记和车道线，实现自动化高精度建图，具有实际应用价值。

Abstract: In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.

</details>


### [52] [Enhancing Worker Safety in Harbors Using Quadruped Robots](https://arxiv.org/abs/2601.19643)
*Zoe Betta,Davide Corongiu,Carmine Tommaso Recchiuto,Antonio Sgorbissa*

Main category: cs.RO

TL;DR: 该研究提出使用四足机器人进行港口基础设施检查的初步解决方案，首先识别港口环境中的关键区域，然后分析四足机器人在这些区域的检查应用。


<details>
  <summary>Details</summary>
Motivation: 基础设施检查对保障工人安全至关重要，港口环境因日常运营复杂而给机器人检查方案设计带来挑战，需要开发专门的机器人解决方案。

Method: 采用两阶段方法：第一阶段识别港口环境中的关键检查区域；第二阶段分析使用四足机器人检查这些关键区域的初步解决方案。

Result: 提出了港口基础设施检查的初步框架，识别了关键检查区域，并分析了四足机器人作为检查平台的适用性。

Conclusion: 四足机器人在港口基础设施检查中具有应用潜力，通过识别关键区域并设计针对性解决方案，可以提高检查效率和安全性。

Abstract: Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.

</details>


### [53] [SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects](https://arxiv.org/abs/2601.19742)
*Ali Jnadi,Hadi Salloum,Yaroslav Kholodov,Alexander Gasnikov,Karam Almaghout*

Main category: cs.RO

TL;DR: SCOPE：一种快速高效的柔性线性物体建模与操控框架，使用凸近似替代传统能量方法，在保持物理合理形变的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于能量的柔性线性物体建模方法计算成本高，难以满足实时或近实时应用需求。需要一种能在计算效率和形变质量之间取得更好平衡的方法。

Method: 采用凸近似方法替代传统的能量最小化框架，通过简化计算复杂度实现快速建模，同时保持形变的平滑性和物理合理性。

Result: 框架在综合仿真实验中表现出色，能够在几何和长度约束下生成平滑的形状轨迹，实现实时或近实时响应。

Conclusion: SCOPE框架在速度与精度之间取得了良好平衡，特别适用于需要实时响应的柔性线性物体操控应用。

Abstract: We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.

</details>


### [54] [Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications](https://arxiv.org/abs/2601.19761)
*Jin Huang,Fethiye Irmak Doğan,Hatice Gunes*

Main category: cs.RO

TL;DR: 该论文提出将推荐系统技术整合到社交机器人中，以解决现有个性化方法的局限性，实现更全面的用户偏好建模和个性化交互。


<details>
  <summary>Details</summary>
Motivation: 现有社交机器人个性化方法（如大语言模型和强化学习）无法全面捕捉用户的长短期偏好，也难以进行动作排序选择、主动个性化调整和确保伦理适应性。需要更系统的个性化解决方案。

Method: 通过三个步骤整合推荐系统技术：1) 对齐社交机器人与推荐系统的基本范式；2) 识别能增强社交机器人个性化的关键技术；3) 将这些技术设计为模块化、即插即用的组件。

Result: 建立了一个将推荐系统技术整合到社交机器人中的框架，为推荐系统和人机交互社区的深度合作开辟了途径，加速两个领域的创新。

Conclusion: 推荐系统技术能够有效解决社交机器人个性化中的关键挑战，通过模块化整合可以实现更全面、主动且符合伦理的个性化交互，促进跨领域协作创新。

Abstract: Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.

</details>


### [55] [Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse](https://arxiv.org/abs/2601.19826)
*Fan Yang,Renkai Ma,Yaxin Hu,Lingyao Li*

Main category: cs.RO

TL;DR: 研究机器人拟人化程度和道德基础如何影响人们对机器人虐待的反应，发现拟人化决定是否给予道德关怀，道德基础影响关怀推理方式


<details>
  <summary>Details</summary>
Motivation: 随着机器人日益融入日常生活，理解人们对机器人虐待的反应具有重要的伦理和设计意义，需要探究拟人化水平和道德基础如何塑造这些反应

Method: 混合方法研究(N=201)，参与者观看不同拟人化程度机器人(蜘蛛型、双足型、人形)被物理虐待的视频，完成道德基础、愤怒和社会距离测量，并进行定性分析

Result: 拟人化程度决定人们是否将道德关怀延伸至机器人，道德基础塑造他们对此类关怀的推理方式；定性分析显示低进步主义个体采用基于特征的判断，高进步主义个体进行未来导向的道德审议

Conclusion: 研究结果为机器人设计和政策沟通提供启示，表明拟人化是道德关怀的关键决定因素，而道德基础影响关怀的推理模式

Abstract: As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.

</details>


### [56] [Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation](https://arxiv.org/abs/2601.19832)
*Elena Merlo,Marta Lagomarsino,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出一种基于单次RGB视频演示的双臂任务编程方法，利用信息论分析手部协调策略，生成模块化行为树执行计划


<details>
  <summary>Details</summary>
Motivation: 双臂任务的示教编程因手部协调复杂性而未被充分探索，现有方法难以处理单次演示视频并生成双臂协调执行计划

Method: 应用香农信息论分析场景元素间的信息流，利用场景图特性检测手部协调策略，从单次RGB视频生成模块化行为树执行计划

Result: 通过多个主题视频演示和公开数据集验证，相比现有方法在生成双臂系统集中式执行计划方面有显著改进

Conclusion: 该方法能有效处理单次双臂任务演示视频，生成基于协调策略的模块化行为树，为双臂机器人编程提供新解决方案

Abstract: Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.

</details>


### [57] [HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs](https://arxiv.org/abs/2601.19839)
*Jeanne Malécot,Hamed Rahimi,Jeanne Cattoni,Marie Samson,Mouad Abrini,Mahdi Khoramshahi,Maribel Pino,Mohamed Chetouani*

Main category: cs.RO

TL;DR: HARMONI是一个基于大语言模型的多模态个性化框架，使社交辅助机器人能够在多用户环境中实现长期个性化交互，通过四个核心模块实现感知、世界建模、用户建模和响应生成。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统缺乏在多用户环境中持续个性化和动态适应的机制，限制了其在真实世界部署中的有效性，特别是在需要长期个性化交互的社交辅助机器人场景中。

Method: 提出HARMONI框架，包含四个关键模块：1) 感知模块识别活跃说话者并提取多模态输入；2) 世界建模模块维护环境表示和短期对话上下文；3) 用户建模模块更新长期说话者特定档案；4) 生成模块产生情境接地且符合伦理的响应。

Result: 在四个数据集上的广泛评估和消融研究，以及在养老院环境中的真实场景用户研究表明，HARMONI支持稳健的说话者识别、在线记忆更新和伦理对齐的个性化，在用户建模准确性、个性化质量和用户满意度方面优于基线LLM驱动方法。

Conclusion: HARMONI框架通过整合多模态感知、世界建模、长期用户建模和伦理约束的响应生成，有效解决了多用户环境中社交辅助机器人的持续个性化问题，为实际部署提供了可行的解决方案。

Abstract: Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.

</details>


### [58] [Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability](https://arxiv.org/abs/2601.19856)
*Giulio Campagna,Marta Lagomarsino,Marta Lorenzini,Dimitrios Chrysostomou,Matthias Rehm,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了一种基于数据驱动的框架，利用行为指标评估人机协作中的信任度，通过偏好优化算法生成增强信任的轨迹，并在化工场景中验证了机器学习模型预测信任度的有效性。


<details>
  <summary>Details</summary>
Motivation: 工业5.0强调以人为中心的人机协作，需要确保安全、舒适和信任。当前缺乏有效评估人机信任的方法，特别是基于行为指标的客观评估框架。本研究旨在开发数据驱动的信任评估方法，促进更有效的人机协作。

Method: 提出数据驱动的信任评估框架：1) 使用基于偏好的优化算法，根据操作员反馈生成增强信任的机器人轨迹；2) 将操作员反馈作为真实标签，训练机器学习模型从行为指标预测信任水平；3) 在化工行业场景中验证框架，机器人协助人类操作员混合化学品。

Result: 机器学习模型在信任分类任务中达到超过80%的准确率，其中投票分类器获得84.07%的准确率和0.90的AUC-ROC分数。这表明行为指标能有效预测人机协作中的信任动态。

Conclusion: 数据驱动方法在评估人机协作信任方面具有显著效果，行为指标在预测人类信任动态中发挥重要作用。该框架为实现工业5.0中安全、舒适、可信的人机协作提供了实用工具。

Abstract: Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\% accuracy, with the Voting Classifier achieving 84.07\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [NavFormer: IGRF Forecasting in Moving Coordinate Frames](https://arxiv.org/abs/2601.18800)
*Yoontae Hwang,Dongwoo Lee,Minseok Choi,Yong Sup Ihn,Daham Kim,Deok-Young Lee*

Main category: cs.LG

TL;DR: NavFormer使用旋转不变标量特征和规范SPD模块预测地磁场总强度，通过稳定三轴磁力计数据的二阶矩谱来消除姿态变化影响，在多种训练场景下优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 三轴磁力计组件随传感器姿态变化而变化，即使国际地磁参考场（IGRF）总强度目标保持不变。这种姿态依赖性给基于磁场的自主导航带来了挑战，需要开发对传感器旋转具有鲁棒性的预测方法。

Method: NavFormer采用旋转不变标量特征和规范对称正定（SPD）模块。该模块通过每个时间窗口的Gram矩阵构建规范坐标系，并在原始坐标中应用状态相关的谱缩放，稳定三轴数据的窗口级二阶矩谱，避免符号不连续性。

Result: 在五个飞行实验中的评估表明，NavFormer在标准训练、少样本训练和零样本迁移场景下均比强基线方法具有更低的预测误差。

Conclusion: NavFormer通过规范SPD模块有效处理三轴磁力计数据的姿态变化问题，为自主导航提供了鲁棒的IGRF总强度预测框架，代码已开源。

Abstract: Triad magnetometer components change with sensor attitude even when the IGRF total intensity target stays invariant. NavFormer forecasts this invariant target with rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window level second moments of the triads without sign discontinuities. The module builds a canonical frame from a Gram matrix per window and applies state dependent spectral scaling in the original coordinates. Experiments across five flights show lower error than strong baselines in standard training, few shot training, and zero shot transfer. The code is available at: https://anonymous.4open.science/r/NavFormer-Robust-IGRF-Forecasting-for-Autonomous-Navigators-0765

</details>


### [60] [VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space](https://arxiv.org/abs/2601.18823)
*Alejandro Ascarate,Leo Lebrat,Rodrigo Santa Cruz,Clinton Fookes,Olivier Salvado*

Main category: cs.LG

TL;DR: 该论文提出使用超球面坐标表示VAE的潜在变量，以解决高维潜在空间中异常检测问题，在多个数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在高维潜在空间中存在多个问题：1）超体积随维度指数增长，严重影响生成能力；2）标准VAE的潜在向量分布在超球面的"赤道"上，这给异常检测带来挑战。

Method: 采用超球面坐标表示VAE的潜在变量，允许将潜在向量压缩到超球面的特定方向上，从而获得更具表达力的近似后验分布。

Result: 该方法显著提升了VAE在完全无监督和分布外异常检测方面的能力，在多个数据集上实现了最佳性能：1）在火星探测器相机异常景观检测和地面图像异常星系检测等复杂真实数据集上表现优异；2）在Cifar10和ImageNet子集等标准基准测试中优于现有方法。

Conclusion: 使用超球面坐标表示VAE潜在变量能有效解决高维潜在空间中的异常检测问题，通过压缩潜在向量到特定方向来增强近似后验的表达能力，从而提升异常检测性能。

Abstract: Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.

</details>


### [61] [IPBC: An Interactive Projection-Based Framework for Human-in-the-Loop Semi-Supervised Clustering of High-Dimensional Data](https://arxiv.org/abs/2601.18828)
*Mohammad Zare*

Main category: cs.LG

TL;DR: 提出交互式投影聚类框架IPBC，通过人机协作迭代优化高维数据聚类效果


<details>
  <summary>Details</summary>
Motivation: 高维数据集聚类困难，传统降维技术生成静态嵌入缺乏可解释性，无法利用分析师直觉进行探索

Method: IPBC框架整合非线性投影模块与反馈循环，用户通过调整视角和提供约束关系（必须连接/不能连接）来修改嵌入，约束重塑投影模型目标，使语义相关点靠近、无关点远离

Result: 在多个基准数据集上的实验表明，少量交互细化步骤能显著提升聚类质量

Conclusion: IPBC将聚类转变为协作发现过程，机器表示与人类洞察相互增强

Abstract: High-dimensional datasets are increasingly common across scientific and industrial domains, yet they remain difficult to cluster effectively due to the diminishing usefulness of distance metrics and the tendency of clusters to collapse or overlap when projected into lower dimensions. Traditional dimensionality reduction techniques generate static 2D or 3D embeddings that provide limited interpretability and do not offer a mechanism to leverage the analyst's intuition during exploration. To address this gap, we propose Interactive Project-Based Clustering (IPBC), a framework that reframes clustering as an iterative human-guided visual analysis process. IPBC integrates a nonlinear projection module with a feedback loop that allows users to modify the embedding by adjusting viewing angles and supplying simple constraints such as must-link or cannot-link relationships. These constraints reshape the objective of the projection model, gradually pulling semantically related points closer together and pushing unrelated points further apart. As the projection becomes more structured and expressive through user interaction, a conventional clustering algorithm operating on the optimized 2D layout can more reliably identify distinct groups. An additional explainability component then maps each discovered cluster back to the original feature space, producing interpretable rules or feature rankings that highlight what distinguishes each cluster. Experiments on various benchmark datasets show that only a small number of interactive refinement steps can substantially improve cluster quality. Overall, IPBC turns clustering into a collaborative discovery process in which machine representation and human insight reinforce one another.

</details>


### [62] [CP Loss: Channel-wise Perceptual Loss for Time Series Forecasting](https://arxiv.org/abs/2601.18829)
*Yaohua Zha,Chunlin Fan,Peiyuan Liu,Yong Jiang,Tao Dai,Hai Wu,Shu-Tao Xia*

Main category: cs.LG

TL;DR: 提出通道感知损失函数（CP Loss），通过为每个时间序列通道学习适配其特性的感知空间，解决传统均方误差损失在异质多通道数据中无法捕捉通道特定动态的问题。


<details>
  <summary>Details</summary>
Motivation: 多通道时间序列数据具有显著的异质性，但现有预测模型通常使用通道无关的损失函数（如MSE），对所有通道采用统一度量标准，导致无法有效捕捉各通道特定的动态特征，如剧烈波动或趋势变化。

Method: 提出通道感知损失函数（CP Loss），核心思想是为每个通道学习一个适配其特性的独特感知空间，并在该空间内计算损失。具体方法：1）设计可学习的通道级滤波器，将原始信号分解为解耦的多尺度表示，构成感知空间基础；2）滤波器与主要预测模型联合优化，确保学习的感知空间明确面向预测任务；3）在这些感知空间内计算损失以优化模型。

Result: 论文提供了代码实现（GitHub链接），表明该方法已在实际应用中验证。通过为每个通道定制感知空间，CP Loss能够更好地捕捉通道特定动态，提升多通道时间序列预测性能。

Conclusion: CP Loss通过为多通道时间序列的每个通道学习任务导向的感知空间，解决了传统通道无关损失函数的局限性，能够更有效地捕捉通道特定动态，提升预测模型的性能。

Abstract: Multi-channel time-series data, prevalent across diverse applications, is characterized by significant heterogeneity in its different channels. However, existing forecasting models are typically guided by channel-agnostic loss functions like MSE, which apply a uniform metric across all channels. This often leads to fail to capture channel-specific dynamics such as sharp fluctuations or trend shifts. To address this, we propose a Channel-wise Perceptual Loss (CP Loss). Its core idea is to learn a unique perceptual space for each channel that is adapted to its characteristics, and to compute the loss within this space. Specifically, we first design a learnable channel-wise filter that decomposes the raw signal into disentangled multi-scale representations, which form the basis of our perceptual space. Crucially, the filter is optimized jointly with the main forecasting model, ensuring that the learned perceptual space is explicitly oriented towards the prediction task. Finally, losses are calculated within these perception spaces to optimize the model. Code is available at https://github.com/zyh16143998882/CP_Loss.

</details>


### [63] [How Much Temporal Modeling is Enough? A Systematic Study of Hybrid CNN-RNN Architectures for Multi-Label ECG Classification](https://arxiv.org/abs/2601.18830)
*Alireza Jafari,Fatemeh Jafari*

Main category: cs.LG

TL;DR: CNN结合单层BiLSTM在ECG多标签分类中取得最佳性能-复杂度平衡，堆叠循环层带来收益递减


<details>
  <summary>Details</summary>
Motivation: ECG信号多标签分类面临多病症共存、类别不平衡和长程时序依赖等挑战。现有研究多依赖深度堆叠循环架构，但未严格验证其必要性和临床合理性。

Method: 系统比较CNN与多种循环配置（LSTM、GRU、BiLSTM及其堆叠变体）在PTB-XL数据集（23个诊断类别）上的性能。CNN作为形态学基线，逐步集成循环层评估其对时序建模和泛化性能的贡献。

Result: CNN集成单层BiLSTM在预测性能和模型复杂度间取得最佳平衡，获得最优的Hamming损失（0.0338）、macro-AUPRC（0.4715）、micro-F1分数（0.6979）和子集准确率（0.5723）。堆叠循环模型虽偶尔提升特定罕见类召回率，但增加循环深度带来收益递减并可能因精度降低和过拟合而损害泛化。

Conclusion: 与ECG信号内在时序结构的架构对齐（而非增加循环深度）是获得稳健性能和临床相关部署的关键决定因素。单层BiLSTM-CNN组合提供了最佳性能-复杂度权衡。

Abstract: Accurate multi-label classification of electrocardiogram (ECG) signals remains challenging due to the coexistence of multiple cardiac conditions, pronounced class imbalance, and long-range temporal dependencies in multi-lead recordings. Although recent studies increasingly rely on deep and stacked recurrent architectures, the necessity and clinical justification of such architectural complexity have not been rigorously examined. In this work, we perform a systematic comparative evaluation of convolutional neural networks (CNNs) combined with multiple recurrent configurations, including LSTM, GRU, Bidirectional LSTM (BiLSTM), and their stacked variants, for multi-label ECG classification on the PTB-XL dataset comprising 23 diagnostic categories. The CNN component serves as a morphology-driven baseline, while recurrent layers are progressively integrated to assess their contribution to temporal modeling and generalization performance. Experimental results indicate that a CNN integrated with a single BiLSTM layer achieves the most favorable trade-off between predictive performance and model complexity. This configuration attains superior Hamming loss (0.0338), macro-AUPRC (0.4715), micro-F1 score (0.6979), and subset accuracy (0.5723) compared with deeper recurrent combinations. Although stacked recurrent models occasionally improve recall for specific rare classes, our results provide empirical evidence that increasing recurrent depth yields diminishing returns and may degrade generalization due to reduced precision and overfitting. These findings suggest that architectural alignment with the intrinsic temporal structure of ECG signals, rather than increased recurrent depth, is a key determinant of robust performance and clinically relevant deployment.

</details>


### [64] [The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning](https://arxiv.org/abs/2601.18832)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: TGR是一个无需训练的框架，通过流形感知的潜在前瞻搜索在严格内存限制下增强长链思维推理，在数学和代码基准上显著提升轨迹覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有扩展测试时计算的方法在计算成本与覆盖质量之间存在基本权衡：要么需要高训练成本，要么产生冗余轨迹。需要一种既能提高长链思维推理质量，又能在严格内存限制下高效运行的方法。

Method: TGR采用无需训练的框架，在分块边界处通过轻量级前瞻估计结合软几何正则化器对候选潜在锚点进行评分，鼓励平滑轨迹和多样化探索。分块级KV缓存重置保持内存与分块长度线性增长。

Result: 在具有挑战性的数学和代码基准测试中，TGR将鲁棒轨迹覆盖率（通过Pass@k曲线下面积AUC衡量）在Qwen3-8B模型上提升了高达13个百分点，而开销仅为约1.1-1.3倍。

Conclusion: TGR通过流形感知的潜在前瞻搜索，在严格内存限制下实现了训练自由的长链思维推理增强，显著提高了轨迹覆盖率，为高效推理提供了新方法。

Abstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.

</details>


### [65] [Time series forecasting with Hahn Kolmogorov-Arnold networks](https://arxiv.org/abs/2601.18837)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.LG

TL;DR: HaKAN：基于Hahn多项式的KAN模型，用于多元时间序列预测，在轻量级和可解释性方面优于现有Transformer和MLP方法


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型存在二次复杂度问题和置换等变注意力限制，MLP模型存在频谱偏差问题，需要一种更高效、轻量且可解释的长期时间序列预测方法

Method: 提出HaKAN模型，基于Kolmogorov-Arnold Networks，采用Hahn多项式基的可学习激活函数，结合通道独立性、分块处理、Hahn-KAN块堆叠（包含块间和块内KAN层）、残差连接和瓶颈结构

Result: 在多个预测基准测试中，HaKAN模型持续优于最新的SOTA方法，消融研究验证了其核心组件的有效性

Conclusion: HaKAN为多元时间序列预测提供了一个轻量级、可解释的替代方案，有效解决了现有Transformer和MLP模型的局限性

Abstract: Recent Transformer- and MLP-based models have demonstrated strong performance in long-term time series forecasting, yet Transformers remain limited by their quadratic complexity and permutation-equivariant attention, while MLPs exhibit spectral bias. We propose HaKAN, a versatile model based on Kolmogorov-Arnold Networks (KANs), leveraging Hahn polynomial-based learnable activation functions and providing a lightweight and interpretable alternative for multivariate time series forecasting. Our model integrates channel independence, patching, a stack of Hahn-KAN blocks with residual connections, and a bottleneck structure comprised of two fully connected layers. The Hahn-KAN block consists of inter- and intra-patch KAN layers to effectively capture both global and local temporal patterns. Extensive experiments on various forecasting benchmarks demonstrate that our model consistently outperforms recent state-of-the-art methods, with ablation studies validating the effectiveness of its core components.

</details>


### [66] [Analysis of Control Bellman Residual Minimization for Markov Decision Problem](https://arxiv.org/abs/2601.18840)
*Donghwan Lee,Hyukjun Yang*

Main category: cs.LG

TL;DR: 该论文研究了用于策略优化的控制贝尔曼残差最小化方法，建立了该领域的基础性理论结果


<details>
  <summary>Details</summary>
Motivation: 贝尔曼残差最小化相比动态规划方法具有更稳定的收敛性等优势，但在策略优化（控制任务）方面的研究相对较少，需要建立基础理论框架

Method: 建立控制贝尔曼残差最小化的理论基础，研究用于策略优化的贝尔曼残差最小化方法

Result: 为控制贝尔曼残差最小化策略优化建立了基础性理论结果

Conclusion: 贝尔曼残差最小化在策略优化领域具有研究价值，本文为该方向建立了理论基础，为进一步研究奠定了基础

Abstract: Markov decision problems are most commonly solved via dynamic programming. Another approach is Bellman residual minimization, which directly minimizes the squared Bellman residual objective function. However, compared to dynamic programming, this approach has received relatively less attention, mainly because it is often less efficient in practice and can be more difficult to extend to model-free settings such as reinforcement learning. Nonetheless, Bellman residual minimization has several advantages that make it worth investigating, such as more stable convergence with function approximation for value functions. While Bellman residual methods for policy evaluation have been widely studied, methods for policy optimization (control tasks) have been scarcely explored. In this paper, we establish foundational results for the control Bellman residual minimization for policy optimization.

</details>


### [67] [ASEHybrid: When Geometry Matters Beyond Homophily in Graph Neural Networks](https://arxiv.org/abs/2601.18912)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将曲率引导的重连和位置几何通过标签信息性的视角联系起来，并实例化为几何感知架构ASEHybrid，用于解决GNN在低同配性图上的性能问题。


<details>
  <summary>Details</summary>
Motivation: 标准消息传递GNN在低同配性图上表现不佳，但同配性本身不能完全解释这一现象。研究发现标签信息性（相邻节点标签间的互信息）能更准确地描述图结构何时有用。需要开发一个理论框架来理解几何感知GNN何时能超越仅使用特征的基线模型。

Method: 1. 建立统一理论框架连接曲率引导重连和位置几何；2. 实例化为几何感知架构ASEHybrid，使用Forman曲率和拉普拉斯位置编码；3. 理论分析：将调整后的同配性和标签信息性与拉普拉斯平滑下标签信号的谱行为联系起来，证明基于度的Forman曲率不增加表达能力但重塑信息流，建立曲率引导重连过程的收敛性和Lipschitz稳定性保证。

Result: 理论分析提供了几何感知GNN何时能改进特征基线模型的充要条件：当且仅当图结构携带超越节点特征的标签相关信息时。在Chameleon、Squirrel、Texas、Tolokers和Minesweeper上的实验表明，ASEHybrid在标签信息性异配性基准上获得增益，而在高基线机制下无显著改进。

Conclusion: 标签信息性而非同配性是决定图结构有用性的关键因素。几何感知GNN仅在图结构提供超越节点特征的标签相关信息时才有优势。曲率引导重连主要重塑信息流而非增加表达能力。该框架为设计针对特定图几何特性的GNN提供了理论基础。

Abstract: Standard message-passing graph neural networks (GNNs) often struggle on graphs with low homophily, yet homophily alone does not explain this behavior, as graphs with similar homophily levels can exhibit markedly different performance and some heterophilous graphs remain easy for vanilla GCNs. Recent work suggests that label informativeness (LI), the mutual information between labels of adjacent nodes, provides a more faithful characterization of when graph structure is useful. In this work, we develop a unified theoretical framework that connects curvature-guided rewiring and positional geometry through the lens of label informativeness, and instantiate it in a practical geometry-aware architecture, ASEHybrid. Our analysis provides a necessary-and-sufficient characterization of when geometry-aware GNNs can improve over feature-only baselines: such gains are possible if and only if graph structure carries label-relevant information beyond node features. Theoretically, we relate adjusted homophily and label informativeness to the spectral behavior of label signals under Laplacian smoothing, show that degree-based Forman curvature does not increase expressivity beyond the one-dimensional Weisfeiler--Lehman test but instead reshapes information flow, and establish convergence and Lipschitz stability guarantees for a curvature-guided rewiring process. Empirically, we instantiate ASEHybrid using Forman curvature and Laplacian positional encodings and conduct controlled ablations on Chameleon, Squirrel, Texas, Tolokers, and Minesweeper, observing gains precisely on label-informative heterophilous benchmarks where graph structure provides label-relevant information beyond node features, and no meaningful improvement in high-baseline regimes.

</details>


### [68] [One Global Model, Many Behaviors: Stockout-Aware Feature Engineering and Dynamic Scaling for Multi-Horizon Retail Demand Forecasting with a Cost-Aware Ordering Policy (VN2 Winner Report)](https://arxiv.org/abs/2601.18919)
*Bartosz Szabłowski*

Main category: cs.LG

TL;DR: VN2库存规划挑战赛获胜方案：两阶段预测-优化流程，结合全局多期预测模型与成本感知订购策略，在官方六轮模拟评估中获第一名


<details>
  <summary>Details</summary>
Motivation: 零售连锁库存规划需要将需求预测转化为订购决策，需考虑不对称缺货成本和持有成本。VN2库存规划挑战赛将此设定为每周决策循环，包含两周产品交付提前期，总成本定义为缺货成本加持有成本

Method: 两阶段预测-优化流程：1) 预测阶段使用全局多期预测模型，采用CatBoost实现的梯度提升决策树作为基础学习器，包含缺货感知特征工程、序列缩放和时间权重；2) 决策阶段将库存投影到交付周开始，计算明确权衡缺货和持有成本的目标库存水平

Result: 在官方竞赛模拟的六轮评估中，该解决方案获得第一名，通过强大的全局预测模型与轻量级成本感知策略的结合实现优异表现

Conclusion: 尽管为VN2设定开发，但所提方法可扩展到实际应用和额外运营约束，展示了全局预测与成本感知决策相结合的有效性

Abstract: Inventory planning for retail chains requires translating demand forecasts into ordering decisions, including asymmetric shortages and holding costs. The VN2 Inventory Planning Challenge formalizes this setting as a weekly decision-making cycle with a two-week product delivery lead time, where the total cost is defined as the shortage cost plus the holding cost. This report presents the winning VN2 solution: a two-stage predict-then-optimize pipeline that combines a single global multi-horizon forecasting model with a cost-aware ordering policy. The forecasting model is trained in a global paradigm, jointly using all available time series. A gradient-boosted decision tree (GBDT) model implemented in CatBoost is used as the base learner. The model incorporates stockout-aware feature engineering to address censored demand during out-of-stock periods, per-series scaling to focus learning on time-series patterns rather than absolute levels, and time-based observation weights to reflect shifts in demand patterns. In the decision stage, inventory is projected to the start of the delivery week, and a target stock level is calculated that explicitly trades off shortage and holding costs. Evaluated by the official competition simulation in six rounds, the solution achieved first place by combining a strong global forecasting model with a lightweight cost-aware policy. Although developed for the VN2 setting, the proposed approach can be extended to real-world applications and additional operational constraints.

</details>


### [69] [Toward Learning POMDPs Beyond Full-Rank Actions and State Observability](https://arxiv.org/abs/2601.18930)
*Seiji Shaw,Travis Manderson,Chad Kessens,Nicholas Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种从动作-观察序列中学习部分可观测马尔可夫决策过程（POMDP）参数的方法，通过结合谱方法和张量分解技术，能够估计观测矩阵和转移矩阵（在状态划分级别上），为下游推理任务提供显式的概率模型。


<details>
  <summary>Details</summary>
Motivation: 研究如何让自主智能体学习和推理具有隐藏状态的系统（如带有隐藏锁定机制的家具）。现有方法存在局限：谱方法（如PSR）能直接估计隐藏状态数量但无法提供转移和观测概率；张量分解方法能估计概率但通常假设完全状态可观测性和满秩转移矩阵。需要一种能放松这些假设的方法。

Method: 结合预测状态表示（PSR）和张量分解方法。PSR学习转移和观测矩阵到相似变换的程度，然后通过张量方法估计这些变换。该方法学习观测矩阵和转移矩阵（在状态划分级别上），其中同一划分内的状态具有相同的观测分布，对应转移矩阵为满秩的动作。

Result: 实验表明，在足够数据量下，该方法学习到的划分级别转移模型在标准基于采样的POMDP求解器中的性能与PSR相当。同时，显式的观测和转移概率可以在模型学习后用于指定规划器行为。

Conclusion: 该方法成功地将PSR的谱学习能力与张量分解的概率估计能力相结合，为学习具有隐藏状态的系统提供了一种有效的框架，既保留了PSR的状态数量估计优势，又提供了下游推理所需的显式概率模型。

Abstract: We are interested in enabling autonomous agents to learn and reason about systems with hidden states, such as furniture with hidden locking mechanisms. We cast this problem as learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP). The agent begins with knowledge of the POMDP's actions and observation spaces, but not its state space, transitions, or observation models. These properties must be constructed from action-observation sequences. Spectral approaches to learning models of partially observable domains, such as learning Predictive State Representations (PSRs), are known to directly estimate the number of hidden states. These methods cannot, however, yield direct estimates of transition and observation likelihoods, which are important for many downstream reasoning tasks. Other approaches leverage tensor decompositions to estimate transition and observation likelihoods but often assume full state observability and full-rank transition matrices for all actions. To relax these assumptions, we study how PSRs learn transition and observation matrices up to a similarity transform, which may be estimated via tensor methods. Our method learns observation matrices and transition matrices up to a partition of states, where the states in a single partition have the same observation distributions corresponding to actions whose transition matrices are full-rank. Our experiments suggest that these partition-level transition models learned by our method, with a sufficient amount of data, meets the performance of PSRs as models to be used by standard sampling-based POMDP solvers. Furthermore, the explicit observation and transition likelihoods can be leveraged to specify planner behavior after the model has been learned.

</details>


### [70] [FSD-CAP: Fractional Subgraph Diffusion with Class-Aware Propagation for Graph Feature Imputation](https://arxiv.org/abs/2601.18938)
*Xin Qiao,Shijie Sun,Anqi Dong,Cong Hua,Xia Zhao,Longfei Zhang,Guangming Zhu,Liang Zhang*

Main category: cs.LG

TL;DR: FSD-CAP是一个两阶段图节点特征补全框架，通过子图扩展局部化扩散和类别感知传播，在99.5%特征缺失的极端稀疏情况下仍能保持接近完整特征的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在表示或全局扩散的图节点特征补全方法在高缺失率下效果不佳，容易产生不可靠估计并在图中传播错误。需要一种能在极端稀疏条件下保持高质量补全的方法。

Method: 两阶段框架：第一阶段使用图距离引导的子图扩展来局部化扩散过程，分数扩散算子根据局部结构调整传播锐度；第二阶段使用类别感知传播细化补全特征，结合伪标签和邻域熵来促进一致性。

Result: 在99.5%特征缺失情况下，节点分类准确率达到80.06%（结构性缺失）和81.01%（均匀缺失），接近完整特征的81.31%；链接预测AUC达到91.65%（结构性）和92.41%（均匀），接近完整特征的95.06%。在大规模和异配性数据集上均优于其他模型。

Conclusion: FSD-CAP通过局部化扩散和类别感知传播，在极端稀疏条件下实现了高质量的特征补全，为高缺失率图数据补全提供了有效解决方案。

Abstract: Imputing missing node features in graphs is challenging, particularly under high missing rates. Existing methods based on latent representations or global diffusion often fail to produce reliable estimates, and may propagate errors across the graph. We propose FSD-CAP, a two-stage framework designed to improve imputation quality under extreme sparsity. In the first stage, a graph-distance-guided subgraph expansion localizes the diffusion process. A fractional diffusion operator adjusts propagation sharpness based on local structure. In the second stage, imputed features are refined using class-aware propagation, which incorporates pseudo-labels and neighborhood entropy to promote consistency. We evaluated FSD-CAP on multiple datasets. With $99.5\%$ of features missing across five benchmark datasets, FSD-CAP achieves average accuracies of $80.06\%$ (structural) and $81.01\%$ (uniform) in node classification, close to the $81.31\%$ achieved by a standard GCN with full features. For link prediction under the same setting, it reaches AUC scores of $91.65\%$ (structural) and $92.41\%$ (uniform), compared to $95.06\%$ for the fully observed case. Furthermore, FSD-CAP demonstrates superior performance on both large-scale and heterophily datasets when compared to other models.

</details>


### [71] [A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy](https://arxiv.org/abs/2601.18939)
*Claire O'Brien,Jessica Seto,Dristi Roy,Aditya Dwivedi,Sunishchal Dev,Kevin Zhu,Sean O'Brien,Ashwinee Panda,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出一种针对大语言模型行为对齐的稀疏神经元更新方法，仅更新与目标行为最相关的少量神经元（约3%），在减少谄媚行为任务上达到或超越SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型行为对齐方法通常采用广泛的微调，这会导致分布偏移和可解释性差等副作用。需要一种更精确、数据效率更高的对齐方法

Method: 使用稀疏自编码器和线性探针识别与目标行为最相关的MLP神经元（约3%），将其解码到残差空间，然后通过梯度掩码仅对这些神经元进行微调

Result: 在减少谄媚行为任务上，该方法在四个基准测试（Syco-Bench、NLP、POLI、PHIL）上匹配或超越了最先进性能，使用Gemma-2-2B和9B模型验证了有效性

Conclusion: 稀疏的神经元级更新为全模型微调提供了可扩展且精确的替代方案，即使在数据有限的情况下也能保持有效性

Abstract: Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior, decode them into residual space, and fine-tune only those neurons using gradient masking. We demonstrate this approach on the task of reducing sycophantic behavior, where our method matches or exceeds state-of-the-art performance on four benchmarks (Syco-Bench, NLP, POLI, PHIL) using Gemma-2-2B and 9B models. Our results show that sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available

</details>


### [72] [Vector-Valued Distributional Reinforcement Learning Policy Evaluation: A Hilbert Space Embedding Approach](https://arxiv.org/abs/2601.18952)
*Mehrdad Mohammadi,Qi Zheng,Ruoqing Zhu*

Main category: cs.LG

TL;DR: 提出KE-DRL框架，利用希尔伯特空间映射估计多维价值分布的内核均值嵌入，替代计算复杂的水距离，实现高效的多维连续状态-动作空间强化学习。


<details>
  <summary>Details</summary>
Motivation: 传统分布强化学习在多维连续状态-动作空间和奖励设置中，直接计算Wasserstein距离计算复杂度高，需要更高效的度量方法。

Method: 通过内核均值嵌入将概率测度映射到再生核希尔伯特空间，用积分概率度量替代Wasserstein度量，建立基于Matern核族的分布贝尔曼算子收缩性质。

Result: 理论证明在Matern核族下分布贝尔曼算子具有收缩性质并提供一致收敛保证，仿真实验显示在Lipschitz连续性和核有界性假设下能稳健进行离策略评估和内核均值嵌入恢复。

Conclusion: 基于嵌入的方法在复杂现实决策场景和风险评估中具有潜力，内核均值嵌入为多维分布强化学习提供了计算高效的理论框架。

Abstract: We propose an (offline) multi-dimensional distributional reinforcement learning framework (KE-DRL) that leverages Hilbert space mappings to estimate the kernel mean embedding of the multi-dimensional value distribution under a proposed target policy. In our setting, the state-action variables are multi-dimensional and continuous. By mapping probability measures into a reproducing kernel Hilbert space via kernel mean embeddings, our method replaces Wasserstein metrics with an integral probability metric. This enables efficient estimation in multi-dimensional state-action spaces and reward settings, where direct computation of Wasserstein distances is computationally challenging. Theoretically, we establish contraction properties of the distributional Bellman operator under our proposed metric involving the Matern family of kernels and provide uniform convergence guarantees. Simulations and empirical results demonstrate robust off-policy evaluation and recovery of the kernel mean embedding under mild assumptions, namely, Lipschitz continuity and boundedness of the kernels, highlighting the potential of embedding-based approaches in complex real-world decision-making scenarios and risk evaluation.

</details>


### [73] [Towards Self-Optimizing Electron Microscope: Robust Tuning of Aberration Coefficients via Physics-Aware Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2601.18972)
*Utkarsh Pratiush,Austin Houston,Richard Liu,Gerd Duscher,Sergei Kalinin*

Main category: cs.LG

TL;DR: 提出基于多目标贝叶斯优化的扫描透射电子显微镜像差校正框架，通过主动学习和帕累托前沿平衡竞争目标，实现快速、数据高效的像差校正。


<details>
  <summary>Details</summary>
Motivation: 传统像差校正方法存在效率低下和灵活性不足的问题：串行梯度搜索方法样本效率低，难以同时校正多个相互作用参数；深度学习方法虽然快速但缺乏适应不同样品条件的灵活性，需要大量重新训练。

Method: 采用多目标贝叶斯优化框架，使用高斯过程回归概率建模像差景观，通过主动学习循环选择信息量最大的透镜设置进行评估，支持用户定义的物理激励奖励函数，利用帕累托前沿揭示竞争实验优先级之间的权衡。

Result: 该方法比传统优化算法更鲁棒，能够有效调谐聚焦、像散和高阶像差，通过平衡竞争目标实现实验期间的动态最优性能维持。

Conclusion: 该多目标贝叶斯优化框架为扫描透射电子显微镜提供了快速、数据高效的像差校正解决方案，通过主动学习和多目标权衡实现"自优化"显微镜，能够在实验中动态维持最优性能。

Abstract: Realizing high-throughput aberration-corrected Scanning Transmission Electron Microscopy (STEM) exploration of atomic structures requires rapid tuning of multipole probe correctors while compensating for the inevitable drift of the optical column. While automated alignment routines exist, conventional approaches rely on serial, gradient-free searches (e.g., Nelder-Mead) that are sample-inefficient and struggle to correct multiple interacting parameters simultaneously. Conversely, emerging deep learning methods offer speed but often lack the flexibility to adapt to varying sample conditions without extensive retraining. Here, we introduce a Multi-Objective Bayesian Optimization (MOBO) framework for rapid, data-efficient aberration correction. Importantly, this framework does not prescribe a single notion of image quality; instead, it enables user-defined, physically motivated reward formulations (e.g., symmetry-induced objectives) and uses Pareto fronts to expose the resulting trade-offs between competing experimental priorities. By using Gaussian Process regression to model the aberration landscape probabilistically, our workflow actively selects the most informative lens settings to evaluate next, rather than performing an exhaustive blind search. We demonstrate that this active learning loop is more robust than traditional optimization algorithms and effectively tunes focus, astigmatism, and higher-order aberrations. By balancing competing objectives, this approach enables "self-optimizing" microscopy by dynamically sustaining optimal performance during experiments.

</details>


### [74] [When Does Adaptation Win? Scaling Laws for Meta-Learning in Quantum Control](https://arxiv.org/abs/2601.18973)
*Nima Leclerc,Chris Miller,Nicholas Brawand*

Main category: cs.LG

TL;DR: 量子硬件存在设备异质性和环境漂移问题，本文推导了元学习的缩放律下界，表明适应增益随梯度步数指数饱和且与任务方差线性相关，为判断适应是否值得提供了量化标准。


<details>
  <summary>Details</summary>
Motivation: 量子硬件存在固有的设备异质性和环境漂移问题，迫使实践者在不理想的自适应控制器和昂贵的每设备重新校准之间做出选择。需要建立量化框架来判断适应策略何时值得其开销。

Method: 推导元学习的缩放律下界，分析适应增益（任务特定梯度步的期望保真度改进）随梯度步数的指数饱和特性以及与任务方差的线性关系。在量子门校准和经典线性二次控制两个领域进行验证。

Result: 在量子门校准中，低方差任务获益可忽略，但在极端分布外条件下（训练噪声10倍），两量子比特门保真度提升超过40%。经典线性二次控制验证表明这些规律源于一般优化几何而非量子特定物理。

Conclusion: 研究结果为自适应控制中的决策提供了可转移的框架，量化了适应开销与收益的权衡，对减少云量子处理器上每设备校准时间有实际意义。

Abstract: Quantum hardware suffers from intrinsic device heterogeneity and environmental drift, forcing practitioners to choose between suboptimal non-adaptive controllers or costly per-device recalibration. We derive a scaling law lower bound for meta-learning showing that the adaptation gain (expected fidelity improvement from task-specific gradient steps) saturates exponentially with gradient steps and scales linearly with task variance, providing a quantitative criterion for when adaptation justifies its overhead. Validation on quantum gate calibration shows negligible benefits for low-variance tasks but $>40\%$ fidelity gains on two-qubit gates under extreme out-of-distribution conditions (10$\times$ the training noise), with implications for reducing per-device calibration time on cloud quantum processors. Further validation on classical linear-quadratic control confirms these laws emerge from general optimization geometry rather than quantum-specific physics. Together, these results offer a transferable framework for decision-making in adaptive control.

</details>


### [75] [Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning](https://arxiv.org/abs/2601.18984)
*Haolin Liu,Dian Yu,Sidi Lu,Yujun Zhou,Rui Liu,Zhenwen Liang,Haitao Mi,Chen-Yu Wei,Dong Yu*

Main category: cs.LG

TL;DR: VPPO是一种新的强化学习方法，使用过程奖励模型仅定位推理路径中的第一个错误，将轨迹划分为已验证的正确前缀和错误后缀，从而提供更稳定的学习信号。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖稀疏的结果奖励，无法对部分成功解决方案中的正确中间步骤给予奖励。过程奖励模型提供细粒度的步骤级监督，但其评分往往嘈杂且难以评估。当前的PRM基准专注于检测推理路径中的第一个错误步骤，这与PRM在RL中的实际使用方式（将其步骤分数视为原始奖励进行最大化）存在不匹配。

Method: 提出可验证前缀策略优化（VPPO），仅使用PRM在RL过程中定位第一个错误。给定一个不正确的rollout，VPPO根据检测到的第一个错误将轨迹划分为已验证的正确前缀和错误后缀，对前者给予奖励，仅在检测到的错误之后应用有针对性的惩罚。

Result: 在多个推理基准测试中，VPPO在Pass@1和Pass@K指标上始终优于稀疏奖励RL和先前的PRM引导基线方法。

Conclusion: VPPO通过使用PRM仅定位第一个错误并相应划分轨迹，提供了稳定、可解释的学习信号，改善了信用分配，从而在推理任务中实现了更好的性能。

Abstract: Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.

</details>


### [76] [Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective](https://arxiv.org/abs/2601.18999)
*Fangzhou Wu,Sandeep Silwal,Qiuyi,Zhang*

Main category: cs.LG

TL;DR: KV缓存优化新方法：结合可证明竞争性随机KV缓存驱逐与基于学习的查询路由，解决多LLM服务中内存限制下的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: KV缓存是加速LLM推理的关键技术，但在有限内存下其效果高度依赖驱逐策略。传统的LRU算法在多LLM服务场景中面临动态查询到达的挑战，平衡查询负载与最大化缓存命中率之间存在固有冲突。

Method: 提出首个统一数学模型捕捉KV缓存驱逐与查询路由之间的核心权衡，开发结合可证明竞争性随机KV缓存驱逐算法与基于学习的自适应查询路由方法，以处理不断变化的查询模式。

Result: 在4个基准测试和3种前缀共享设置下验证，相比最先进方法：缓存命中率提升最高6.92倍，延迟降低11.96倍，首令牌时间降低14.06倍，吞吐量提高77.4%。

Conclusion: 通过理论分析与实验验证，证明了所提方法在多LLM服务场景中能有效平衡查询负载与缓存命中率，显著提升KV缓存性能，为有限内存下的LLM推理加速提供了新解决方案。

Abstract: KV caching is a fundamental technique for accelerating Large Language Model (LLM) inference by reusing key-value (KV) pairs from previous queries, but its effectiveness under limited memory is highly sensitive to the eviction policy. The default Least Recently Used (LRU) eviction algorithm struggles with dynamic online query arrivals, especially in multi-LLM serving scenarios, where balancing query load across workers and maximizing cache hit rate of each worker are inherently conflicting objectives. We give the first unified mathematical model that captures the core trade-offs between KV cache eviction and query routing. Our analysis reveals the theoretical limitations of existing methods and leads to principled algorithms that integrate provably competitive randomized KV cache eviction with learning-based methods to adaptively route queries with evolving patterns, thus balancing query load and cache hit rate. Our theoretical results are validated by extensive experiments across 4 benchmarks and 3 prefix-sharing settings, demonstrating improvements of up to 6.92$\times$ in cache hit rate, 11.96$\times$ reduction in latency, 14.06$\times$ reduction in time-to-first-token (TTFT), and 77.4% increase in throughput over the state-of-the-art methods. Our code is available at https://github.com/fzwark/KVRouting.

</details>


### [77] [Accelerated training of Gaussian processes using banded square exponential covariances](https://arxiv.org/abs/2601.19007)
*Emily C. Ehrhardt,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出一种基于协方差矩阵带状近似的计算高效高斯过程训练方法，通过消除接近零的非对角线元素来降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程训练的计算复杂度较高，特别是协方差矩阵求逆和行列式计算。观察到平方指数协方差矩阵中存在大量接近零的非对角线元素，这为降低计算复杂度提供了机会

Method: 提出一种原则性方法消除协方差矩阵中接近零的非对角线元素，构建带状矩阵近似。该方法保持原始协方差结构，在1D设置下使用平方指数核进行理论分析，实现高效似然函数近似

Result: 带状矩阵近似能显著降低协方差矩阵求逆和行列式计算的计算成本。与变分自由能稀疏高斯过程方法相比，验证了计算效率优势

Conclusion: 通过消除接近零的非对角线元素构建带状协方差矩阵近似是一种有效的高斯过程训练加速方法，在保持结构的同时显著降低计算复杂度

Abstract: We propose a novel approach to computationally efficient GP training based on the observation that square-exponential (SE) covariance matrices contain several off-diagonal entries extremely close to zero. We construct a principled procedure to eliminate those entries to produce a \emph{banded}-matrix approximation to the original covariance, whose inverse and determinant can be computed at a reduced computational cost, thus contributing to an efficient approximation to the likelihood function. We provide a theoretical analysis of the proposed method to preserve the structure of the original covariance in the 1D setting with SE kernel, and validate its computational efficiency against the variational free energy approach to sparse GPs.

</details>


### [78] [EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting](https://arxiv.org/abs/2601.19022)
*Antanas Zilinskas,Robert N. Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: EVEREST：基于Transformer的罕见事件概率预测架构，通过可学习注意力瓶颈、证据头、极值头和轻量级前兆头实现校准预测和尾部风险估计，在空间天气数据上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中罕见事件预测面临严重类别不平衡、长程依赖和分布不确定性的挑战，需要能够提供校准预测、尾部风险估计和可解释性的解决方案。

Method: 提出EVEREST架构，包含四个组件：1) 可学习注意力瓶颈用于软聚合时间动态；2) 证据头通过Normal-Inverse-Gamma分布估计偶然和认知不确定性；3) 极值头使用广义帕累托分布建模尾部风险；4) 轻量级前兆头用于早期事件检测。这些模块通过复合损失（焦点损失、证据NLL和尾部敏感EVT惩罚）联合优化，仅在训练时使用，部署时使用单一分类头。

Result: 在十年空间天气数据上，EVEREST在C级耀斑的24/48/72小时预测中分别达到0.973/0.970/0.966的True Skill Statistic (TSS)，实现最先进性能。模型紧凑（约0.81M参数），可在普通硬件上高效训练。

Conclusion: EVEREST为罕见事件预测提供了校准的概率预测和尾部风险估计，适用于工业监控、天气和卫星诊断等高风险领域。局限性包括依赖固定长度输入和排除基于图像的模态，未来可扩展到流式和多模态预测。

Abstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.

</details>


### [79] [Is Finer Better? The Limits of Microscaling Formats in Large Language Models](https://arxiv.org/abs/2601.19026)
*Andrea Fasoli,Monodeep Kar,Chi-Chun Liu,Swagath Venkataramani,Viji Srinivasan,Leland Chang,Naigang Wang*

Main category: cs.LG

TL;DR: 研究发现微缩量化格式在块尺寸减小时出现性能下降的异常现象，提出使用FP8 UE5M3格式作为FP4微缩数据类型的硬件友好尺度表示方案


<details>
  <summary>Details</summary>
Motivation: 微缩数据格式通过逐块张量量化实现激进模型压缩，但需要硬件友好的实现来充分发挥其在训练和推理中的潜力。研究发现当块尺寸减小到特定阈值以下时，量化模型输出性能反而下降，这与预期相悖，需要深入探究

Method: 通过实验和理论分析相结合的方法：实验上分析多个大语言模型的分布特征，识别驱动异常行为的条件；理论上建立分析框架，与实验数据和理想分布进行对比验证

Result: 发现异常现象由窄张量分布与量化尺度有限动态范围之间的相互作用驱动；提出使用FP8 UE5M3作为FP4微缩数据类型的尺度格式，该方案在性能上与传统的FP8 E4M3尺度相当，同时避免了权重和激活的全局缩放操作

Conclusion: 微缩量化在块尺寸减小时存在性能下降的异常现象，这源于窄分布与有限动态范围的相互作用。提出的FP8 UE5M3尺度格式为硬件友好的微缩量化实现提供了有效解决方案

Abstract: Microscaling data formats leverage per-block tensor quantization to enable aggressive model compression with limited loss in accuracy. Unlocking their potential for efficient training and inference necessitates hardware-friendly implementations that handle matrix multiplications in a native format and adopt efficient error-mitigation strategies. Herein, we report the emergence of a surprising behavior associated with microscaling quantization, whereas the output of a quantized model degrades as block size is decreased below a given threshold. This behavior clashes with the expectation that a smaller block size should allow for a better representation of the tensor elements. We investigate this phenomenon both experimentally and theoretically, decoupling the sources of quantization error behind it. Experimentally, we analyze the distributions of several Large Language Models and identify the conditions driving the anomalous behavior. Theoretically, we lay down a framework showing remarkable agreement with experimental data from pretrained model distributions and ideal ones. Overall, we show that the anomaly is driven by the interplay between narrow tensor distributions and the limited dynamic range of the quantized scales. Based on these insights, we propose the use of FP8 unsigned E5M3 (UE5M3) as a novel hardware-friendly format for the scales in FP4 microscaling data types. We demonstrate that UE5M3 achieves comparable performance to the conventional FP8 unsigned E4M3 scales while obviating the need of global scaling operations on weights and activations.

</details>


### [80] [A Unifying View of Coverage in Linear Off-Policy Evaluation](https://arxiv.org/abs/2601.19030)
*Philip Amortila,Audrey Huang,Akshay Krishnamurthy,Nan Jiang*

Main category: cs.LG

TL;DR: 本文针对线性离策略评估(OPE)问题，提出了一种新的有限样本分析框架，引入特征动态覆盖这一新覆盖参数，为不同假设下的覆盖定义提供了统一理解。


<details>
  <summary>Details</summary>
Motivation: 当前线性OPE的有限样本保证通常依赖于覆盖参数C^π，但在目标值函数线性可实现的极小设定下，合适的覆盖定义仍不明确。现有文献中的候选定义存在不良特性，且与标准定义脱节，需要统一的覆盖理解框架。

Method: 采用工具变量视角，对经典算法LSTDQ进行新颖的有限样本分析。引入特征动态覆盖这一新参数，该参数可解释为特征演化诱导动态系统中的线性覆盖。在贝尔曼完备性等进一步假设下，该定义能恢复特定设定下的覆盖参数。

Result: 建立了依赖于特征动态覆盖的误差界，该覆盖参数为线性OPE提供了统一的覆盖概念。在贝尔曼完备性等假设下，成功恢复了针对特定设定的覆盖参数，实现了线性OPE中覆盖定义的统一理解。

Conclusion: 特征动态覆盖为线性离策略评估提供了统一的覆盖概念框架，解决了现有覆盖定义碎片化的问题，在不同假设下都能提供一致的覆盖理解，为线性OPE的统计率刻画奠定了基础。

Abstract: Off-policy evaluation (OPE) is a fundamental task in reinforcement learning (RL). In the classic setting of linear OPE, finite-sample guarantees often take the form $$ \textrm{Evaluation error} \le \textrm{poly}(C^π, d, 1/n,\log(1/δ)), $$ where $d$ is the dimension of the features and $C^π$ is a coverage parameter that characterizes the degree to which the visited features lie in the span of the data distribution. While such guarantees are well-understood for several popular algorithms under stronger assumptions (e.g. Bellman completeness), the understanding is lacking and fragmented in the minimal setting where only the target value function is linearly realizable in the features. Despite recent interest in tight characterizations of the statistical rate in this setting, the right notion of coverage remains unclear, and candidate definitions from prior analyses have undesirable properties and are starkly disconnected from more standard definitions in the literature.
  We provide a novel finite-sample analysis of a canonical algorithm for this setting, LSTDQ. Inspired by an instrumental-variable view, we develop error bounds that depend on a novel coverage parameter, the feature-dynamics coverage, which can be interpreted as linear coverage in an induced dynamical system for feature evolution. With further assumptions -- such as Bellman-completeness -- our definition successfully recovers the coverage parameters specialized to those settings, finally yielding a unified understanding for coverage in linear OPE.

</details>


### [81] [Unravelling the (In)compatibility of Statistical-Parity and Equalized-Odds](https://arxiv.org/abs/2601.19035)
*Mortaza S. Bargh,Sunil Choenni,Floris ter Braak*

Main category: cs.LG

TL;DR: 该研究分析了统计公平性度量中统计均等与均衡几率之间的关系，重点探讨了基础率不平衡如何导致这两个度量标准的不兼容性。


<details>
  <summary>Details</summary>
Motivation: 在数据、算法和数据驱动系统中，公平性和正义原则的遵循是一个关键挑战。统计公平性度量是检测数据和算法公平性问题的重要技术/形式机制。统计均等度量不依赖真实标签，因此在实践中被广泛采用，而均衡几率度量需要可靠的真实标签，但在某些情况下必须满足以确保敏感社会群体间的错误预测均等。需要理解这两个度量之间的关系及其在实际应用中的权衡。

Method: 提出了一种新颖的分析方法，研究统计均等与均衡几率之间的关系，特别关注敏感群体的基础率（base-rates）对这两个度量兼容性的影响。通过分析基础率不平衡如何导致统计均等与均衡几率之间的不兼容性。

Result: 分析直观地展示了基础率不平衡如何以及何时导致统计均等与均衡几率度量之间的不兼容性。该研究为实践中在这两个度量之间进行权衡提供了见解，并建议在强制执行或依赖统计均等准则之前，应检查基础率平衡性并调查这种不兼容性的可能性。

Conclusion: 该研究揭示了统计均等与均衡几率之间的关系，特别强调了基础率不平衡对这两个公平性度量兼容性的影响。研究结果呼吁在采用统计均等准则前，应检查基础率平衡性并评估不兼容风险。这些见解可能推动改进当前实践和现有法律框架的倡议。

Abstract: A key challenge in employing data, algorithms and data-driven systems is to adhere to the principle of fairness and justice. Statistical fairness measures belong to an important category of technical/formal mechanisms for detecting fairness issues in data and algorithms. In this contribution we study the relations between two types of statistical fairness measures namely Statistical-Parity and Equalized-Odds. The Statistical-Parity measure does not rely on having ground truth, i.e., (objectively) labeled target attributes. This makes Statistical-Parity a suitable measure in practice for assessing fairness in data and data classification algorithms. Therefore, Statistical-Parity is adopted in many legal and professional frameworks for assessing algorithmic fairness. The Equalized-Odds measure, on the contrary, relies on having (reliable) ground-truth, which is not always feasible in practice. Nevertheless, there are several situations where the Equalized-Odds definition should be satisfied to enforce false prediction parity among sensitive social groups. We present a novel analyze of the relation between Statistical-Parity and Equalized-Odds, depending on the base-rates of sensitive groups. The analysis intuitively shows how and when base-rate imbalance causes incompatibility between Statistical-Parity and Equalized-Odds measures. As such, our approach provides insight in (how to make design) trade-offs between these measures in practice. Further, based on our results, we plea for examining base-rate (im)balance and investigating the possibility of such an incompatibility before enforcing or relying on the Statistical-Parity criterion. The insights provided, we foresee, may trigger initiatives to improve or adjust the current practice and/or the existing legal frameworks.

</details>


### [82] [OATS: Online Data Augmentation for Time Series Foundation Models](https://arxiv.org/abs/2601.19040)
*Junwei Deng,Chang Xu,Jiaqi W. Ma,Ming Jin,Chenghao Liu,Jiang Bian*

Main category: cs.LG

TL;DR: OATS：面向时间序列基础模型的在线数据增强方法，通过动态生成与训练阶段匹配的合成数据来提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列数据增强方法通常基于启发式规则和静态范式，而动态数据优化研究表明样本贡献在不同训练阶段存在差异，因此需要针对不同训练步骤定制合成数据

Method: 提出OATS框架：1）利用有价值的训练样本作为指导信号；2）基于扩散模型生成逼真的时间序列数据；3）引入探索-利用机制平衡效率与效果；4）动态生成与训练阶段匹配的高质量合成数据

Result: 在六个验证数据集和两种TSFM架构上，OATS持续优于常规训练，并在静态数据增强基线方法上取得显著性能提升

Conclusion: OATS为时间序列基础模型提供了一种原则性的在线数据增强策略，通过动态生成与训练阶段匹配的合成数据，有效提升了模型性能

Abstract: Time Series Foundation Models (TSFMs) are a powerful paradigm for time series analysis and are often enhanced by synthetic data augmentation to improve the training data quality. Existing augmentation methods, however, typically rely on heuristics and static paradigms. Motivated by dynamic data optimization, which shows that the contribution of samples varies across training stages, we propose OATS (Online Data Augmentation for Time Series Foundation Models), a principled strategy that generates synthetic data tailored to different training steps. OATS leverages valuable training samples as principled guiding signals and dynamically generates high-quality synthetic data conditioned on them. We further design a diffusion-based framework to produce realistic time series and introduce an explore-exploit mechanism to balance efficiency and effectiveness. Experiments on TSFMs demonstrate that OATS consistently outperforms regular training and yields substantial performance gains over static data augmentation baselines across six validation datasets and two TSFM architectures. The code is available at the link https://github.com/microsoft/TimeCraft.

</details>


### [83] [Speed is Confidence](https://arxiv.org/abs/2601.19085)
*Joshua V. Dillon*

Main category: cs.LG

TL;DR: 论文提出基于"首个信号行动"生物原理的快速推理方法，通过仅使用最先停止的模型预测而非平均预测，在Sudoku-Extreme上实现97.2%准确率且计算量减少10倍。训练时通过维护K=4并行潜在状态但仅反向传播最低损失的"获胜者"，单模型单次前向即可达到96.9%准确率，匹配测试时增强性能。


<details>
  <summary>Details</summary>
Motivation: 生物神经系统需要快速但受能量约束，进化解决方案是"基于首个信号行动"。论文旨在将这一原理应用于Tiny Recursive Models (TRM)集成，通过将推理速度作为置信度的隐式指示，实现高效准确的推理。

Method: 1) 推理方法：基于集成中首个停止的模型进行预测，而非平均预测；2) 训练方法：维护K=4个并行潜在状态，训练时仅通过最低损失的"获胜者"进行反向传播；3) 技术改进：提出改进的SwiGLU激活函数使Muon模型可行；4) 资源约束：所有实验在单张RTX 5090上完成。

Result: 在Sudoku-Extreme上：1) 集成方法达到97.2%谜题准确率，比测试时增强(TTA)基线减少10倍计算量（基线单次通过86.1%，TTA 97.3%）；2) 单模型单次前向通过达到96.9% ± 0.6%准确率，匹配TTA性能；3) 训练效率：K=1训练7k步（40分钟）即超越TRM基线，更高精度需要36k步（K=1需1.5小时，K=4需6小时）。

Conclusion: 论文成功将生物神经系统的"首个信号行动"原理应用于深度学习，证明推理速度可作为置信度的有效指示。通过仅使用最先停止的模型预测，实现了计算效率的大幅提升。训练时维护并行潜在状态但仅反向传播最低损失路径的方法，使单模型能够匹配集成方法的性能，为资源受限环境下的高效推理提供了新思路。

Abstract: Biological neural systems must be fast but are energy-constrained. Evolution's solution: act on the first signal. Winner-take-all circuits and time-to-first-spike coding implicitly treat when a neuron fires as an expression of confidence. We apply this principle to ensembles of Tiny Recursive Models (TRM). By basing the ensemble prediction solely on the first to halt rather than averaging predictions, we achieve 97.2% puzzle accuracy on Sudoku-Extreme while using 10x less compute than test-time augmentation (the baseline achieves 86.1% single-pass, 97.3% with TTA). Inference speed is an implicit indication of confidence. But can this capability be manifested as a training-only cost? Evidently yes: by maintaining K = 4 parallel latent states during training but backpropping only through the lowest-loss "winner," a single model achieves 96.9% +/- 0.6% puzzle accuracy with a single forward pass-matching TTA performance without any test-time augmentation. As in nature, this work was also resource constrained: all experimentation used a single RTX 5090. This necessitated efficiency and compelled our invention of a modified SwiGLU which made Muon viable. With Muon and K = 1 training, we exceed TRM baseline performance in 7k steps (40 min). Higher accuracy requires 36k steps: 1.5 hours for K = 1, 6 hours for K = 4.

</details>


### [84] [EPAS: Efficient Training with Progressive Activation Sharing](https://arxiv.org/abs/2601.19089)
*Rezaul Karim,Maryam Dialameh,Yang Liu,Boxing Chen,Walid Ahmed*

Main category: cs.LG

TL;DR: EPAS是一种高效的渐进激活共享训练方法，通过逐步将解码器层切换到激活共享模式来减少计算量，在LLaMA模型上实现了最高11.1%的训练吞吐量提升和29%的推理吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 动机是利用Transformer深层中存在的冗余QK（或KV）激活现象，通过渐进式训练范式减少计算开销，同时保持模型性能。

Method: EPAS方法在训练过程中逐步扩展激活共享区域，从模型深层开始向浅层扩展，将解码器层切换到激活共享模式，从而减少计算量。该方法支持在推理时根据计算预算调整激活共享区域长度。

Result: 在125M到7B参数的LLaMA模型上，QK激活共享实现了最高11.1%的训练吞吐量提升和29%的推理吞吐量提升，同时保持与基线模型相似的损失曲线。在TinyLLaMA的持续预训练中，EPAS使注意力共享模型的平均准确率比最先进方法提高了10%。

Conclusion: EPAS通过渐进式训练范式有效利用了Transformer深层激活冗余，显著提升了训练和推理效率，证明了渐进式训练在跨层激活共享模型中的重要性。

Abstract: We present a novel method for Efficient training with Progressive Activation Sharing (EPAS). This method bridges progressive training paradigm with the phenomenon of redundant QK (or KV ) activations across deeper layers of transformers. EPAS gradually grows a sharing region during training by switching decoder layers to activation sharing mode. This results in throughput increase due to reduced compute. To utilize deeper layer redundancy, the sharing region starts from the deep end of the model and grows towards the shallow end. The EPAS trained models allow for variable region lengths of activation sharing for different compute budgets during inference. Empirical evaluations with QK activation sharing in LLaMA models ranging from 125M to 7B parameters show up to an 11.1% improvement in training throughput and up to a 29% improvement in inference throughput while maintaining similar loss curve to the baseline models. Furthermore, applying EPAS in continual pretraining to transform TinyLLaMA into an attention-sharing model yields up to a 10% improvement in average accuracy over state-of-the-art methods, emphasizing the significance of progressive training in cross layer activation sharing models.

</details>


### [85] [Privacy-Preserving Model Transcription with Differentially Private Synthetic Distillation](https://arxiv.org/abs/2601.19090)
*Bochao Liu,Shiming Ge,Pengju Wang,Shikun Li,Tongliang Liu*

Main category: cs.LG

TL;DR: 提出一种无数据的隐私保护模型转录方法，通过差分隐私合成蒸馏将预训练模型转换为隐私保护的学生模型，无需访问原始私有数据。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在私有数据集上训练后部署存在隐私泄露风险，攻击者可能从模型中恢复敏感数据或标签信息。需要一种无需原始数据即可将预训练模型转换为隐私保护版本的方法。

Method: 提出差分隐私合成蒸馏方法，采用合作-竞争学习框架，包含三个参与者：生成器、教师模型和学生模型。通过交替优化：1)生成器学习生成合成数据；2)教师和学生处理合成数据并计算差分隐私标签；3)学生用噪声标签更新，生成器以学生为判别器进行对抗训练。

Result: 理论证明方法能保证差分隐私和收敛性。转录的学生模型具有良好的性能和隐私保护能力，生成的合成数据可用于下游任务。实验表明方法优于26个最先进方法。

Conclusion: 提出的隐私保护模型转录方法通过无数据的模型到模型转换，在保证差分隐私的同时维持模型性能，为安全模型部署提供了有效解决方案。

Abstract: While many deep learning models trained on private datasets have been deployed in various practical tasks, they may pose a privacy leakage risk as attackers could recover informative data or label knowledge from models. In this work, we present \emph{privacy-preserving model transcription}, a data-free model-to-model conversion solution to facilitate model deployment with a privacy guarantee. To this end, we propose a cooperative-competitive learning approach termed \emph{differentially private synthetic distillation} that learns to convert a pretrained model (teacher) into its privacy-preserving counterpart (student) via a trainable generator without access to private data. The learning collaborates with three players in a unified framework and performs alternate optimization: i)~the generator is learned to generate synthetic data, ii)~the teacher and student accept the synthetic data and compute differential private labels by flexible data or label noisy perturbation, and iii)~the student is updated with noisy labels and the generator is updated by taking the student as a discriminator for adversarial training. We theoretically prove that our approach can guarantee differential privacy and convergence. The transcribed student has good performance and privacy protection, while the resulting generator can generate private synthetic data for downstream tasks. Extensive experiments clearly demonstrate that our approach outperforms 26 state-of-the-arts.

</details>


### [86] [Out-of-Distribution Generalization for Neural Physics Solvers](https://arxiv.org/abs/2601.19091)
*Zhao Wei,Chin Chun Ooi,Jian Cheng Wong,Abhishek Gupta,Pao-Hsiung Chiu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: NOVA是一种可泛化的神经物理求解器，能够在分布偏移下提供快速准确的物理模拟，相比数据驱动基线在分布外误差上降低1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有神经物理求解器在训练分布外泛化能力差，限制了新颖设计和长期预测的探索，需要能够可靠外推到未知机制的能力。

Method: 通过从初始稀疏场景集学习物理对齐表示，实现可泛化的神经物理求解器，能够处理偏微分方程参数、几何和初始条件的分布偏移。

Result: 在热传导、扩散反应和流体流动等复杂非线性问题上，相比数据驱动基线实现1-2个数量级的分布外误差降低；在非线性图灵系统和流体芯片优化中展示了长期动力学稳定性和生成设计改进。

Conclusion: NOVA能够实现超出已知机制的可信外推，这是科学发现中探索新假设空间的关键能力，超越了仅限于先验空间内检索和/或仿真的神经物理求解器。

Abstract: Neural physics solvers are increasingly used in scientific discovery, given their potential for rapid in silico insights into physical, materials, or biological systems and their long-time evolution. However, poor generalization beyond their training support limits exploration of novel designs and long-time horizon predictions. We introduce NOVA, a route to generalizable neural physics solvers that can provide rapid, accurate solutions to scenarios even under distributional shifts in partial differential equation parameters, geometries and initial conditions. By learning physics-aligned representations from an initial sparse set of scenarios, NOVA consistently achieves 1-2 orders of magnitude lower out-of-distribution errors than data-driven baselines across complex, nonlinear problems including heat transfer, diffusion-reaction and fluid flow. We further showcase NOVA's dual impact on stabilizing long-time dynamical rollouts and improving generative design through application to the simulation of nonlinear Turing systems and fluidic chip optimization. Unlike neural physics solvers that are constrained to retrieval and/or emulation within an a priori space, NOVA enables reliable extrapolation beyond known regimes, a key capability given the need for exploration of novel hypothesis spaces in scientific discovery

</details>


### [87] [Safe Exploration via Policy Priors](https://arxiv.org/abs/2601.19612)
*Manuel Wendl,Yarden As,Manish Prajapat,Anton Pollak,Stelian Coros,Andreas Krause*

Main category: cs.LG

TL;DR: SOOPER是一种安全的强化学习方法，利用次优但保守的策略作为先验，通过概率动力学模型进行乐观探索，同时在需要时悲观地回退到保守策略，保证学习过程中的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在真实环境中在线学习和适应的安全探索问题，超越受控模拟环境的限制。

Method: 利用离线数据或模拟器获得的次优但保守策略作为先验，使用概率动力学模型进行乐观探索，同时在需要时悲观地回退到保守策略先验。

Result: SOOPER在学习过程中保证安全性，并通过累积遗憾边界证明收敛到最优策略。在关键安全RL基准测试和真实硬件实验中，SOOPER具有可扩展性，性能优于现有最优方法，并验证了理论保证。

Conclusion: SOOPER是一种有效的安全强化学习方法，能够利用保守策略先验实现安全探索，保证学习过程的安全性并收敛到最优策略，在理论和实践中都表现出色。

Abstract: Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.

</details>


### [88] [OWLEYE: Zero-Shot Learner for Cross-Domain Graph Data Anomaly Detection](https://arxiv.org/abs/2601.19102)
*Lecheng Zheng,Dongqi Fu,Zihao Li,Jingrui He*

Main category: cs.LG

TL;DR: OWLEYE是一个零样本图异常检测框架，通过跨域特征对齐、多域多模式字典学习和截断注意力重建模块，实现在未见图数据上的异常检测而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 面对大规模多领域的图数据，现有方法难以开发能够检测未见图中异常的通用基础模型。跨域图数据的特征语义和维度差异严重阻碍了图基础模型的发展，使得持续学习和推理能力成为一个开放问题。

Method: 1) 跨域特征对齐模块：协调不同图的特征分布，在保持领域特定语义的同时进行对齐；2) 多域多模式字典学习：编码共享的结构和属性模式以实现持续学习能力；3) 截断注意力重建模块：基于上下文学习，无需标注数据即可在未见图数据上稳健检测异常。

Result: 在真实世界数据集上的大量实验表明，OWLEYE相比最先进的基线方法实现了优越的性能和泛化能力，为可扩展和标签高效的异常检测建立了坚实基础。

Conclusion: OWLEYE通过创新的特征对齐、字典学习和重建机制，成功解决了跨域图异常检测中的特征语义差异问题，实现了零样本检测能力，为图基础模型的发展提供了重要进展。

Abstract: Graph data is informative to represent complex relationships such as transactions between accounts, communications between devices, and dependencies among machines or processes. Correspondingly, graph anomaly detection (GAD) plays a critical role in identifying anomalies across various domains, including finance, cybersecurity, manufacturing, etc. Facing the large-volume and multi-domain graph data, nascent efforts attempt to develop foundational generalist models capable of detecting anomalies in unseen graphs without retraining. To the best of our knowledge, the different feature semantics and dimensions of cross-domain graph data heavily hinder the development of the graph foundation model, leaving further in-depth continual learning and inference capabilities a quite open problem. Hence, we propose OWLEYE, a novel zero-shot GAD framework that learns transferable patterns of normal behavior from multiple graphs, with a threefold contribution. First, OWLEYE proposes a cross-domain feature alignment module to harmonize feature distributions, which preserves domain-specific semantics during alignment. Second, with aligned features, to enable continuous learning capabilities, OWLEYE designs the multi-domain multi-pattern dictionary learning to encode shared structural and attribute-based patterns. Third, for achieving the in-context learning ability, OWLEYE develops a truncated attention-based reconstruction module to robustly detect anomalies without requiring labeled data for unseen graph-structured data. Extensive experiments on real-world datasets demonstrate that OWLEYE achieves superior performance and generalizability compared to state-of-the-art baselines, establishing a strong foundation for scalable and label-efficient anomaly detection.

</details>


### [89] [Native LLM and MLLM Inference at Scale on Apple Silicon](https://arxiv.org/abs/2601.19139)
*Wayner Barrios*

Main category: cs.LG

TL;DR: vllm-mlx：基于MLX的苹果芯片原生推理框架，针对文本和多模态模型优化，相比llama.cpp提升21-87%吞吐量，通过内容哈希前缀缓存消除冗余视觉编码，实现28倍多模态加速。


<details>
  <summary>Details</summary>
Motivation: 苹果芯片在机器学习开发中的普及催生了对其统一内存架构高效推理解决方案的需求。现有工具要么缺乏原生优化（PyTorch MPS），要么仅专注于文本模型（llama.cpp），导致多模态工作负载得不到充分支持。

Method: 基于MLX原生构建苹果芯片推理框架。文本模型方面实现连续批处理，支持16个并发请求时达到4.3倍聚合吞吐量。多模态模型方面引入基于内容的前缀缓存，通过内容哈希识别相同图像，消除冗余视觉编码，无论输入格式如何。

Result: 在苹果M4 Max上评估：文本模型吞吐量最高达525 tokens/秒，相比llama.cpp提升21-87%（Qwen3-0.6B到Nemotron-30B）。多模态模型重复图像查询实现28倍加速，延迟从21.7秒降至1秒以内。64帧视频分析达到24.7倍缓存加速。

Conclusion: vllm-mlx为苹果芯片提供了高效的文本和多模态模型推理解决方案，通过原生MLX优化和创新的内容哈希缓存机制显著提升性能，已开源支持消费级苹果硬件的高效推理。

Abstract: The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models (llama.cpp), leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.

</details>


### [90] [A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction](https://arxiv.org/abs/2601.19175)
*Jinkyu Sung,Myunggeum Jee,Joonseok Lee*

Main category: cs.LG

TL;DR: 提出一种基于高斯copula的符号图链接符号预测方法，通过边嵌入的Gramian表示和相关矩阵，显著降低计算复杂度并实现线性收敛。


<details>
  <summary>Details</summary>
Motivation: 符号图中的负边违反了图同质性假设，传统图方法需要辅助结构来处理。需要直接建模边之间的潜在统计依赖关系，但朴素建模边-边关系在中等规模图上计算不可行。

Method: 1) 将相关矩阵表示为边嵌入的Gramian，大幅减少参数数量；2) 重新形式化条件概率分布，显著降低推理成本。扩展CopulaGNN，基于高斯copula建模边间统计依赖。

Result: 理论证明方法具有线性收敛性。实验表明比基线方法收敛速度显著更快，同时保持与最先进模型相当的预测性能。

Conclusion: 提出的方法通过高效建模边间依赖关系，解决了符号图链接符号预测的计算复杂度问题，实现了可扩展且性能优异的解决方案。

Abstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN. However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models.

</details>


### [91] [Learning Ordered Representations in Latent Space for Intrinsic Dimension Estimation via Principal Component Autoencoder](https://arxiv.org/abs/2601.19179)
*Qipeng Zhan,Zhuoping Zhou,Zexuan Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出一种结合非均匀方差正则化和等距约束的新型自编码器框架，作为PCA的自然非线性扩展，保持有序表示和方差保留特性


<details>
  <summary>Details</summary>
Motivation: 传统线性自编码器(LAEs)通过非均匀ℓ₂正则化或损失函数调整可以恢复PCA的有序主轴对齐主成分，但这些方法在非线性设置中不足，因为剩余方差无法独立于非线性映射被适当捕获

Method: 提出新型自编码器框架，集成非均匀方差正则化与等距约束，作为PCA的自然泛化

Result: 该设计能够保持PCA的关键优势，如有序表示和方差保留，同时在非线性降维任务中保持有效性

Conclusion: 提出的框架成功将PCA的有序性和方差保留特性扩展到非线性领域，解决了传统方法在非线性设置中的局限性

Abstract: Autoencoders have long been considered a nonlinear extension of Principal Component Analysis (PCA). Prior studies have demonstrated that linear autoencoders (LAEs) can recover the ordered, axis-aligned principal components of PCA by incorporating non-uniform $\ell_2$ regularization or by adjusting the loss function. However, these approaches become insufficient in the nonlinear setting, as the remaining variance cannot be properly captured independently of the nonlinear mapping. In this work, we propose a novel autoencoder framework that integrates non-uniform variance regularization with an isometric constraint. This design serves as a natural generalization of PCA, enabling the model to preserve key advantages, such as ordered representations and variance retention, while remaining effective for nonlinear dimensionality reduction tasks.

</details>


### [92] [Foresight Learning for SEC Risk Prediction](https://arxiv.org/abs/2601.19189)
*Benjamin Turtel,Paul Wilczewski,Danny Franklin,Kris Skotheim*

Main category: cs.LG

TL;DR: 开发了从SEC风险披露中自动生成监督数据并训练小型LLM预测风险实现概率的方法，性能优于GPT-5等前沿模型


<details>
  <summary>Details</summary>
Motivation: SEC文件中的风险披露通常只描述潜在不利事件而不量化其可能性，缺乏大规模的风险级监督数据将披露风险与实际结果联系起来，限制了概率分析的有用性

Method: 引入全自动数据生成流程，将定性SEC风险披露转换为基于时间的监督数据：从风险因素部分生成公司特定、时间限定的风险查询，并通过自动解析后续披露结果进行标注；使用该数据集训练紧凑的大型语言模型来估计披露风险在指定时间内实现的概率

Result: 尽管模型规模适中，但在概率准确性和校准方面显著优于预训练和启发式基线，并超越了包括GPT-5在内的前沿通用模型；模型可在单个GPU上部署，实现前沿性能

Conclusion: 前瞻性学习能够仅使用原始、按时间顺序排列的领域内文本，无需专有数据、外部语料库或手动标注，即可实现领域特定专家模型的可扩展、全自动训练；这为从自然产生的企业文档中学习校准的、与决策相关的信号提供了一条通用路径

Abstract: Risk disclosures in SEC filings describe potential adverse events but rarely quantify their likelihood, limiting their usefulness for probabilistic analysis. A central obstacle is the absence of large-scale, risk-level supervision linking disclosed risks to realized outcomes.
  We introduce a fully automated data generation pipeline that converts qualitative SEC risk disclosures into temporally grounded supervision using only public data. For each filing, the pipeline generates firm-specific, time-bounded risk queries from the Risk Factors section and labels them by automatically resolving outcomes against subsequent disclosures.
  Using this dataset of risk queries and outcomes grounded in SEC filings, we train a compact large language model to estimate the probability that a disclosed risk will materialize within a specified horizon. Despite its modest size, the resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier general-purpose models, including GPT-5, on probabilistic accuracy and calibration.
  More broadly, this work demonstrates that Foresight Learning enables scalable and fully automated training of domain-specific expert models using only raw, chronological, in-domain text -- without proprietary data, external corpora, or manual annotation. The resulting models achieve frontier-level performance while remaining deployable on a single GPU. This result suggests a general pathway for learning calibrated, decision-relevant signals from naturally occurring enterprise documents.
  To support transparency and reproducibility, we open-source the evaluation dataset used in this study.
  Evaluation Data: https://huggingface.co/datasets/LightningRodLabs/sec_risk_questions_test_set
  Data Generation Platform: https://lightningrod.ai/
  SDK: https://github.com/lightning-rod-labs/lightningrod-python-sdk

</details>


### [93] [Accelerated Multiple Wasserstein Gradient Flows for Multi-objective Distributional Optimization](https://arxiv.org/abs/2601.19220)
*Dai Hai Nguyen,Duc Dung Nguyen,Atsuyoshi Nakamura,Hiroshi Mamitsuka*

Main category: cs.LG

TL;DR: 提出A-MWGraD算法，这是MWGraD的加速版本，用于Wasserstein空间中的多目标优化，通过Nesterov加速技术提高了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在Wasserstein空间中，现有的多目标优化算法MWGraD收敛速度较慢（O(1/t)），需要开发更快的算法来提高优化效率。

Method: 提出A-MWGraD算法，基于Nesterov加速技术，分析其连续时间动力学，并开发了实用的基于核的离散化方法。

Result: 理论证明A-MWGraD在测地凸目标下达到O(1/t²)收敛率，在β-强测地凸目标下达到O(e^{-√βt})收敛率，优于MWGraD的O(1/t)。数值实验显示在收敛速度和采样效率上均优于MWGraD。

Conclusion: A-MWGraD是MWGraD的有效加速版本，显著提高了Wasserstein空间中多目标优化的收敛速度，在理论和实践上都有优势。

Abstract: We study multi-objective optimization over probability distributions in Wasserstein space. Recently, Nguyen et al. (2025) introduced Multiple Wasserstein Gradient Descent (MWGraD) algorithm, which exploits the geometric structure of Wasserstein space to jointly optimize multiple objectives. Building on this approach, we propose an accelerated variant, A-MWGraD, inspired by Nesterov's acceleration. We analyze the continuous-time dynamics and establish convergence to weakly Pareto optimal points in probability space. Our theoretical results show that A-MWGraD achieves a convergence rate of O(1/t^2) for geodesically convex objectives and O(e^{-\sqrtβt}) for $β$-strongly geodesically convex objectives, improving upon the O(1/t) rate of MWGraD in the geodesically convex setting. We further introduce a practical kernel-based discretization for A-MWGraD and demonstrate through numerical experiments that it consistently outperforms MWGraD in convergence speed and sampling efficiency on multi-target sampling tasks.

</details>


### [94] [Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model](https://arxiv.org/abs/2601.19232)
*Qi Si,Xuyang Liu,Penglei Wang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.LG

TL;DR: 提出SOLD框架，结合强化学习和潜在扩散模型优化RNA逆折叠，通过单步噪声优化实现多结构目标的高效精炼，超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有RNA逆折叠方法主要关注序列恢复，难以处理非可微的结构目标（如二级结构一致性、最小自由能、局部距离差异测试），导致结构准确性不足

Method: 提出SOLD框架：1) 集成潜在扩散模型，使用RNA-FM预训练嵌入捕获共进化模式；2) 结合强化学习，通过策略驱动的奖励优化处理非可微结构目标；3) 采用单步噪声优化策略，无需采样完整扩散轨迹

Result: SOLD在各项指标上均超越其LDM基线和最先进方法，显著提高了RNA逆折叠的结构准确性

Conclusion: SOLD为RNA逆折叠建立了稳健框架，对生物技术和治疗应用具有深远意义，展示了强化学习与扩散模型结合在处理复杂结构优化问题上的优势

Abstract: RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.

</details>


### [95] [LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection](https://arxiv.org/abs/2601.19255)
*Haoting Zhang,Shekhar Jain*

Main category: cs.LG

TL;DR: 提出一个利用大语言模型将人类专业知识编码为可解释逻辑规则的供应链时间序列异常检测框架，在准确性和可解释性上优于无监督方法，且比直接使用LLM更高效、一致


<details>
  <summary>Details</summary>
Motivation: 供应链管理中传统无监督异常检测结果与业务需求脱节，而专家手动分析无法扩展到数百万产品。需要一种既能编码专业知识又能实现规模化自动化的方法

Method: 三阶段框架：1) LLM基于领域知识标注训练数据；2) 通过LLM驱动的优化自动生成并迭代改进符号规则；3) 用LLM支持的业务相关异常类别增强规则可解释性

Result: 该方法在检测准确性和可解释性上均优于无监督学习方法，相比直接使用LLM进行异常检测，提供了一致、确定性的结果，计算延迟和成本更低，适合生产部署

Conclusion: 该框架展示了LLM如何弥合运营环境中规模化自动化与专家驱动决策之间的差距，为供应链时间序列异常检测提供了高效、可解释的解决方案

Abstract: Time series anomaly detection is critical for supply chain management to take proactive operations, but faces challenges: classical unsupervised anomaly detection based on exploiting data patterns often yields results misaligned with business requirements and domain knowledge, while manual expert analysis cannot scale to millions of products in the supply chain. We propose a framework that leverages large language models (LLMs) to systematically encode human expertise into interpretable, logic-based rules for detecting anomaly patterns in supply chain time series data. Our approach operates in three stages: 1) LLM-based labeling of training data instructed by domain knowledge, 2) automated generation and iterative improvements of symbolic rules through LLM-driven optimization, and 3) rule augmentation with business-relevant anomaly categories supported by LLMs to enhance interpretability. The experiment results showcase that our approach outperforms the unsupervised learning methods in both detection accuracy and interpretability. Furthermore, compared to direct LLM deployment for time series anomaly detection, our approach provides consistent, deterministic results with low computational latency and cost, making it ideal for production deployment. The proposed framework thus demonstrates how LLMs can bridge the gap between scalable automation and expert-driven decision-making in operational settings.

</details>


### [96] [E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification](https://arxiv.org/abs/2601.19256)
*Zhiyang Liang,Qingkai Zhang*

Main category: cs.LG

TL;DR: 提出E-QRGMM框架，通过立方埃尔米特插值和梯度估计加速分位数回归生成元建模，显著提升计算效率，同时保持收敛率，为协变量依赖的不确定性量化提供实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 基于仿真的推理中协变量依赖的不确定性量化对高风险决策至关重要，但现有方法（如保形预测和经典自助法）在协变量特定条件化方面存在局限，需要更高效准确的解决方案。

Method: 提出E-QRGMM框架，将立方埃尔米特插值与梯度估计相结合，加速分位数回归生成元建模过程，显著降低网格复杂度，同时保持原始QRGMM的收敛特性。

Result: 理论上证明E-QRGMM保持原始QRGMM收敛率，同时将多数分位数水平的网格复杂度从O(n^{1/2})降至O(n^{1/5})；实证显示在合成和实际数据集上，相比QRGMM和其他深度生成模型，在分布准确性和训练速度间取得更优权衡。

Conclusion: E-QRGMM为协变量依赖的不确定性量化提供高效实用解决方案，通过自助法构建任意估计量的置信区间，在高风险决策支持中具有重要应用价值。

Abstract: Covariate-dependent uncertainty quantification in simulation-based inference is crucial for high-stakes decision-making but remains challenging due to the limitations of existing methods such as conformal prediction and classical bootstrap, which struggle with covariate-specific conditioning. We propose Efficient Quantile-Regression-Based Generative Metamodeling (E-QRGMM), a novel framework that accelerates the quantile-regression-based generative metamodeling (QRGMM) approach by integrating cubic Hermite interpolation with gradient estimation. Theoretically, we show that E-QRGMM preserves the convergence rate of the original QRGMM while reducing grid complexity from $O(n^{1/2})$ to $O(n^{1/5})$ for the majority of quantile levels, thereby substantially improving computational efficiency. Empirically, E-QRGMM achieves a superior trade-off between distributional accuracy and training speed compared to both QRGMM and other advanced deep generative models on synthetic and practical datasets. Moreover, by enabling bootstrap-based construction of confidence intervals for arbitrary estimands of interest, E-QRGMM provides a practical solution for covariate-dependent uncertainty quantification.

</details>


### [97] [StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths](https://arxiv.org/abs/2601.19320)
*Tianyi Chen,Sihan Chen,Xiaoyi Qu,Dan Zhao,Ruomei Yan,Jongwoo Ko,Luming Liang,Pashmina Cameron*

Main category: cs.LG

TL;DR: 提出StableQAT框架，通过离散傅里叶分析推导出轻量级替代梯度，解决超低位宽量化训练中的梯度失配和不稳定问题，在2-4位量化中实现稳定高效训练。


<details>
  <summary>Details</summary>
Motivation: 量化感知训练在严格内存和延迟约束下部署大模型至关重要，但在超低位宽下实现稳定鲁棒的优化仍然具有挑战性。基于直通估计器或软量化器的常见方法存在梯度失配、不稳定或高计算开销问题。

Method: 提出StableQAT框架，通过对舍入算子进行离散傅里叶分析，推导出理论上有依据的轻量级替代梯度用于反向传播。该方法严格泛化了STE，将其作为更表达性替代梯度族的一个特例，产生平滑、有界且计算成本低的梯度。

Result: 在2-4位量化场景中，StableQAT表现出稳定高效的训练性能，相比标准QAT技术，在训练稳定性、鲁棒性和性能方面均有提升，且训练开销可忽略不计。

Conclusion: StableQAT通过理论上有依据的轻量级替代梯度解决了超低位宽量化训练中的稳定性问题，为在严格资源约束下部署大模型提供了有效的解决方案。

Abstract: Quantization-aware training (QAT) is essential for deploying large models under strict memory and latency constraints, yet achieving stable and robust optimization at ultra-low bitwidths remains challenging. Common approaches based on the straight-through estimator (STE) or soft quantizers often suffer from gradient mismatch, instability, or high computational overhead. As such, we propose StableQAT, a unified and efficient QAT framework that stabilizes training in ultra low-bit settings via a novel, lightweight, and theoretically grounded surrogate for backpropagation derived from a discrete Fourier analysis of the rounding operator. StableQAT strictly generalizes STE as the latter arises as a special case of our more expressive surrogate family, yielding smooth, bounded, and inexpensive gradients that improve QAT training performance and stability across various hyperparameter choices. In experiments, StableQAT exhibits stable and efficient QAT at 2-4 bit regimes, demonstrating improved training stability, robustness, and superior performance with negligible training overhead against standard QAT techniques. Our code is available at https://github.com/microsoft/StableQAT.

</details>


### [98] [From Observations to Events: Event-Aware World Model for Reinforcement Learning](https://arxiv.org/abs/2601.19336)
*Zhao-Han Peng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.LG

TL;DR: EAWM提出了一种事件感知世界模型框架，通过自动事件生成和事件边界识别来学习事件感知表示，提升模型在结构化相似场景中的泛化能力，在多个基准上显著提升MBRL基线性能10%-45%


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的强化学习方法在结构相似场景中泛化能力不足，容易受到纹理或颜色变化等虚假变化的影响。受认知科学启发，人类通过将连续感官流分割为离散事件并依赖关键事件进行决策，因此需要开发能够自动学习事件感知表示的方法

Method: 提出事件感知世界模型(EAWM)框架，包含：1)自动事件生成器从原始观测中推导事件；2)通用事件分割器(GES)识别事件边界；3)通过事件预测塑造表示空间以捕获有意义的时空转换；4)提供不同世界模型架构的统一公式化表示

Result: 在Atari 100K、Craftax 1M、DeepMind Control 500K和DMC-GB2 500K等基准测试中，EAWM将强MBRL基线的性能提升10%-45%，创造了新的最先进结果

Conclusion: EAWM通过事件感知表示学习有效提升了基于模型强化学习的样本效率和泛化能力，证明了事件分割在强化学习中的重要性，为世界模型学习提供了通用框架

Abstract: While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.

</details>


### [99] [Queue Length Regret Bounds for Contextual Queueing Bandits](https://arxiv.org/abs/2601.19300)
*Seoungbin Bae,Garyeong Kang,Dabeen Lee*

Main category: cs.LG

TL;DR: 论文提出了一种新的上下文感知排队模型——上下文排队赌博机，用于在调度作业时同时学习未知的服务率，通过上下文特征匹配作业与服务器以最大化离开率，并分析了队列长度遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统的调度问题通常假设服务率已知，但在实际应用中服务率往往是未知且与上下文特征相关的。需要一种能够同时学习未知服务率并进行最优调度的框架，以最大化系统吞吐量。

Method: 提出了上下文排队赌博机框架，其中作业携带异构上下文特征，服务率由逻辑斯蒂模型控制，包含未知的服务器特定参数。设计了两种算法：CQB-ε（针对随机上下文）和CQB-Opt（针对对抗性上下文）。引入了策略切换队列和复杂的耦合论证来处理队列状态差异。

Result: CQB-ε算法实现了Õ(T^{-1/4})的遗憾上界，CQB-Opt算法在对抗性上下文设置下实现了O(log²T)的遗憾上界。实验验证了理论结果。

Conclusion: 该研究成功地将上下文感知学习引入排队调度问题，提出了有效的算法框架和理论分析工具，为在未知服务率下的最优调度提供了新的解决方案。

Abstract: We introduce contextual queueing bandits, a new context-aware framework for scheduling while simultaneously learning unknown service rates. Individual jobs carry heterogeneous contextual features, based on which the agent chooses a job and matches it with a server to maximize the departure rate. The service/departure rate is governed by a logistic model of the contextual feature with an unknown server-specific parameter. To evaluate the performance of a policy, we consider queue length regret, defined as the difference in queue length between the policy and the optimal policy. The main challenge in the analysis is that the lists of remaining job features in the queue may differ under our policy versus the optimal policy for a given time step, since they may process jobs in different orders. To address this, we propose the idea of policy-switching queues equipped with a sophisticated coupling argument. This leads to a novel queue length regret decomposition framework, allowing us to understand the short-term effect of choosing a suboptimal job-server pair and its long-term effect on queue state differences. We show that our algorithm, CQB-$\varepsilon$, achieves a regret upper bound of $\widetilde{\mathcal{O}}(T^{-1/4})$. We also consider the setting of adversarially chosen contexts, for which our second algorithm, CQB-Opt, achieves a regret upper bound of $\mathcal{O}(\log^2 T)$. Lastly, we provide experimental results that validate our theoretical findings.

</details>


### [100] [Robust Uncertainty Estimation under Distribution Shift via Difference Reconstruction](https://arxiv.org/abs/2601.19341)
*Xinran Xu,Li Rong Wang,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出DRUE方法，通过重构两个中间层的输入并测量输出差异作为不确定性分数，改进深度学习模型的不确定性估计


<details>
  <summary>Details</summary>
Motivation: 在医学影像等高风险应用中，深度学习模型的不确定性估计对可靠决策至关重要。现有方法通过比较输入样本与辅助模型重构版本来估计不确定性，但直接比较会受到信息损失和表面细节敏感性的影响，限制了其有效性。

Method: 提出差异重构不确定性估计（DRUE）方法：从两个中间层重构输入，并测量它们输出之间的差异作为不确定性分数。使用分布外（OOD）检测范式进行评估，将分布内（ID）训练数据与具有逐渐增大的域偏移的数据集进行比较。

Result: 以青光眼检测作为ID任务，DRUE在多个OOD数据集上始终实现优越的AUC和AUPR，表明其在分布偏移下的鲁棒性和可靠性。

Conclusion: DRUE为增强模型在不确定环境中的可靠性提供了一个原则性且有效的框架，改进了深度学习模型的不确定性估计。

Abstract: Estimating uncertainty in deep learning models is critical for reliable decision-making in high-stakes applications such as medical imaging. Prior research has established that the difference between an input sample and its reconstructed version produced by an auxiliary model can serve as a useful proxy for uncertainty. However, directly comparing reconstructions with the original input is degraded by information loss and sensitivity to superficial details, which limits its effectiveness. In this work, we propose Difference Reconstruction Uncertainty Estimation (DRUE), a method that mitigates this limitation by reconstructing inputs from two intermediate layers and measuring the discrepancy between their outputs as the uncertainty score. To evaluate uncertainty estimation in practice, we follow the widely used out-of-distribution (OOD) detection paradigm, where in-distribution (ID) training data are compared against datasets with increasing domain shift. Using glaucoma detection as the ID task, we demonstrate that DRUE consistently achieves superior AUC and AUPR across multiple OOD datasets, highlighting its robustness and reliability under distribution shift. This work provides a principled and effective framework for enhancing model reliability in uncertain environments.

</details>


### [101] [LightSBB-M: Bridging Schrödinger and Bass for Generative Diffusion Modeling](https://arxiv.org/abs/2601.19312)
*Alexandre Alouadi,Pierre Henry-Labordère,Grégoire Loeper,Othmane Mazhar,Huyên Pham,Nizar Touzi*

Main category: cs.LG

TL;DR: LightSBB-M算法在Schrodinger Bridge and Bass框架下，通过联合控制漂移和波动率，在少量迭代中计算最优传输方案，在合成和真实生成任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Schrodinger Bridge and Bass（SBB）作为经典Schrodinger Bridge的扩展，联合控制漂移和波动率，但需要高效算法来计算最优传输方案。现有方法在计算效率和性能上存在局限，需要开发可扩展的高保真SBB求解器。

Method: 提出LightSBB-M算法：1）利用SBB目标的对偶表示获得最优漂移和波动率的解析表达式；2）引入可调参数β>0，在纯漂移（Schrodinger Bridge）和纯波动率（Bass鞅传输）之间插值；3）通过少量迭代计算最优SBB传输方案。

Result: 1）在合成数据集上，LightSBB-M达到最低的2-Wasserstein距离，比最先进的SB和扩散基线提升高达32%；2）在非配对图像到图像翻译任务（FFHQ中成人到儿童面部）上展示了生成能力；3）代码已开源。

Conclusion: LightSBB-M提供了一个可扩展、高保真的SBB求解器，在合成和真实世界生成任务中均优于现有的SB和扩散基线，为联合控制漂移和波动率的传输问题提供了高效解决方案。

Abstract: The Schrodinger Bridge and Bass (SBB) formulation, which jointly controls drift and volatility, is an established extension of the classical Schrodinger Bridge (SB). Building on this framework, we introduce LightSBB-M, an algorithm that computes the optimal SBB transport plan in only a few iterations. The method exploits a dual representation of the SBB objective to obtain analytic expressions for the optimal drift and volatility, and it incorporates a tunable parameter beta greater than zero that interpolates between pure drift (the Schrodinger Bridge) and pure volatility (Bass martingale transport). We show that LightSBB-M achieves the lowest 2-Wasserstein distance on synthetic datasets against state-of-the-art SB and diffusion baselines with up to 32 percent improvement. We also illustrate the generative capability of the framework on an unpaired image-to-image translation task (adult to child faces in FFHQ). These findings demonstrate that LightSBB-M provides a scalable, high-fidelity SBB solver that outperforms existing SB and diffusion baselines across both synthetic and real-world generative tasks. The code is available at https://github.com/alexouadi/LightSBB-M.

</details>


### [102] [Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection](https://arxiv.org/abs/2601.19375)
*Quy-Anh Dang,Chris Ngo*

Main category: cs.LG

TL;DR: 提出Selective Steering方法，通过规范保持的旋转公式和判别性层选择，解决现有激活导向技术的局限性，实现更稳定有效的LLM行为控制。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型对齐方面取得进展，但大型语言模型仍易受对抗攻击引发有害行为。现有激活导向技术存在关键限制：激活加法需要仔细调整系数且对层特定规范变化敏感，而方向消融仅提供二进制控制。Angular Steering虽引入连续控制，但其实际实现违反规范保持，导致分布偏移和生成崩溃，尤其在7B参数以下模型中。

Method: 提出Selective Steering方法，包含两个关键创新：(1) 数学上严谨的规范保持旋转公式，保持激活分布完整性；(2) 判别性层选择，仅在特征表示呈现相反符号类别对齐的层应用导向。

Result: 在九个模型上的实验表明，Selective Steering相比先前方法实现了5.5倍更高的攻击成功率，同时保持零困惑度违规，在标准基准测试上实现约100%的能力保留。

Conclusion: 该方法为可控且稳定的大型语言模型行为修改提供了一个原则性、高效的框架。

Abstract: Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering

</details>


### [103] [Generalizable IoT Traffic Representations for Cross-Network Device Identification](https://arxiv.org/abs/2601.19315)
*Arunan Sivanathan,David Warren,Deepak Mishra,Sushmita Ruj,Natasha Fernandes,Quan Z. Sheng,Minh Tran,Ben Luo,Daniel Coscia,Gustavo Batista,Hassan Habibi Gharakaheili*

Main category: cs.LG

TL;DR: 该论文研究了从无标签物联网流量中学习通用流量表示的方法，用于物联网设备识别，通过紧凑编码器架构和冻结编码器协议，在跨环境部署中实现了超过0.9的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有物联网设备识别方法大多依赖端到端监督管道或任务特定微调，导致流量表示与标记数据集和部署环境紧密耦合，限制了泛化能力。需要学习更具通用性的流量表示。

Method: 设计了紧凑编码器架构，从无标签物联网流量中学习每流嵌入；采用冻结编码器协议，使用简单监督分类器评估表示质量；开发了无监督编码器-解码器模型，通过重构分析评估表示质量。

Result: 使用超过1800万真实物联网流量流，在不相交标记子集上进行设备类型分类，实现了超过0.9的宏F1分数；证明了学习表示在跨环境部署中的鲁棒性；系统基准测试表明更大模型不一定产生更鲁棒的物联网流量表示。

Conclusion: 通过无监督学习获得的紧凑流量表示能够有效支持物联网设备类型分类，且具有跨环境泛化能力；证明了简单轻量级分类器在冻结嵌入上的有效性；为物联网流量表示学习提供了系统评估框架。

Abstract: Machine learning models have demonstrated strong performance in classifying network traffic and identifying Internet-of-Things (IoT) devices, enabling operators to discover and manage IoT assets at scale. However, many existing approaches rely on end-to-end supervised pipelines or task-specific fine-tuning, resulting in traffic representations that are tightly coupled to labeled datasets and deployment environments, which can limit generalizability. In this paper, we study the problem of learning generalizable traffic representations for IoT device identification. We design compact encoder architectures that learn per-flow embeddings from unlabeled IoT traffic and evaluate them using a frozen-encoder protocol with a simple supervised classifier. Our specific contributions are threefold. (1) We develop unsupervised encoder--decoder models that learn compact traffic representations from unlabeled IoT network flows and assess their quality through reconstruction-based analysis. (2) We show that these learned representations can be used effectively for IoT device-type classification using simple, lightweight classifiers trained on frozen embeddings. (3) We provide a systematic benchmarking study against the state-of-the-art pretrained traffic encoders, showing that larger models do not necessarily yield more robust representations for IoT traffic. Using more than 18 million real IoT traffic flows collected across multiple years and deployment environments, we learn traffic representations from unlabeled data and evaluate device-type classification on disjoint labeled subsets, achieving macro F1-scores exceeding 0.9 for device-type classification and demonstrating robustness under cross-environment deployment.

</details>


### [104] [Metric $k$-clustering using only Weak Comparison Oracles](https://arxiv.org/abs/2601.19333)
*Rahul Raychaudhury,Aryan Esmailpour,Sainyam Galhotra,Stavros Sintos*

Main category: cs.LG

TL;DR: 该论文研究在Rank模型下的聚类问题，使用四元组相对距离比较替代精确距离，设计出在噪声查询下具有常数近似比的聚类算法，并针对有界加倍维度的度量空间提供更好的查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 经典聚类算法需要精确的成对距离，但在实际应用中（如学习模型或人类反馈）往往无法获得。因此需要研究在只有相对距离比较（四元组查询）且存在噪声的情况下进行聚类的方法。

Method: 提出基于Rank模型的随机化算法，使用噪声四元组查询替代精确距离。算法输出O(k·polylog(n))个中心点及其分配映射，查询复杂度为O(n·k·polylog(n))。对于有界加倍维度的度量空间，查询复杂度可降至O((n+k²)·polylog(n))，并能实现(1+ε)近似。

Result: 算法在任意度量空间中实现常数近似比的聚类成本，查询复杂度为O(n·k·polylog(n))。对于有界加倍维度的度量空间，查询复杂度降至O((n+k²)·polylog(n))，且能实现(1+ε)近似。证明了噪声、低成本查询（如大语言模型）可系统集成到可扩展聚类算法中。

Conclusion: 该研究展示了在只有相对距离比较的Rank模型下进行有效聚类的可行性，为实际应用中无法获得精确距离的场景提供了理论框架和实用算法，特别适用于基于学习模型或人类反馈的聚类任务。

Abstract: Clustering is a fundamental primitive in unsupervised learning. However, classical algorithms for $k$-clustering (such as $k$-median and $k$-means) assume access to exact pairwise distances -- an unrealistic requirement in many modern applications. We study clustering in the \emph{Rank-model (R-model)}, where access to distances is entirely replaced by a \emph{quadruplet oracle} that provides only relative distance comparisons. In practice, such an oracle can represent learned models or human feedback, and is expected to be noisy and entail an access cost.
  Given a metric space with $n$ input items, we design randomized algorithms that, using only a noisy quadruplet oracle, compute a set of $O(k \cdot \mathsf{polylog}(n))$ centers along with a mapping from the input items to the centers such that the clustering cost of the mapping is at most constant times the optimum $k$-clustering cost. Our method achieves a query complexity of $O(n\cdot k \cdot \mathsf{polylog}(n))$ for arbitrary metric spaces and improves to $O((n+k^2) \cdot \mathsf{polylog}(n))$ when the underlying metric has bounded doubling dimension. When the metric has bounded doubling dimension we can further improve the approximation from constant to $1+\varepsilon$, for any arbitrarily small constant $\varepsilon\in(0,1)$, while preserving the same asymptotic query complexity. Our framework demonstrates how noisy, low-cost oracles, such as those derived from large language models, can be systematically integrated into scalable clustering algorithms.

</details>


### [105] [APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition](https://arxiv.org/abs/2601.19452)
*Finn Rietz,Pedro Zuidberg dos Martires,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: APC是一种分层模型，通过自适应组合多个数据驱动的归一化流先验来整合演示数据到强化学习中，能够处理稀疏、次优或未对齐的演示，避免严格遵循次优演示导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设演示数据是最优且与目标任务完全对齐的，但实际应用中演示数据往往是稀疏、次优或未对齐的，将这些演示整合到强化学习中可能导致性能下降。

Method: 提出自适应策略组合（APC）分层模型，自适应组合多个数据驱动的归一化流（NF）先验。APC不强制严格遵循先验，而是估计每个先验对目标任务的适用性，同时利用它们进行探索。APC可以细化有用的先验，或在必要时绕过未对齐的先验以优化下游奖励。

Result: 在多样化基准测试中，APC在演示对齐时加速学习，在严重未对齐情况下保持鲁棒性，利用次优演示引导探索，同时避免因过度严格遵循次优演示导致的性能下降。

Conclusion: APC通过自适应组合归一化流先验，有效解决了将演示数据整合到强化学习中时面临的稀疏、次优和未对齐问题，在多种情况下都能实现稳健的性能提升。

Abstract: Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.

</details>


### [106] [LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment](https://arxiv.org/abs/2601.19487)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Wenhai Wang*

Main category: cs.LG

TL;DR: LLM-VA通过将回答向量与安全判断向量对齐，解决了LLM的安全对齐困境，无需微调即可同时减少越狱和过度拒绝


<details>
  <summary>Details</summary>
Motivation: 现有向量调控方法在减少越狱和过度拒绝之间存在根本性权衡，因为LLM将回答决策和安全判断编码为近似正交的独立过程

Method: 识别各层的回答向量和安全向量，选择安全相关层，通过闭式权重更新对齐两个向量，使模型回答意愿因果依赖于安全评估

Result: 在12个LLM上，LLM-VA比最佳基线F1分数提高11.45%，同时保持95.92%的实用性，无需手动调优即可自适应各模型的安全偏差

Conclusion: LLM-VA通过向量对齐解决了安全对齐的基本困境，为LLM安全调控提供了无需微调的有效方法

Abstract: Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.

</details>


### [107] [Scale-Consistent State-Space Dynamics via Fractal of Stationary Transformations](https://arxiv.org/abs/2601.19551)
*Geunhyeok Yu,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出FROST框架，通过分形归纳偏置强制状态空间模型在迭代细化过程中保持尺度一致的潜在动态，使中间状态对应共享表示的不同分辨率，从而实现自适应计算和自然停止。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型依赖深度但缺乏对中间表示有效性的结构保证，导致早期停止和自适应计算难以实现。需要为状态空间模型建立跨迭代细化的尺度一致潜在动态的结构要求。

Method: 提出FROST（分形平稳变换）框架，通过分形归纳偏置强制自相似表示流形。该几何结构使中间状态对应共享表示的不同分辨率，并提供几何分析建立跨迭代的收缩和稳定收敛。

Result: 在ImageNet-100上的控制实验验证了预测的尺度一致行为，显示自适应效率源于对齐的潜在几何结构。停止机制自然地采用基于排名的公式化，由内在特征质量而非外在目标驱动。

Conclusion: FROST通过强制尺度一致的结构，为状态空间模型提供了结构保证，使中间表示有效且可解释，从而实现自然停止和自适应计算，解决了深度模型缺乏中间表示结构保证的问题。

Abstract: Recent deep learning models increasingly rely on depth without structural guarantees on the validity of intermediate representations, rendering early stopping and adaptive computation ill-posed. We address this limitation by formulating a structural requirement for state-space model's scale-consistent latent dynamics across iterative refinement, and derive Fractal of Stationary Transformations (FROST), which enforces a self-similar representation manifold through a fractal inductive bias. Under this geometry, intermediate states correspond to different resolutions of a shared representation, and we provide a geometric analysis establishing contraction and stable convergence across iterations. As a consequence of this scale-consistent structure, halting naturally admits a ranking-based formulation driven by intrinsic feature quality rather than extrinsic objectives. Controlled experiments on ImageNet-100 empirically verify the predicted scale-consistent behavior, showing that adaptive efficiency emerges from the aligned latent geometry.

</details>


### [108] [AROMMA: Unifying Olfactory Embeddings for Single Molecules and Mixtures](https://arxiv.org/abs/2601.19561)
*Dayoung Kang,JongWon Kim,Jiho Park,Keonseock Lee,Ji-Woong Choi,Jinhyun So*

Main category: cs.LG

TL;DR: AROMMA框架学习单分子和双分子混合物的统一嵌入空间，通过注意力聚合器处理混合物，并使用知识蒸馏和伪标注对齐气味描述符，在单分子和分子对数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前公共嗅觉数据集规模小且分散在单分子和混合物之间，限制了可泛化气味表示的学习。现有方法要么学习单分子嵌入，要么通过相似性或成对标签预测处理混合物，导致表示分离且未对齐。

Method: 提出AROMMA框架：1) 使用化学基础模型编码单分子；2) 通过基于注意力的聚合器组合混合物，确保排列不变性和不对称分子相互作用；3) 使用知识蒸馏和类感知伪标注对齐气味描述符集，丰富缺失的混合物标注。

Result: AROMMA在单分子和分子对数据集上均达到最先进性能，AUROC提升高达19.1%，在两个领域都表现出强大的泛化能力。

Conclusion: AROMMA成功学习了单分子和双分子混合物的统一嵌入空间，通过创新的注意力聚合和标注对齐方法，显著提升了气味表示的泛化性能。

Abstract: Public olfaction datasets are small and fragmented across single molecules and mixtures, limiting learning of generalizable odor representations. Recent works either learn single-molecule embeddings or address mixtures via similarity or pairwise label prediction, leaving representations separate and unaligned. In this work, we propose AROMMA, a framework that learns a unified embedding space for single molecules and two-molecule mixtures. Each molecule is encoded by a chemical foundation model and the mixtures are composed by an attention-based aggregator, ensuring both permutation invariance and asymmetric molecular interactions. We further align odor descriptor sets using knowledge distillation and class-aware pseudo-labeling to enrich missing mixture annotations. AROMMA achieves state-of-the-art performance in both single-molecule and molecule-pair datasets, with up to 19.1% AUROC improvement, demonstrating a robust generalization in two domains.

</details>


### [109] [DSP-Reg: Domain-Sensitive Parameter Regularization for Robust Domain Generalization](https://arxiv.org/abs/2601.19394)
*Xudong Han,Senkang Hu,Yihang Tao,Yu Guo,Philip Birch,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出了一种基于参数敏感性分析的域泛化方法DSP-Reg，通过协方差分析量化参数对域偏移的敏感性，并利用软正则化技术引导模型更多地依赖域不变参数，在多个基准数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有域泛化方法主要关注学习域不变特征，假设对源域变化鲁棒的模型能很好地泛化到未见目标域。但这些方法缺乏在参数层面的深入分析，难以明确区分对域偏移敏感的参数和鲁棒参数，可能阻碍模型的整体泛化能力。

Method: 首先构建基于协方差的参数敏感性分析框架，通过计算多个源域上参数梯度的协方差来量化每个参数对域偏移的敏感性。基于此提出域敏感参数正则化(DSP-Reg)框架，采用软正则化技术引导模型优化，鼓励模型更多地依赖域不变参数，同时抑制域特定参数。

Result: 在PACS、VLCS、OfficeHome和DomainNet等基准数据集上的广泛实验表明，DSP-Reg优于现有最先进方法，平均准确率达到66.7%，超越了所有基线方法。

Conclusion: 通过参数层面的敏感性分析，DSP-Reg提供了对模型学习过程更精细的控制，提高了对未见域的鲁棒性和泛化能力，为域泛化研究提供了新的视角和方法。

Abstract: Domain Generalization (DG) is a critical area that focuses on developing models capable of performing well on data from unseen distributions, which is essential for real-world applications. Existing approaches primarily concentrate on learning domain-invariant features, which assume that a model robust to variations in the source domains will generalize well to unseen target domains. However, these approaches neglect a deeper analysis at the parameter level, which makes the model hard to explicitly differentiate between parameters sensitive to domain shifts and those robust, potentially hindering its overall ability to generalize. In order to address these limitations, we first build a covariance-based parameter sensitivity analysis framework to quantify the sensitivity of each parameter in a model to domain shifts. By computing the covariance of parameter gradients across multiple source domains, we can identify parameters that are more susceptible to domain variations, which serves as our theoretical foundation. Based on this, we propose Domain-Sensitive Parameter Regularization (DSP-Reg), a principled framework that guides model optimization by a soft regularization technique that encourages the model to rely more on domain-invariant parameters while suppressing those that are domain-specific. This approach provides a more granular control over the model's learning process, leading to improved robustness and generalization to unseen domains. Extensive experiments on benchmarks, such as PACS, VLCS, OfficeHome, and DomainNet, demonstrate that DSP-Reg outperforms state-of-the-art approaches, achieving an average accuracy of 66.7\% and surpassing all baselines.

</details>


### [110] [From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation](https://arxiv.org/abs/2601.19588)
*Yongqi Wang,Xiaofeng Ji,Jie Wang,Qingbin Li,Xiao Xiong,Zheming Yang,Jian Xu,Minghui Qiu,Xinxiao Wu*

Main category: cs.LG

TL;DR: DGRC框架通过从推理路径分歧中动态构建原子知识和推理链的双重课程，解决无标注数据下LLM领域适应问题，避免继承教师模型的推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在无标注数据下存在两个核心问题：学生模型低效地针对自身弱点进行模仿，以及可能继承教师模型的推理缺陷。这揭示了一个关键的教学困境：当教师本身不是完美专家时，如何设计可靠的课程。

Method: 提出分歧引导的推理课程（DGRC），基于LLM在原子子问题上具有高保真度的洞察，当学生和教师产生冲突结果时，引导教师进行诊断分析：分析推理路径分歧点，制定针对性的原子查询并自回答，创建高置信度的原子问答对。这些问答对既作为原子课程纠正学生知识缺口，又作为事实标准过滤教师原始推理链，产生经过验证的思维链课程。

Result: 在医疗和法律领域的实验中，DGRC框架对各种规模的学生模型均有效。特别地，在医疗领域，1.5B学生模型相比强无标注基线实现了7.76%的相对提升。

Conclusion: DGRC通过利用LLM在原子问题上的可靠性，从推理分歧中动态构建双重课程，有效解决了无标注数据下LLM领域适应的教学困境，避免了传统知识蒸馏的缺陷。

Abstract: Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inheriting the teacher's reasoning flaws. This exposes a critical pedagogical dilemma: how to devise a reliable curriculum when the teacher itself is not an infallible expert. Our work resolves this by capitalizing on a key insight: while LLMs may exhibit fallibility in complex, holistic reasoning, they often exhibit high fidelity on focused, atomic sub-problems. Based on this, we propose Divergence-Guided Reasoning Curriculum (DGRC), which constructs a learning path from atomic knowledge to reasoning chains by dynamically deriving two complementary curricula from disagreements in reasoning pathways. When a student and teacher produce conflicting results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes both reasoning paths to formulate atomic queries that target the specific points of divergence, and then self-answers these queries to create high-confidence atomic question-answer pairs. These pairs then serve a dual purpose: (1) providing an atomic curriculum to rectify the student's knowledge gaps, and (2) serving as factual criteria to filter the teacher's original reasoning chains, yielding a verified CoT curriculum that teaches the student how to integrate atomic knowledge into complete reasoning paths. Experiments across the medical and legal domains on student models of various sizes demonstrate the effectiveness of our DGRC framework. Notably, our method achieves a 7.76% relative improvement for the 1.5B student model in the medical domain over strong unlabeled baseline.

</details>


### [111] [SEAFormer: A Spatial Proximity and Edge-Aware Transformer for Real-World Vehicle Routing Problems](https://arxiv.org/abs/2601.19395)
*Saeed Nasehi Basharzad,Farhana Choudhury,Egemen Tanin*

Main category: cs.LG

TL;DR: SEAFormer是一种新型Transformer模型，通过聚类邻近注意力和边缘感知模块，有效解决大规模真实世界车辆路径问题，首次实现1000+节点RWVRP的神经求解。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在经典VRP基准上表现良好，但难以解决真实世界车辆路径问题，因为它们忽略了序列依赖性和边缘级信息，而这些正是RWVRP复杂性的关键特征。

Method: 提出SEAFormer模型，包含两个关键创新：1) 聚类邻近注意力(CPA)，利用局部感知聚类将注意力复杂度从O(n²)降至O(n)，同时保持全局视角；2) 轻量级边缘感知模块，通过残差融合捕获成对特征，有效整合边缘信息并加速收敛。

Result: 在四种RWVRP变体上的广泛实验表明，SEAFormer优于现有最先进方法。值得注意的是，它是首个能有效解决1000+节点RWVRP的神经方法，同时在经典VRP上也表现出优越性能。

Conclusion: SEAFormer通过同时整合节点级和边缘级信息，为研究基准和实际应用提供了一个通用解决方案，能够有效处理大规模真实世界车辆路径问题的复杂性。

Abstract: Real-world Vehicle Routing Problems (RWVRPs) require solving complex, sequence-dependent challenges at scale with constraints such as delivery time window, replenishment or recharging stops, asymmetric travel cost, etc. While recent neural methods achieve strong results on large-scale classical VRP benchmarks, they struggle to address RWVRPs because their strategies overlook sequence dependencies and underutilize edge-level information, which are precisely the characteristics that define the complexity of RWVRPs. We present SEAFormer, a novel transformer that incorporates both node-level and edge-level information in decision-making through two key innovations. First, our Clustered Proximity Attention (CPA) exploits locality-aware clustering to reduce the complexity of attention from $O(n^2)$ to $O(n)$ while preserving global perspective, allowing SEAFormer to efficiently train on large instances. Second, our lightweight edge-aware module captures pairwise features through residual fusion, enabling effective incorporation of edge-based information and faster convergence. Extensive experiments across four RWVRP variants with various scales demonstrate that SEAFormer achieves superior results over state-of-the-art methods. Notably, SEAFormer is the first neural method to solve 1,000+ node RWVRPs effectively, while also achieving superior performance on classic VRPs, making it a versatile solution for both research benchmarks and real-world applications.

</details>


### [112] [Intersectional Fairness via Mixed-Integer Optimization](https://arxiv.org/abs/2601.19595)
*Jiří Němeček,Mark Kozdoba,Illia Kryvoviaz,Tomáš Pevný,Jakub Mareček*

Main category: cs.LG

TL;DR: 提出基于混合整数优化的统一框架，训练具有交叉公平性和内在可解释性的分类器，满足高风险AI应用的监管要求


<details>
  <summary>Details</summary>
Motivation: 高风险领域（金融、医疗）的AI部署需要公平透明的模型，但现有监管框架（如欧盟AI法案）对偏见的定义模糊，且真正公平需要解决受保护群体交叉点的偏见问题

Method: 提出统一框架，利用混合整数优化（MIO）训练具有交叉公平性和内在可解释性的分类器，证明两种交叉公平度量（MSD和SPSF）在检测最不公平子群体上的等价性

Result: 基于MIO的算法在发现偏见方面性能提升，训练出高性能、可解释的分类器，能够将交叉偏见限制在可接受阈值以下

Conclusion: 该框架为受监管行业及其他领域提供了强大的解决方案，能够同时满足公平性、可解释性和高性能的要求

Abstract: The deployment of Artificial Intelligence in high-risk domains, such as finance and healthcare, necessitates models that are both fair and transparent. While regulatory frameworks, including the EU's AI Act, mandate bias mitigation, they are deliberately vague about the definition of bias. In line with existing research, we argue that true fairness requires addressing bias at the intersections of protected groups. We propose a unified framework that leverages Mixed-Integer Optimization (MIO) to train intersectionally fair and intrinsically interpretable classifiers. We prove the equivalence of two measures of intersectional fairness (MSD and SPSF) in detecting the most unfair subgroup and empirically demonstrate that our MIO-based algorithm improves performance in finding bias. We train high-performing, interpretable classifiers that bound intersectional bias below an acceptable threshold, offering a robust solution for regulated industries and beyond.

</details>


### [113] [OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation](https://arxiv.org/abs/2601.19439)
*Giuseppe Chiari,Michele Piccoli,Davide Zoni*

Main category: cs.LG

TL;DR: OSIRIS是一个用于模拟集成电路设计的可扩展数据集生成管道，解决了该领域缺乏高质量开放数据集的问题，并发布了包含87,100个电路变体的数据集及基于强化学习的基线方法。


<details>
  <summary>Details</summary>
Motivation: 模拟集成电路设计自动化面临长期挑战，主要原因是物理布局、寄生效应和电路性能之间的复杂相互依赖关系。传统方法难以准确捕捉和优化这些复杂约束，而机器学习方法的发展受到缺乏专门针对模拟领域的高质量开放数据集的限制，这阻碍了基准测试和方法的泛化能力。

Method: 提出了OSIRIS，一个可扩展的模拟IC设计数据集生成管道。该系统系统地探索模拟电路的设计空间，生成全面的性能指标和元数据。此外，还发布了基于OSIRIS生成的87,100个电路变体数据集，并提供了一个利用OSIRIS进行模拟设计优化的基于强化学习的基线方法。

Result: 开发了OSIRIS数据集生成管道，并发布了包含87,100个电路变体的数据集。该数据集支持机器学习驱动的电子设计自动化研究，为模拟IC设计领域提供了首个大规模、高质量、开放的数据资源。

Conclusion: OSIRIS解决了模拟IC设计领域缺乏高质量开放数据集的关键瓶颈问题，为机器学习驱动的电子设计自动化研究提供了重要基础设施。通过提供系统化的数据集生成管道和大规模数据集，有望加速模拟电路设计自动化的研究进展。

Abstract: The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.

</details>


### [114] [Explicit Multi-head Attention for Inter-head Interaction in Large Language Models](https://arxiv.org/abs/2601.19611)
*Runyu Peng,Yunhua Zhou,Demin Song,Kai Lv,Bo Wang,Qipeng Guo,Xipeng Qiu*

Main category: cs.LG

TL;DR: 提出Multi-head Explicit Attention (MEA)，一种通过显式建模跨头交互来增强Transformer注意力的方法，包含头级线性组合模块和头级组归一化层，既能提升训练鲁棒性和性能，又能通过虚拟头实现KV缓存压缩。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的大语言模型中，已有研究表明多头间的交互可以提升注意力性能。受此启发，作者希望设计一种能够显式建模跨头交互的注意力变体，以增强模型表现并探索参数效率。

Method: 提出Multi-head Explicit Attention (MEA)，包含两个核心组件：1) Head-level Linear Composition (HLC)模块，分别对跨头的key和value向量应用可学习的线性组合，实现丰富的跨头通信；2) 头级Group Normalization层，对齐重组后头的统计特性。此外，通过减少注意力头数量并利用HLC重构低秩"虚拟头"，实现参数效率优化。

Result: MEA在预训练中表现出强鲁棒性，允许使用更大的学习率实现更快收敛，最终获得更低的验证损失和跨任务性能提升。通过虚拟头策略，实现了KV缓存内存使用减少50%，在知识密集和科学推理任务上性能损失可忽略，仅在奥林匹克级数学基准上准确率下降3.59%。

Conclusion: MEA是一种简单有效的注意力变体，通过显式建模跨头交互提升了Transformer模型的性能和训练效率，同时通过虚拟头重构实现了实用的KV缓存压缩，为大型语言模型的优化提供了新思路。

Abstract: In large language models built upon the Transformer architecture, recent studies have shown that inter-head interaction can enhance attention performance. Motivated by this, we propose Multi-head Explicit Attention (MEA), a simple yet effective attention variant that explicitly models cross-head interaction. MEA consists of two key components: a Head-level Linear Composition (HLC) module that separately applies learnable linear combinations to the key and value vectors across heads, thereby enabling rich inter-head communication; and a head-level Group Normalization layer that aligns the statistical properties of the recombined heads. MEA shows strong robustness in pretraining, which allows the use of larger learning rates that lead to faster convergence, ultimately resulting in lower validation loss and improved performance across a range of tasks. Furthermore, we explore the parameter efficiency of MEA by reducing the number of attention heads and leveraging HLC to reconstruct them using low-rank "virtual heads". This enables a practical key-value cache compression strategy that reduces KV-cache memory usage by 50% with negligible performance loss on knowledge-intensive and scientific reasoning tasks, and only a 3.59% accuracy drop for Olympiad-level mathematical benchmarks.

</details>


### [115] [R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning](https://arxiv.org/abs/2601.19620)
*Zhizheng Jiang,Kang Zhao,Weikai Xu,Xinkui Lin,Wei Liu,Jian Luan,Shuo Shang,Peng Han*

Main category: cs.LG

TL;DR: R³方法通过跨上下文回放、上下文内自反思和结构熵排序奖励三个机制，解决了大推理模型在强化学习中组内优势崩溃的问题，在数学基准测试中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法依赖同批次内高质量样本诱导的优势差距，但在挑战性任务中组内优势容易崩溃，导致训练过程脆弱且低效。需要解决组内优势维持和样本质量评估问题。

Method: 提出R³强化学习机制：1)跨上下文回放策略，通过回忆同一查询历史轨迹中的有价值示例维持组内优势；2)上下文内自反思机制，利用过去失败经验精炼输出；3)结构熵排序奖励，基于token级熵模式对截断或失败样本分配相对奖励，捕捉局部探索和全局稳定性。

Result: 在Deepseek-R1-Distill-Qwen-1.5B模型上实现，使用DeepscaleR-40k数学数据集训练。实验表明在多个数学基准测试中达到SOTA性能，相比基础模型有显著改进且使用更少推理token。

Conclusion: R³方法通过三个创新机制有效解决了大推理模型强化学习中的组内优势崩溃问题，实现了更稳定高效的训练，在数学推理任务上表现出优越性能。

Abstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.

</details>


### [116] [Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2601.19624)
*Tongxi Wang,Zhuoyang Xia,Xinran Chen,Shan Liu*

Main category: cs.LG

TL;DR: AES（自适应熵调度）通过可观测的漂移信号在线调整熵系数，解决环境漂移下传统固定熵系数导致的探索不足或过度探索问题


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习面临环境漂移，现有方法依赖静态熵系数/目标熵，导致稳定期过度探索、漂移后探索不足且恢复缓慢，缺乏探索强度与漂移幅度关系的理论指导

Method: 将非平稳环境下的熵调度简化为单维逐轮权衡，基于可观测的在线漂移信号驱动探索强度，提出AES方法在线自适应调整熵系数/温度，几乎无需结构改变且开销极小

Result: 在4种算法变体、12个任务和4种漂移模式下，AES显著减少了漂移导致的性能下降比例，并加速了突变后的恢复过程

Conclusion: AES通过可测量的漂移信号自适应调整探索强度，有效解决了环境漂移下的探索-利用权衡问题，为实际应用中的非平稳环境强化学习提供了实用解决方案

Abstract: Real-world reinforcement learning often faces environment drift, but most existing methods rely on static entropy coefficients/target entropy, causing over-exploration during stable periods and under-exploration after drift (thus slow recovery), and leaving unanswered the principled question of how exploration intensity should scale with drift magnitude. We prove that entropy scheduling under non-stationarity can be reduced to a one-dimensional, round-by-round trade-off, faster tracking of the optimal solution after drift vs. avoiding gratuitous randomness when the environment is stable, so exploration strength can be driven by measurable online drift signals. Building on this, we propose AES (Adaptive Entropy Scheduling), which adaptively adjusts the entropy coefficient/temperature online using observable drift proxies during training, requiring almost no structural changes and incurring minimal overhead. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces the fraction of performance degradation caused by drift and accelerates recovery after abrupt changes.

</details>


### [117] [ProToken: Token-Level Attribution for Federated Large Language Models](https://arxiv.org/abs/2601.19672)
*Waris Gill,Ahmad Humayun,Ali Anwar,Muhammad Ali Gulzar*

Main category: cs.LG

TL;DR: ProToken：一种用于联邦大语言模型的令牌级溯源方法，能够在保持隐私约束的同时，在自回归文本生成过程中实现客户端贡献溯源，准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习使大语言模型能够在分布式数据源上协作训练并保护隐私，但在关键应用中部署时，无法确定哪些客户端对特定生成响应做出了贡献，这阻碍了调试、恶意客户端识别、公平奖励分配和信任验证。

Method: ProToken利用两个关键洞察实现令牌级溯源：(1) Transformer架构将任务特定信号集中在后层，支持策略性层选择以提高计算可行性；(2) 基于梯度的相关性加权过滤无关神经激活，聚焦直接影响令牌生成的神经元。

Result: 在16种配置（涵盖4种LLM架构：Gemma、Llama、Qwen、SmolLM和4个领域：医疗、金融、数学、编程）的评估中，ProToken平均溯源准确率达98%，在客户端数量扩展时仍保持高精度。

Conclusion: ProToken为联邦大语言模型提供了实用的令牌级客户端贡献溯源方法，解决了关键应用中的调试、恶意客户端识别、公平奖励分配和信任验证问题，验证了其在实际部署环境中的可行性。

Abstract: Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.

</details>


### [118] [Cross-Domain Offshore Wind Power Forecasting: Transfer Learning Through Meteorological Clusters](https://arxiv.org/abs/2601.19674)
*Dominic Weisser,Chloé Hashimoto-Cullen,Benjamin Guedj*

Main category: cs.LG

TL;DR: 提出一种基于聚类和专家模型集成的新型迁移学习框架，用于解决新建海上风电场数据稀缺下的功率预测问题，仅需5个月站点数据即可实现准确预测


<details>
  <summary>Details</summary>
Motivation: 新建海上风电场缺乏历史数据，而传统机器学习模型需要大量站点特定数据，这阻碍了准确功率预测的实现，影响电网稳定、备用管理和能源交易效率

Method: 提出基于气象特征聚类的迁移学习框架：1) 根据协变量气象特征对功率输出进行聚类；2) 为每个聚类训练专家模型；3) 使用专家模型集成进行预测；4) 模型已校准季节性变化，无需完整年度数据

Result: 在8个海上风电场评估，仅需不到5个月的站点特定数据即可实现准确跨域预测，MAE达到3.52%，验证了可靠预测无需完整年度周期

Conclusion: 该气候感知迁移学习方法不仅解决了新建风电场功率预测的数据稀缺问题，还为早期风资源评估等应用开辟新机会，可显著加速项目开发并有效降低风险

Abstract: Ambitious decarbonisation targets are catalysing growth in orders of new offshore wind farms. For these newly commissioned plants to run, accurate power forecasts are needed from the onset. These allow grid stability, good reserve management and efficient energy trading. Despite machine learning models having strong performances, they tend to require large volumes of site-specific data that new farms do not yet have. To overcome this data scarcity, we propose a novel transfer learning framework that clusters power output according to covariate meteorological features. Rather than training a single, general-purpose model, we thus forecast with an ensemble of expert models, each trained on a cluster. As these pre-trained models each specialise in a distinct weather pattern, they adapt efficiently to new sites and capture transferable, climate-dependent dynamics. Through the expert models' built-in calibration to seasonal and meteorological variability, we remove the industry-standard requirement of local measurements over a year. Our contributions are two-fold - we propose this novel framework and comprehensively evaluate it on eight offshore wind farms, achieving accurate cross-domain forecasting with under five months of site-specific data. Our experiments achieve a MAE of 3.52\%, providing empirical verification that reliable forecasts do not require a full annual cycle. Beyond power forecasting, this climate-aware transfer learning method opens new opportunities for offshore wind applications such as early-stage wind resource assessment, where reducing data requirements can significantly accelerate project development whilst effectively mitigating its inherent risks.

</details>


### [119] [GenCP: Towards Generative Modeling Paradigm of Coupled Physics](https://arxiv.org/abs/2601.19541)
*Tianrun Gao,Haoren Zheng,Wenhao Deng,Haodong Feng,Tao Zhang,Ruiqi Feng,Qianyi Chen,Tailin Wu*

Main category: cs.LG

TL;DR: GenCP提出了一种新颖的生成式范式，用于耦合多物理场模拟，通过将耦合物理建模转化为概率建模问题，在生成建模中集成概率密度演化与迭代多物理场耦合，能够在解耦数据上训练并在采样时推断耦合物理。


<details>
  <summary>Details</summary>
Motivation: 现实物理系统本质复杂，涉及多物理场耦合，模拟既具高价值又具挑战性。主流方法在处理解耦数据时面临困难，且在强耦合时空物理系统中存在效率和保真度低的问题。

Method: 将耦合物理建模转化为概率建模问题，关键创新是将生成建模中的概率密度演化与迭代多物理场耦合相结合，从而能够在解耦模拟数据上训练，在采样时推断耦合物理。利用概率演化空间中的算子分裂理论建立"条件到联合"采样方案的误差可控性保证。

Result: 在一个合成设置和三个具有挑战性的多物理场场景中评估了该范式，展示了GenCP的原理性洞察和优越的应用性能。

Conclusion: GenCP为耦合多物理场模拟提供了一种新颖且优雅的生成式范式，通过概率建模框架解决了现有方法在处理解耦数据和强耦合系统时的局限性，具有理论保证和实际应用优势。

Abstract: Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant generative paradigm for coupled multiphysics simulation. By formulating coupled-physics modeling as a probability modeling problem, our key innovation is to integrate probability density evolution in generative modeling with iterative multiphysics coupling, thereby enabling training on data from decoupled simulation and inferring coupled physics during sampling. We also utilize operator-splitting theory in the space of probability evolution to establish error controllability guarantees for this "conditional-to-joint" sampling scheme. We evaluate our paradigm on a synthetic setting and three challenging multi-physics scenarios to demonstrate both principled insight and superior application performance of GenCP. Code is available at this repo: github.com/AI4Science-WestlakeU/GenCP.

</details>


### [120] [Out-of-Distribution Generalization via Invariant Trajectories for Multimodal Large Language Model Editing](https://arxiv.org/abs/2601.19700)
*Jiajie Su,Haoyuan Wang,Xiaohua Feng,Yunshan Ma,Xiaobo Xia,Yuyuan Li,Xiaolin Zheng,Jianmao Xiao,Chaochao Chen*

Main category: cs.LG

TL;DR: 本文提出ODEdit框架，将多模态大语言模型的知识编辑重新定义为分布外泛化问题，通过不变学习提升编辑的可靠性、局部性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有单模态LLM编辑方法依赖于刚性的参数-输出映射，在多模态LLM的级联推理中会导致因果欠拟合和因果过拟合，无法适应多样的跨模态提示。

Method: 提出ODEdit框架，将MLLM编辑重构为OOD泛化问题，通过优化三元OOD风险目标来增强编辑可靠性、局部性和泛化性，并引入编辑轨迹不变学习方法，在风险最小化目标中加入全变差惩罚以稳定编辑轨迹。

Result: 理论分析和大量实验证明了ODEdit的有效性，能够准确识别语义偏移与事实偏移，在多样跨模态提示中实现鲁棒编辑。

Conclusion: ODEdit通过将MLLM知识编辑重新定义为OOD泛化问题，并采用不变学习框架，成功解决了现有编辑方法在跨模态推理中的局限性，实现了更可靠、局部和泛化的知识编辑。

Abstract: Knowledge editing emerges as a crucial technique for efficiently correcting incorrect or outdated knowledge in large language models (LLM). Existing editing methods for unimodal LLM rely on a rigid parameter-to-output mapping, which causes causal-underfit and causal-overfit in cascaded reasoning for Multimodal LLM (MLLM). In this paper, we reformulate MLLM editing as an out-of-distribution (OOD) generalization problem, where the goal is to discern semantic shift with factual shift and thus achieve robust editing among diverse cross-modal prompting. The key challenge of this OOD problem lies in identifying invariant causal trajectories that generalize accurately while suppressing spurious correlations. To address it, we propose ODEdit, a plug-and-play invariant learning based framework that optimizes the tripartite OOD risk objective to simultaneously enhance editing reliability, locality, and generality.We further introduce an edit trajectory invariant learning method, which integrates a total variation penalty into the risk minimization objective to stabilize edit trajectories against environmental variations. Theoretical analysis and extensive experiments demonstrate the effectiveness of ODEdit.

</details>


### [121] [The Geometric Mechanics of Contrastive Representation Learning: Alignment Potentials, Entropic Dispersion, and Cross-Modal Divergence](https://arxiv.org/abs/2601.19597)
*Yichao Cai,Zhen Zhang,Yuhang Liu,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 该论文提出了一个测度论框架，将对比学习建模为固定嵌入流形上表示测度的演化，揭示了InfoNCE目标函数在单模态和多模态设置下的几何分岔现象。


<details>
  <summary>Details</summary>
Motivation: 尽管InfoNCE是现代对比学习的核心，但其几何机制在经典的对齐-均匀性分解之外仍缺乏深入理解。研究者希望建立一个理论框架，从表示测度演化的角度揭示对比学习的几何本质。

Method: 提出了一个测度论框架，将学习过程建模为固定嵌入流形上表示测度的演化。通过在大批次极限下建立值和梯度一致性，将随机目标函数与显式确定性能量景观联系起来。分析了单模态和多模态两种设置下的几何特性。

Result: 在单模态设置中，内在景观是严格凸的，具有唯一的吉布斯平衡；熵仅作为平局决胜因素，"均匀性"被澄清为对齐盆地内的约束扩张。在多模态对称目标中，存在持久的负对称散度项，即使在核锐化后仍然存在，该项诱导了障碍驱动的协同适应，强制产生群体水平的模态间隙。

Conclusion: 研究将分析视角从点对点判别转向群体几何，为诊断和控制分布错配提供了理论基础。揭示了模态间隙是结构几何必要性而非初始化伪影，为理解对比学习的几何机制提供了新框架。

Abstract: While InfoNCE powers modern contrastive learning, its geometric mechanisms remain under-characterized beyond the canonical alignment--uniformity decomposition. We present a measure-theoretic framework that models learning as the evolution of representation measures on a fixed embedding manifold. By establishing value and gradient consistency in the large-batch limit, we bridge the stochastic objective to explicit deterministic energy landscapes, uncovering a fundamental geometric bifurcation between the unimodal and multimodal regimes. In the unimodal setting, the intrinsic landscape is strictly convex with a unique Gibbs equilibrium; here, entropy acts merely as a tie-breaker, clarifying "uniformity" as a constrained expansion within the alignment basin. In contrast, the symmetric multimodal objective contains a persistent negative symmetric divergence term that remains even after kernel sharpening. We show that this term induces barrier-driven co-adaptation, enforcing a population-level modality gap as a structural geometric necessity rather than an initialization artifact. Our results shift the analytical lens from pointwise discrimination to population geometry, offering a principled basis for diagnosing and controlling distributional misalignment.

</details>


### [122] [LoPRo: Enhancing Low-Rank Quantization via Permuted Block-Wise Rotation](https://arxiv.org/abs/2601.19675)
*Hongyaoxing Gu,Lijuan Hu,Liye Yu,Haowei Li,Fangfang Liu*

Main category: cs.LG

TL;DR: LoPRo是一种无需微调的后训练量化方法，通过块级置换和Walsh-Hadamard变换旋转相似重要性的列，同时显式保护最重要列块的量化精度，结合基于rank-1 sketch的混合精度快速低秩分解最小化量化成本。


<details>
  <summary>Details</summary>
Motivation: 当前仅权重量化的后训练量化方法主要关注具有挑战性的3位以下量化，这些方法通常存在显著的精度下降，需要微调才能获得有竞争力的性能。本文重新审视权重量化的基本特性，分析低秩近似下残差矩阵量化的挑战。

Method: 提出LoPRo算法：1) 应用块级置换和Walsh-Hadamard变换旋转相似重要性的列，显式保护最重要列块的量化精度；2) 引入基于rank-1 sketch的混合精度快速低秩分解进一步最小化量化成本。

Result: LoPRo在2位和3位量化上优于现有无需微调的PTQ方法，达到与微调基线相当的精度。在LLaMA-2和LLaMA-3系列模型上实现最先进的量化精度，同时提供高达4倍的加速。在MoE模型Mixtral-8x7B上，2.5小时内完成量化，困惑度降低0.4，准确率提高8%。相比其他低秩量化方法，LoPRo以显著更低的秩实现更优的精度，同时保持高推理效率和最小额外延迟。

Conclusion: LoPRo是一种高效的无微调后训练量化方法，通过创新的列旋转技术和快速低秩分解，在保持高推理效率的同时实现了优异的量化精度，特别适用于大规模语言模型的高效部署。

Abstract: Post-training quantization (PTQ) enables effective model compression while preserving relatively high accuracy. Current weight-only PTQ methods primarily focus on the challenging sub-3-bit regime, where approaches often suffer significant accuracy degradation, typically requiring fine-tuning to achieve competitive performance. In this work, we revisit the fundamental characteristics of weight quantization and analyze the challenges in quantizing the residual matrix under low-rank approximation. We propose LoPRo, a novel fine-tuning-free PTQ algorithm that enhances residual matrix quantization by applying block-wise permutation and Walsh-Hadamard transformations to rotate columns of similar importance, while explicitly preserving the quantization accuracy of the most salient column blocks. Furthermore, we introduce a mixed-precision fast low-rank decomposition based on rank-1 sketch (R1SVD) to further minimize quantization costs. Experiments demonstrate that LoPRo outperforms existing fine-tuning-free PTQ methods at both 2-bit and 3-bit quantization, achieving accuracy comparable to fine-tuning baselines. Specifically, LoPRo achieves state-of-the-art quantization accuracy on LLaMA-2 and LLaMA-3 series models while delivering up to a 4$\times$ speedup. In the MoE model Mixtral-8x7B, LoPRo completes quantization within 2.5 hours, simultaneously reducing perplexity by 0.4$\downarrow$ and improving accuracy by 8\%$\uparrow$. Moreover, compared to other low-rank quantization methods, LoPRo achieves superior accuracy with a significantly lower rank, while maintaining high inference efficiency and minimal additional latency.

</details>


### [123] [Rethinking Divisive Hierarchical Clustering from a Distributional Perspective](https://arxiv.org/abs/2601.19718)
*Kaifeng Zhang,Kai Ming Ting,Tianrun Liang,Qiuran Zhao*

Main category: cs.LG

TL;DR: 论文揭示了当前基于目标的划分式层次聚类方法存在三个缺陷，提出使用分布核代替集合导向准则来解决这些问题，并证明了新方法在理论和实证上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于目标的划分式层次聚类方法产生的树状图缺乏三个期望属性：无不当分裂、相似聚类归入同一子集、与真实对应关系一致。这些缺陷源于使用集合导向的二分评估准则。

Method: 提出使用分布核代替集合导向准则，实现新的分布导向目标：最大化所有聚类的总相似度。理论分析证明该方法能保证TSC的下界。

Result: 在人工和空间转录组学数据集上的实证评估显示，该方法能创建与生物区域一致的树状图，而其他竞争方法失败。理论分析表明生成的树状图能保证TSC的下界。

Conclusion: 通过使用分布核替代集合导向准则，解决了当前划分式层次聚类方法的缺陷，实现了更符合期望属性的聚类结果，在空间转录组学等生物信息学应用中表现出色。

Abstract: We uncover that current objective-based Divisive Hierarchical Clustering (DHC) methods produce a dendrogram that does not have three desired properties i.e., no unwarranted splitting, group similar clusters into a same subset, ground-truth correspondence. This shortcoming has their root cause in using a set-oriented bisecting assessment criterion. We show that this shortcoming can be addressed by using a distributional kernel, instead of the set-oriented criterion; and the resultant clusters achieve a new distribution-oriented objective to maximize the total similarity of all clusters (TSC). Our theoretical analysis shows that the resultant dendrogram guarantees a lower bound of TSC. The empirical evaluation shows the effectiveness of our proposed method on artificial and Spatial Transcriptomics (bioinformatics) datasets. Our proposed method successfully creates a dendrogram that is consistent with the biological regions in a Spatial Transcriptomics dataset, whereas other contenders fail.

</details>


### [124] [Stability and Generalization of Nonconvex Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2601.19730)
*Hongxu Chen,Ke Wei,Xiaoming Yuan,Luo Luo*

Main category: cs.LG

TL;DR: 提出一个在重尾梯度噪声下建立泛化界的一般框架，基于截断论证和算法稳定性，适用于p∈(1,2]阶有界中心矩假设，并应用于多种随机优化算法。


<details>
  <summary>Details</summary>
Motivation: 经验证据表明，具有重尾梯度噪声的随机优化比标准有界梯度方差噪声更适合描述机器学习模型的训练。现有工作主要关注优化误差收敛，而对重尾噪声下的泛化界分析有限。

Method: 开发一个建立重尾噪声下泛化界的一般框架：引入截断论证，基于算法稳定性在有界p阶中心矩(p∈(1,2])假设下获得泛化误差界。基于此框架分析多种算法的稳定性与泛化性。

Result: 为多种流行的重尾噪声下随机算法提供了稳定性与泛化分析，包括裁剪和归一化随机梯度下降及其小批量和动量变体。

Conclusion: 该框架填补了重尾梯度噪声下泛化理论分析的空白，为实际训练中常见的重尾噪声场景提供了理论保证，扩展了随机优化算法的泛化分析范围。

Abstract: The empirical evidence indicates that stochastic optimization with heavy-tailed gradient noise is more appropriate to characterize the training of machine learning models than that with standard bounded gradient variance noise. Most existing works on this phenomenon focus on the convergence of optimization errors, while the analysis for generalization bounds under the heavy-tailed gradient noise remains limited. In this paper, we develop a general framework for establishing generalization bounds under heavy-tailed noise. Specifically, we introduce a truncation argument to achieve the generalization error bound based on the algorithmic stability under the assumption of bounded $p$th centered moment with $p\in(1,2]$. Building on this framework, we further provide the stability and generalization analysis for several popular stochastic algorithms under heavy-tailed noise, including clipped and normalized stochastic gradient descent, as well as their mini-batch and momentum variants.

</details>


### [125] [Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers](https://arxiv.org/abs/2601.19788)
*Sixing Tan,Xianmin Liu*

Main category: cs.LG

TL;DR: FedKACE：针对流式联邦持续学习场景的新框架，解决类别重叠、无任务标识符下的知识混淆问题，通过自适应推理模型切换、梯度平衡重放和核谱边界缓冲区实现个性化与泛化的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于批处理的联邦持续学习方法无法适应流式场景，特别是当新旧数据存在类别重叠且缺乏任务标识符时，会导致新旧知识难以区分、样本任务分配不确定以及知识混淆问题。

Method: 提出流式联邦持续学习设置，并设计FedKACE框架：1) 自适应推理模型切换机制，实现从局部模型到全局模型的单向切换以平衡个性化与泛化；2) 自适应梯度平衡重放方案，在重叠类别场景下协调新知识学习和旧知识保留；3) 核谱边界缓冲区维护，保留高信息量和高边界影响力的样本以优化跨轮次知识保留。

Result: 在多种场景下的实验和遗憾分析证明了FedKACE的有效性，能够更好地处理流式联邦持续学习中的挑战。

Conclusion: FedKACE成功解决了流式联邦持续学习中的关键问题，通过创新的自适应机制在类别重叠、无任务标识符的场景下实现了更好的知识保留与学习平衡。

Abstract: Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task assignments for samples, and knowledge confusion.To address this, we propose streaming federated continual learning setting: per federated learning (FL) round, clients process streaming data with disjoint samples and potentially overlapping categories without task identifiers, necessitating sustained inference capability for all prior categories after each FL round.Next, we introduce FedKACE: 1) an adaptive inference model switching mechanism that enables unidirectional switching from local model to global model to achieve a trade-off between personalization and generalization; 2) a adaptive gradient-balanced replay scheme that reconciles new knowledge learning and old knowledge retention under overlapping-class scenarios; 3) a kernel spectral boundary buffer maintenance that preserves high-information and high-boundary-influence samples to optimize cross-round knowledge retention. Experiments across multiple scenarios and regret analysis demonstrate the effectiveness of FedKACE.

</details>


### [126] [Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation](https://arxiv.org/abs/2601.19794)
*Ganesh Sundaram,Jonas Ulmen,Daniel Görges*

Main category: cs.LG

TL;DR: 提出基于梯度的组件感知剪枝框架，使用三种重要性度量（梯度累积、Fisher信息、贝叶斯不确定性）来识别神经控制器中的关键结构依赖，相比传统基于范数的剪枝方法能更好地捕捉功能重要性。


<details>
  <summary>Details</summary>
Motivation: 从单体到多组件神经架构的转变带来了计算复杂度挑战，传统基于范数的结构化剪枝方法往往无法准确捕捉参数组的功能重要性，需要更精细的重要性评估方法。

Method: 提出组件感知剪枝框架，在训练过程中利用梯度信息计算三种重要性度量：梯度累积、Fisher信息和贝叶斯不确定性，以识别参数组之间的结构依赖和动态重要性变化。

Result: 在自编码器和TD-MPC智能体上的实验表明，该框架能够揭示传统静态启发式方法常忽略的关键结构依赖和重要性动态变化，支持更明智的压缩决策。

Conclusion: 基于梯度的组件感知剪枝框架比传统基于范数的剪枝方法更能准确评估多组件神经架构中参数组的功能重要性，为复杂神经控制器的压缩提供了更有效的工具。

Abstract: The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions.

</details>


### [127] [A Multi-directional Meta-Learning Framework for Class-Generalizable Anomaly Detection](https://arxiv.org/abs/2601.19833)
*Padmaksha Roy,Lamine Mili,Almuatazbellah Boker*

Main category: cs.LG

TL;DR: 提出一种多方向元学习算法，通过内层学习正常数据流形、外层利用少量异常样本进行元调优，实现类别泛化的异常检测


<details>
  <summary>Details</summary>
Motivation: 解决类别泛化异常检测问题，即开发统一模型来检测完全未见过的异常类别（OOD类），同时面临异常数据稀缺且标注成本高的挑战

Method: 提出多方向元学习框架：内层学习正常数据流形（表示学习）；外层利用少量异常样本进行元调优，最大化正常与异常样本间的softmax置信度边界（决策面校准）；通过多轮迭代实现双向优化

Result: 该方法通过多方向训练增强的双层优化，实现了对未见异常类别更强的泛化能力

Conclusion: 多方向元学习框架能够有效解决类别泛化异常检测问题，在异常数据稀缺的情况下实现对未见异常类别的鲁棒检测

Abstract: In this paper, we address the problem of class-generalizable anomaly detection, where the objective is to develop a unified model by focusing our learning on the available normal data and a small amount of anomaly data in order to detect the completely unseen anomalies, also referred to as the out-of-distribution (OOD) classes. Adding to this challenge is the fact that the anomaly data is rare and costly to label. To achieve this, we propose a multidirectional meta-learning algorithm -- at the inner level, the model aims to learn the manifold of the normal data (representation); at the outer level, the model is meta-tuned with a few anomaly samples to maximize the softmax confidence margin between the normal and anomaly samples (decision surface calibration), treating normals as in-distribution (ID) and anomalies as out-of-distribution (OOD). By iteratively repeating this process over multiple episodes of predominantly normal and a small number of anomaly samples, we realize a multidirectional meta-learning framework. This two-level optimization, enhanced by multidirectional training, enables stronger generalization to unseen anomaly classes.

</details>


### [128] [Calibration without Ground Truth](https://arxiv.org/abs/2601.19862)
*Yuqing Kong,Mingyu Song,Yizhou Wang,Yifan Wu*

Main category: cs.LG

TL;DR: 提出无需标签的后处理框架，利用弱但校准好的参考模型改进强但校准差的模型，保证在任意proper loss下严格提升性能


<details>
  <summary>Details</summary>
Motivation: 公开人类文本将在未来十年内耗尽，因此无需真实标签改进模型变得日益重要。现有强模型存在校准问题，而弱模型可能校准更好

Method: 提出标签自由后处理框架，基于强模型与参考模型是否相互校准的条件分析，连接经济学中的套利和无交易结果，开发高效的Bregman投影算法保证最坏情况损失减少

Result: 在不同规模的代表性LLM上实验表明，该标签自由方法显著减少proper loss和校准误差，性能与监督基线相当

Conclusion: 该框架为无需标签改进模型提供了理论保证和实用算法，在数据稀缺时代具有重要价值

Abstract: Villalobos et al. [2024] predict that publicly available human text will be exhausted within the next decade. Thus, improving models without access to ground-truth labels becomes increasingly important. We propose a label-free post-processing framework that improves a strong but miscalibrated model using a weaker yet better-calibrated reference. Our framework guarantees a strict performance improvement under any proper loss. Our approach is based on a characterization of when strict improvement is possible: when the strong and reference models are not mutually calibrated. We formalize this condition, connect it to arbitrage and no-trade results from economics, and develop an efficient Bregman projection algorithm that guarantees worst-case loss reduction without labels. Experiments on representative LLMs across varying scales demonstrate that our label-free method significantly reduces proper losses and calibration errors, achieving performance competitive with supervised baselines.

</details>


### [129] [Bandits in Flux: Adversarial Constraints in Dynamic Environments](https://arxiv.org/abs/2601.19867)
*Tareq Si Salem*

Main category: cs.LG

TL;DR: 提出了一种用于时变约束下对抗性多臂老虎机的新颖原始-对偶算法，该算法通过集成梯度估计器和有效约束处理扩展了在线镜像下降，实现了次线性动态遗憾和约束违反。


<details>
  <summary>Details</summary>
Motivation: 研究时变约束下的对抗性多臂老虎机问题，该场景受到众多现实世界应用的驱动，需要处理动态环境中的约束条件。

Method: 提出一种新颖的原始-对偶算法，通过集成合适的梯度估计器和有效的约束处理机制来扩展在线镜像下降方法。

Result: 理论保证表明该策略实现了次线性动态遗憾和次线性约束违反，在遗憾和约束违反方面达到了最先进的性能，实证评估证明了方法的优越性。

Conclusion: 该算法成功解决了时变约束下的对抗性多臂老虎机问题，在理论和实证上都表现出色，为复杂约束环境下的在线决策提供了有效解决方案。

Abstract: We investigate the challenging problem of adversarial multi-armed bandits operating under time-varying constraints, a scenario motivated by numerous real-world applications. To address this complex setting, we propose a novel primal-dual algorithm that extends online mirror descent through the incorporation of suitable gradient estimators and effective constraint handling. We provide theoretical guarantees establishing sublinear dynamic regret and sublinear constraint violation for our proposed policy. Our algorithm achieves state-of-the-art performance in terms of both regret and constraint violation. Empirical evaluations demonstrate the superiority of our approach.

</details>


### [130] [Post-LayerNorm Is Back: Stable, ExpressivE, and Deep](https://arxiv.org/abs/2601.19895)
*Chen Chen,Lai Wei*

Main category: cs.LG

TL;DR: Keel提出了一种使用Highway-style连接替代传统残差连接的Post-LN Transformer架构，解决了深度训练中的梯度消失问题，实现了超过1000层的稳定训练，性能优于Pre-LN。


<details>
  <summary>Details</summary>
Motivation: 当前LLM扩展面临瓶颈：宽度扩展收益递减，上下文长度扩展不改善基本表达能力，而深度扩展理论上具有更优表达能力但现有Transformer架构在极端深度下训练不稳定。Post-LN因不稳定被Pre-LN取代，但其核心失败模式源于残差路径导致的梯度消失问题。

Method: 提出Keel架构，在Post-LN Transformer中用Highway-style连接替代传统的ResNet-style残差路径。这种修改保持了通过残差分支的梯度流，防止信号从顶层到底层的消失。该方法无需特殊初始化或复杂优化技巧即可实现极端深度下的稳定训练。

Result: Keel能够在超过1000层的深度下稳健训练，在困惑度和深度扩展特性上持续优于Pre-LN。实验表明Post-LN配合Highway-style连接为构建深度可扩展LLM提供了简单有效的基础。

Conclusion: Post-LN配合Highway-style连接为解决深度扩展问题提供了有效方案，为未来无限深度架构开辟了可能性。该方法简单有效，无需复杂优化技巧，有望成为构建深度可扩展LLM的新基础。

Abstract: Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.

</details>


### [131] [Self-Distillation Enables Continual Learning](https://arxiv.org/abs/2601.19897)
*Idan Shenfeld,Mehul Damani,Jonas Hübotter,Pulkit Agrawal*

Main category: cs.LG

TL;DR: SDFT是一种通过自我蒸馏实现从演示中进行在线策略学习的方法，用于解决基础模型的持续学习问题，相比监督微调能更好地减少灾难性遗忘并提高新任务性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习是基础模型面临的基本挑战，现有方法存在局限：在线强化学习需要显式奖励函数（通常不可得），而主要替代方案监督微调是离线策略的，容易导致灾难性遗忘。

Method: 提出自我蒸馏微调（SDFT），利用上下文学习，通过演示条件模型作为自己的教师，生成在线策略训练信号，在获取新技能的同时保持原有能力。

Result: 在技能学习和知识获取任务中，SDFT始终优于监督微调，获得更高的新任务准确率，同时显著减少灾难性遗忘。在顺序学习实验中，SDFT使单个模型能够随时间累积多个技能而无性能退化。

Conclusion: SDFT建立了一种实用的持续学习路径，通过在线策略蒸馏直接从演示中学习，为减少灾难性遗忘和实现技能累积提供了有效解决方案。

Abstract: Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [132] [Variable Elliptic Structures on the Plane: Transport Dynamics, Rigidity, and Function Theory](https://arxiv.org/abs/2601.19274)
*Daniel Alayón-Solarz*

Main category: math.CV

TL;DR: 论文研究平面上的变系数椭圆结构，通过二次关系定义，推导出结构系数的通用输运系统，在椭圆区域简化为强迫复Burgers方程，识别刚性条件，并展示经典复分析工具在变系数设置中的再现。


<details>
  <summary>Details</summary>
Motivation: 研究平面上的变系数椭圆结构，旨在理解结构系数的变化规律，探索经典复分析工具在变系数环境中的适用性，建立可变系数椭圆结构与经典复分析之间的桥梁。

Method: 通过微分结构关系推导结构系数的导数表达式，建立通用输运系统，在椭圆区域简化为强迫复Burgers方程，识别刚性条件，分析广义Cauchy-Riemann算子的性质，展示二阶算子的因式分解。

Result: 建立了变系数椭圆结构的统一理论框架，推导出结构系数的输运系统，识别了刚性条件，证明了在刚性条件下广义Cauchy-Riemann算子满足Leibniz规则，二阶算子可分解为一阶分量，经典复分析工具（Cauchy-Pompeiu公式、积分表示、椭圆二阶算子）在变系数设置中得以再现。

Conclusion: 论文成功建立了平面变系数椭圆结构的系统理论，揭示了结构系数的输运动力学、刚性条件与函数理论之间的深刻联系，为经典复分析工具在变系数环境中的应用提供了理论基础，强调了可积性机制的透明性和计算直接性。

Abstract: We study variable elliptic structures in the plane defined by a smoothly varying quadratic relation i^2 + beta(x,y) i + alpha(x,y) = 0, and the associated first order operator dbar = 1/2 (dx + i dy). Differentiating the structure relation yields explicit expressions for the derivatives of i(x,y) in terms of the coefficient functions alpha and beta, leading to a universal transport system governing their admissible variations. In the elliptic regime this system reduces to a forced complex Burgers equation for a scalar spectral parameter encoding the structure coefficients. We identify a rigidity condition under which the transport becomes conservative, and show that in this regime the generalized Cauchy Riemann operator satisfies a Leibniz rule and admits a factorization of the associated second order operator into first order components. As a consequence, classical tools of planar complex analysis, including Cauchy Pompeiu type formulas, integral representations, and elliptic second order operators, reappear in a variable coefficient setting with explicit structure. The theory is developed at the level of direct computation, emphasizing transparency of the integrability mechanism and the interplay between transport dynamics, rigidity, and function theory.

</details>
