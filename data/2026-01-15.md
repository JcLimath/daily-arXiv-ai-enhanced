<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 47]
- [cs.AI](#cs.AI) [Total: 17]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 12]
- [cs.RO](#cs.RO) [Total: 11]
- [math.CV](#math.CV) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: SGFMs是一种基于物理启发的生成模型，使用连续场演化和多尺度小波表示替代传统的离散token序列和注意力机制，通过约束随机动力学实现文本和视频生成。


<details>
  <summary>Details</summary>
Motivation: 提出一种物理启发的替代方案，以克服基于transformer的大语言模型在处理长程相干性、多模态通用性和物理结构归纳偏置方面的局限性，将生成过程重新构想为连续场的演化而非离散token序列的处理。

Method: 采用场论本体论，将文本和视频统一为随机偏微分方程的轨迹；使用多尺度小波基表示实现稀疏性和尺度分离；设计约束随机流确保稳定性、相干性和不确定性传播；用局部算子、谱投影和Navier-Stokes类输运替代全局注意力机制。

Result: 提出了一种全新的生成架构，从根本上区别于自回归建模和基于扩散的方法，为下一代生成模型提供了实现长程相干性、多模态通用性和物理结构归纳偏置的原则性路径。

Conclusion: SGFMs通过将生成视为连续场在物理约束下的演化，为生成建模提供了基于连续性、几何和物理结构的新范式，有望解决当前基于离散token和注意力机制的模型的根本局限性。

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [2] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: 开发基于XGBoost的机器学习框架，用于预测尼泊尔证券交易所指数的日对数收益率，通过特征工程和超参数优化，在滚动窗口验证中优于传统基准模型。


<details>
  <summary>Details</summary>
Motivation: 针对尼泊尔证券交易所指数这一新兴市场的高波动性时间序列，传统线性模型难以捕捉非线性动态，需要开发更强大的机器学习框架进行准确预测。

Method: 使用XGBoost回归器构建预测模型，特征工程包括滞后对数收益率（最多30天）和技术指标（滚动波动率和14期RSI），通过Optuna进行超参数优化，采用时间序列交叉验证和滚动窗口验证（扩展窗口和固定长度窗口）避免前瞻偏差。

Result: 最优配置（扩展窗口+20个滞后项）在RMSE（0.013450）、MAE（0.009814）和方向准确率（65.15%）上均优于调优的ARIMA和Ridge回归基准模型，尽管R²值相对较低，但实现了显著的相对误差减少。

Conclusion: 梯度提升集成方法能有效建模新兴市场时间序列的非线性动态，为NEPSE指数预测建立了可复现的基准，特征重要性分析增强了模型可解释性。

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [3] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: DriftGuard是一个端到端的供应链预测漂移管理系统，通过多方法集成检测、层次化传播分析、SHAP根因诊断和成本感知重训练策略，解决概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 供应链预测模型随时间退化（概念漂移），导致库存问题而无系统预警。现有方法仅关注检测，忽略诊断和修复，且未考虑供应链数据的层次结构。需要端到端系统实现早期检测、根因解释和自动修正。

Method: 提出DriftGuard五模块框架：1)四种互补检测方法集成（基于误差监控、统计检验、自编码器异常检测、CUSUM变点分析）；2)层次化传播分析定位产品线漂移位置；3)SHAP分析诊断根因；4)成本感知重训练策略选择性更新受影响模型；5)端到端系统集成。

Result: 在M5零售数据集的30,000+时间序列上评估，DriftGuard在4.2天内达到97.8%检测召回率，通过针对性修复实现高达417倍投资回报率。

Conclusion: DriftGuard解决了供应链预测中概念漂移的完整生命周期管理问题，提供早期检测、根因诊断和选择性修复的端到端解决方案，显著优于现有方法和行业实践。

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [4] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: 本文通过Reverse Transition Kernel框架重新解释DDDM，将确定性去噪统一到概率形式化中，解决了分子扩散模型的采样效率、数值稳定性和结构保持问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在分子设计中表现出强大能力，但存在采样轨迹长、反向过程随机方差大、去噪动态结构感知有限等问题。DDDM通过确定性去噪步骤提高了效率，但其理论基础不明确。本文旨在通过RTK框架为DDDM提供理论解释，并解决分子扩散中的瓶颈问题。

Method: 采用Reverse Transition Kernel框架重新解释DDDM的反向过程，将其表达为近似核算子。该方法将确定性去噪过程视为噪声样本与干净样本之间的结构化传输映射优化。RTK视角确保数值稳定性、消除随机方差，并支持保持SE(3)等变性的可扩展对称保持去噪器。

Result: RTK引导的确定性去噪在GEOM-DRUGS数据集上实现了比随机扩散模型更快的收敛速度和更高的结构保真度，同时保持了化学有效性。该方法解决了数值稳定性、样本一致性和对称性保持等长期存在的瓶颈问题。

Conclusion: 通过RTK框架为DDDM提供了理论解释，将确定性和随机性扩散统一在共享的概率形式化中。该方法不仅阐明了确定性去噪实现高效推理的原因，还解决了分子扩散模型的实际瓶颈，为高效、稳定的分子设计提供了新途径。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [5] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 提出了一种在数据流滑动窗口中实施连续组公平性的新模型，通过块级粒度确保更细粒度的公平性，设计了基于草图的数据结构进行实时监控，并开发了高效的重新排序算法来纠正公平性违规。


<details>
  <summary>Details</summary>
Motivation: 当滑动窗口较大时，窗口级别的组公平性可能过于粗糙，无法确保窗口内部更细粒度的公平性。需要一种能够在数据流中连续监控和强制执行更细粒度公平性的方法，特别是在实时流处理场景中。

Method: 1) 提出块级组公平性模型，将每个滑动窗口划分为更细粒度的块；2) 设计基于草图的数据结构来高效监控每个滑动窗口是否满足块级公平性；3) 开发最优的重新排序算法，当检测到公平性违规时重新排列当前窗口中的元素。

Result: 在四个真实世界流处理场景中的评估显示：平均处理时间达到毫秒级，吞吐量约30,000查询/秒；重新排序算法在某些情况下将块级组公平性提高95%，在数据集上平均提高50-60%；定性研究证实块级公平性相比窗口级公平性具有优势。

Conclusion: 提出的块级组公平性模型能够在数据流中实现更细粒度的公平性保证，通过高效的监控和重新排序算法，在保持实时处理性能的同时显著改善公平性，为流处理系统中的公平性保障提供了实用解决方案。

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [6] [Optimising for Energy Efficiency and Performance in Machine Learning](https://arxiv.org/abs/2601.08991)
*Emile Dos Santos Ferreira,Neil D. Lawrence,Andrei Paleyes*

Main category: cs.LG

TL;DR: ECOpt是一个超参数调优器，专门优化机器学习模型的能源效率和性能，通过帕累托前沿量化两者权衡，帮助从业者在满足性能需求的同时降低能源消耗和环境影


<details>
  <summary>Details</summary>
Motivation: 机器学习模型规模扩大导致能源消耗和环境问题日益严重，但现有研究主要关注训练成本而忽略更大的推理成本，且缺乏提供可操作反馈的能源测量工具

Method: 开发了ECOpt超参数调优器，优化能源效率和模型性能，通过量化两者权衡形成可解释的帕累托前沿，使从业者能在能源成本和环境影响方面做出明智决策

Result: 发现参数数量和浮点运算次数不能可靠地代表能源消耗；Transformer文本生成模型的能源效率在不同硬件上相对一致；发现了7个CIFAR-10模型在准确率和能源效率综合指标上优于现有最佳模型

Conclusion: ECOpt能产生净正面的环境影响，激励测量和发布机器学习模型的能源指标，为从业者提供在满足性能需求的同时降低能源消耗的实用工具

Abstract: The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback.
  To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations.
  Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.

</details>


### [7] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: 提出物理引导的反事实解释框架，用于太阳高能粒子事件预测，在保持物理合理性的同时提升解释质量与效率


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在空间天气预测中缺乏领域特定的可行性约束，反事实解释方法通常忽略物理合理性，需要开发既能保持预测准确性又符合物理原理的解释框架

Method: 提出物理引导的反事实解释框架，将领域知识融入反事实生成过程，确保解释符合物理原理，应用于太阳高能粒子事件预测的时间序列分类任务

Result: 相比DiCE等基线方法，动态时间规整距离减少80%以上，反事实解释稀疏性更高，运行时间减少近50%，同时确保生成的解释具有物理合理性和可操作性

Conclusion: 该框架能生成既有效又物理一致的反事实解释，为大数据环境中的可扩展反事实生成奠定基础，提升科学领域模型的可解释性和实用性

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [8] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: WSD学习率调度器在语言模型中表现出色，但作者探究其性能是否特定于Transformer架构。通过比较语言模型和CNN的训练路径，发现两者在训练信号、优化器路径特征和锐度动态方面具有相似性，表明不同非凸问题的损失景观具有共享的几何特性。


<details>
  <summary>Details</summary>
Motivation: 研究WSD学习率调度器的优异性能是否特定于Transformer语言模型，还是反映了更普遍的优化几何特性。通过比较不同架构的训练动态，探索高维非凸优化问题的共享几何特征。

Method: 比较WSD路径下Adam优化器在Pythia类语言模型和CIFAR10图像分类小CNN上的表现。分析训练信号、优化器路径特征和锐度动态的相似性。

Result: 观察到两种架构在大多数训练信号、优化器路径特征和锐度动态方面具有定性相似性。这表明新旧非凸问题的损失景观具有共享的几何特性。

Conclusion: WSD的性能优势并非Transformer特有，而是反映了更普遍的优化几何特性。这为理解高维优化问题的几何结构提供了新视角，并指出了未来研究方向。

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [9] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: 该论文系统比较了传统深度学习与元学习在时间序列分类中应对数据偏移的能力，发现元学习在小数据和简单模型时表现更好，但随着数据和模型规模增加优势减弱，并提出了SeisTask地震基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据具有动态变化特性（数据偏移），导致传统深度学习模型性能快速下降，需要昂贵的重新标注和低效的重新训练。元学习能够快速适应新数据，为解决这一挑战提供了有前景的替代方案。

Method: 系统比较传统深度学习（带微调）与基于优化的元学习算法，评估它们在时间序列分类中处理数据偏移的能力。引入受控的、面向任务的地震基准数据集（SeisTask），分析不同数据量、模型容量和任务多样性条件下的性能表现。

Result: 元学习通常在数据稀缺和模型架构较小时，能实现更快、更稳定的适应，并减少过拟合。随着数据可用性和模型容量增加，其优势减弱，传统深度学习（带微调）表现相当。任务多样性对元学习的影响表明，训练与测试分布的对齐比多样性本身更能驱动性能提升。

Conclusion: 该研究系统评估了元学习在数据偏移条件下何时以及为何优于传统深度学习，为时间序列领域的自适应学习研究提供了SeisTask基准数据集，并揭示了元学习优势的具体条件。

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [10] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: 提出SCaLE算法解决带无界度量移动成本的强盗在线凸优化问题，在随机环境中实现分布无关的次线性动态遗憾


<details>
  <summary>Details</summary>
Motivation: 解决强盗在线凸优化中无界度量移动成本的基本问题，特别是在高维动态二次命中成本和ℓ₂范数切换成本的噪声强盗反馈模型中

Method: 提出SCaLE算法，采用新颖的谱遗憾分析，分别量化特征值误差驱动的遗憾和特征基扰动驱动的遗憾，无需命中成本结构的先验知识

Result: 在一般随机环境中首次实现分布无关的次线性动态遗憾，数值实验验证了算法性能并突显统计一致性

Conclusion: SCaLE算法成功解决了无界度量移动成本的强盗在线凸优化问题，通过谱分析框架为高维动态环境提供了有效的解决方案

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [11] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: DIMVC-HIA：一种结合层次化插补与对齐的深度不完整多视图聚类框架，通过跨视图对比相似性估计缺失聚类分配，利用能量语义对齐提升簇内紧密度，在多种缺失率下表现优异。


<details>
  <summary>Details</summary>
Motivation: 不完整多视图聚类（IMVC）需要从存在部分观测的多视图数据中发现共享的聚类结构。核心挑战在于：1）准确插补缺失视图而不引入偏差；2）保持跨视图的语义一致性；3）维持簇内紧密度。现有方法难以同时解决这些挑战。

Method: 提出DIMVC-HIA框架，包含四个关键组件：1）视图特定自编码器提取潜在特征，结合视图共享聚类预测器生成软聚类分配；2）层次化插补模块：先基于跨视图对比相似性估计缺失聚类分配，再利用视图内、簇内统计重建缺失特征；3）基于能量的语义对齐模块：通过最小化低能量簇锚点周围的能量方差来促进簇内紧密度；4）对比分配对齐模块：增强跨视图一致性并鼓励置信度高、分离度好的聚类预测。

Result: 在基准数据集上的实验表明，该框架在不同缺失率下均实现了优越性能，验证了层次化插补与对齐策略的有效性。

Conclusion: DIMVC-HIA通过集成层次化插补和多种对齐机制，有效解决了不完整多视图聚类中的关键挑战，在准确插补缺失视图的同时保持了语义一致性和簇内紧密度，为IMVC任务提供了强有力的解决方案。

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


### [12] [Resolving Predictive Multiplicity for the Rashomon Set](https://arxiv.org/abs/2601.09071)
*Parian Haghighat,Hadis Anahideh,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出三种减少Rashomon集合预测不一致性的方法：异常值校正、局部修补和成对调和，以降低模型间预测分歧同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 在预测任务中，存在多个准确率相近但预测结果不同的模型（Rashomon集合），这种预测不一致性在高风险应用中会削弱信任度，需要解决。

Method: 1. 异常值校正：修正那些所有好模型都无法正确预测的异常值，减少局部区域预测方差；2. 局部修补：在测试点周围局部区域检测并修正模型偏差；3. 成对调和：识别在测试点周围区域存在分歧的模型对，修改不一致的预测以减少偏差。

Result: 在多个数据集上的实验表明，这些方法能够有效降低预测分歧指标，同时保持具有竞争力的准确率。

Conclusion: 提出的三种方法可以单独或组合使用，有效减少Rashomon集合的预测不一致性，调和后的预测可以蒸馏为单个可解释模型用于实际部署。

Abstract: The existence of multiple, equally accurate models for a given predictive task leads to predictive multiplicity, where a ``Rashomon set'' of models achieve similar accuracy but diverges in their individual predictions. This inconsistency undermines trust in high-stakes applications where we want consistent predictions. We propose three approaches to reduce inconsistency among predictions for the members of the Rashomon set. The first approach is \textbf{outlier correction}. An outlier has a label that none of the good models are capable of predicting correctly. Outliers can cause the Rashomon set to have high variance predictions in a local area, so fixing them can lower variance. Our second approach is local patching. In a local region around a test point, models may disagree with each other because some of them are biased. We can detect and fix such biases using a validation set, which also reduces multiplicity. Our third approach is pairwise reconciliation, where we find pairs of models that disagree on a region around the test point. We modify predictions that disagree, making them less biased. These three approaches can be used together or separately, and they each have distinct advantages. The reconciled predictions can then be distilled into a single interpretable model for real-world deployment. In experiments across multiple datasets, our methods reduce disagreement metrics while maintaining competitive accuracy.

</details>


### [13] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: SRT是一种利用树结构缓存加速语言模型在线强化学习的方法，通过存储历史生成内容作为草稿模型进行推测解码，显著减少生成延迟和推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在线强化学习中生成延迟高、计算成本大的问题，利用训练过程中相同提示下生成内容的相似性来加速学习过程。

Method: 为每个提示构建树结构缓存存储历史生成内容，在生成时作为草稿模型进行推测解码；通过在线更新缓存和利用GPU空闲时间进行前瞻生成来保持缓存新鲜度。

Result: 在标准RL流程（PPO、GRPO、DAPO）和多轮对话设置中，SRT能显著降低生成延迟和单token推理成本，在rollout阶段实现最高2.08倍的实时加速。

Conclusion: SRT是一种简单有效的模型无关方法，能在不牺牲分布正确性的前提下，显著加速语言模型的在线强化学习过程。

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [14] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: MMR-GRPO通过集成最大边际相关性来基于完成多样性重新加权奖励，减少语义冗余完成，加速GRPO训练收敛


<details>
  <summary>Details</summary>
Motivation: GRPO依赖每个提示的多个完成，训练计算成本高；现有方法减少了达到峰值性能所需的训练步数，但每步成本增加导致总体训练时间未减少甚至增加

Method: 提出MMR-GRPO，集成最大边际相关性（MMR）基于完成多样性重新加权奖励，优先考虑多样化解以获得更多信息更新，加速收敛

Result: 在三个模型规模（1.5B、7B、8B）、三个GRPO变体和五个数学推理基准上的评估显示，MMR-GRPO达到可比峰值性能，平均减少47.9%训练步数和70.2%训练时间

Conclusion: MMR-GRPO通过优先考虑多样化解有效加速GRPO训练，在不同模型、方法和基准上获得一致的性能提升，将发布代码、训练模型和实验协议

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [15] [Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning](https://arxiv.org/abs/2601.09088)
*Shaotian Yan,Kaiyuan Liu,Chen Shen,Bing Wang,Sinan Fan,Jun Zhang,Yue Wu,Zheng Wang,Jieping Ye*

Main category: cs.LG

TL;DR: DASD-4B-Thinking是一个轻量级但能力强大的开源推理模型，在数学、科学推理和代码生成基准测试中达到SOTA性能，仅使用448K训练样本，远少于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前广泛采用的序列级蒸馏范式存在三个关键局限：1)教师序列级分布表示不足；2)教师输出分布与学生学习能力不匹配；3)教师强制训练与自回归推理之间的暴露偏差。这些反映了蒸馏过程中缺乏明确的师生交互，未充分利用蒸馏本质。

Method: 提出增强的序列级蒸馏训练流程，通过方法论创新解决现有问题，使学生模型能够学习教师的完整输出分布以继承其泛化能力，仅使用448K训练样本。

Result: DASD-4B-Thinking在数学、科学推理和代码生成等挑战性基准测试中，在同等规模的开源模型中达到SOTA性能，甚至超越了一些更大的模型。

Conclusion: 通过重新审视序列级蒸馏范式并解决其核心局限，提出的增强蒸馏方法能够以极少的训练数据实现卓越性能，为社区研究提供了有价值的模型和数据集。

Abstract: In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.

</details>


### [16] [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)
*Zhixiang Liang,Beichen Huang,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: STEP：基于步骤级评估与剪枝的推理加速框架，通过轻量级步骤评分器评估推理步骤质量，在KV缓存饱和时动态剪枝低质量推理轨迹，显著降低延迟同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过生成多个推理轨迹来增强推理能力，但长推理轨迹与多重采样结合导致计算量大、端到端延迟高。现有基于相似性或置信度的剪枝方法信号不可靠，无法准确指示轨迹质量。

Method: 提出STEP框架：1）训练轻量级步骤评分器，利用隐藏状态评估推理步骤质量；2）设计GPU内存感知剪枝策略，当KV缓存饱和时动态剪枝无前景的推理轨迹；3）在推理过程中实时评估和剪枝。

Result: 在多个挑战性推理基准测试中，STEP相比self-consistency方法平均减少45%-70%的端到端推理延迟，同时提高了推理准确率。

Conclusion: STEP通过步骤级评估和动态剪枝有效解决了LLM推理加速问题，在显著降低延迟的同时保持甚至提升推理性能，为高效推理系统提供了新思路。

Abstract: Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

</details>


### [17] [Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion](https://arxiv.org/abs/2601.09103)
*Haijian Shao,Wei Liu,Xing Deng,Daze Lu*

Main category: cs.LG

TL;DR: 提出基于小波变换特征融合的ECG分类方法，同时解决类别不平衡和噪声问题，在CPSC 2018数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 心电图数据存在类别不平衡问题（某些心脏疾病样本稀少），同时采集过程中的噪声干扰ECG处理，这阻碍了深度学习ECG分类算法的效果和鲁棒性

Method: 采用基于小波变换的特征融合方法，特别是基于小波变换的类间融合，生成训练特征库和测试集特征库；将原始训练和测试数据与各自的特征数据库合并，得到更平衡的训练和测试数据集

Result: 在CPSC 2018数据集上，对Normal、AF、I-AVB、LBBB、RBBB、PAC、PVC、STD、STE等类别的识别准确率分别达到99%、98%、97%、98%、96%、92%、93%；各类别平均识别准确率在92%到98%之间，超越了已知算法

Conclusion: 提出的数据融合方法能有效解决ECG分类中的类别不平衡和噪声问题，在CPSC 2018数据集上实现了卓越的分类精度，为自动化ECG处理提供了有效的解决方案

Abstract: Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.

</details>


### [18] [EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge](https://arxiv.org/abs/2601.09142)
*Shijian Ma,Yan Lin,Yi Yang*

Main category: cs.LG

TL;DR: 论文提出了EvasionBench基准和Eva-4B模型，用于检测财报电话会议中的回避性回答，通过多模型标注框架挖掘边界案例，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 检测财报电话会议中的回避性回答对财务透明度至关重要，但缺乏大规模基准阻碍了研究进展，需要构建高质量数据集和有效检测方法。

Method: 提出多模型标注框架：利用前沿LLM之间的分歧识别困难样本，通过法官模型解决标注冲突；构建包含30,000训练样本和1,000人工标注测试样本的EvasionBench；训练4B参数的Eva-4B模型。

Result: EvasionBench测试集Cohen's Kappa为0.835；分歧挖掘方法比单模型蒸馏性能提升2.4%；Eva-4B模型准确率达81.3%，比基础模型提升25个百分点，接近前沿LLM性能但推理成本大幅降低。

Conclusion: 多模型标注框架通过挖掘边界案例有效提升回避性回答检测性能，分歧挖掘可作为隐式正则化；Eva-4B模型在保持高性能的同时显著降低推理成本，为财务透明度分析提供实用工具。

Abstract: Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.

</details>


### [19] [Discrete Solution Operator Learning for Geometry-Dependent PDEs](https://arxiv.org/abs/2601.09143)
*Jinshuai Bai,Haolin Li,Zahra Sharif Khodaei,M. H. Aliabadi,YuanTong Gu,Xi-Qiao Feng*

Main category: cs.LG

TL;DR: DiSOL是一种学习离散解算程序而非连续函数空间算子的新范式，通过分解为局部贡献编码、多尺度组装和隐式重构等阶段，适应几何相关的离散结构变化。


<details>
  <summary>Details</summary>
Motivation: 在工程应用中，几何变化会引发离散结构变化（拓扑变化、边界条件突变、计算域变化），这些变化破坏了连续函数空间算子学习所需的平滑变化前提，需要新的学习范式来处理几何主导的PDE求解问题。

Method: DiSOL将求解器分解为三个可学习阶段：1) 局部贡献编码（类似经典离散化），2) 多尺度组装，3) 在嵌入网格上的隐式解重构。这种方法保持程序级一致性，同时适应几何相关的离散结构。

Result: 在几何相关的泊松方程、对流扩散、线性弹性以及时空热传导问题中，DiSOL在分布内和强分布外几何（包括不连续边界和拓扑变化）下都能产生稳定且准确的预测。

Conclusion: DiSOL展示了在几何主导区域中程序化算子表示的必要性，将离散解算子学习定位为科学机器学习中一个独特且互补的方向，为处理几何相关离散结构变化提供了有效框架。

Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.

</details>


### [20] [KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education](https://arxiv.org/abs/2601.09156)
*Woojin Kim,Changkwon Lee,Hyeoncheol Kim*

Main category: cs.LG

TL;DR: 该论文提出KTCF方法，为知识追踪生成反事实解释，并将其转化为教育指令，以提升AI在教育中的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能改进教学需要更好的适应性和可扩展性。知识追踪在教育中具有重要应用价值，但需要可解释的AI方法。反事实解释具有可操作性、因果性、局部性且易于教育利益相关者理解，因此被引入知识追踪领域。

Method: 提出KTCF方法，为知识追踪生成反事实解释，该方法考虑了知识概念之间的关系。同时设计后处理方案，将反事实解释转化为一系列教育指令序列。

Result: 在大规模教育数据集上的实验表明，KTCF方法在各项指标上比现有方法提升了5.7%到34%，表现出优越且稳健的性能。定性评估显示，生成的教育指令有助于减轻学生的学习负担。

Conclusion: 反事实解释有潜力推动AI在教育中的负责任和实际应用。未来可解释AI在知识追踪领域的研究可以从教育基础概念化和开发以利益相关者为中心的方法中受益。

Abstract: Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.

</details>


### [21] [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: 提出EBC算法，在固定置信度设置下解决Bandit Clustering问题，通过单步优化替代完整优化，实现计算高效且保持渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 现有Bandit Clustering算法在固定置信度设置下存在两个主要问题：1) 仅适用于高斯分布假设，限制了应用范围；2) 每次采样都需要解决完整优化问题，计算成本高昂。

Method: 提出EBC算法，采用向量参数化分布假设，每次采样仅向最优值方向单步优化而非完整求解。同时提出启发式变体EBC-H，进一步简化采样规则，利用停止规则中已计算的量进行臂选择。

Result: EBC算法在保持渐近最优性的同时显著提升计算效率，EBC-H进一步简化计算。仿真实验在合成和真实数据集上验证了算法性能优于现有方法。

Conclusion: EBC算法在更广泛的分布假设下解决了Bandit Clustering问题，通过高效的单步优化策略实现了计算效率与渐近最优性的平衡，为实际应用提供了可行方案。

Abstract: We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.

</details>


### [22] [Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation](https://arxiv.org/abs/2601.09165)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出基于概率域蒸馏框架的多教师集成知识蒸馏的公理化算子理论框架，定义五个核心公理来规范知识聚合算子，证明满足这些公理的算子族存在且不唯一，并建立算子无关的理论保证。


<details>
  <summary>Details</summary>
Motivation: 为多教师集成知识蒸馏提供理论基础，避免规定具体的聚合公式，而是通过公理化方法建立统一的数学框架，为从多样化前沿模型进行知识蒸馏提供理论依据。

Method: 基于Sparse-KD的概率域蒸馏框架，发展算子理论框架，定义五个核心公理：凸性、正性、连续性、权重单调性和温度一致性，研究满足这些公理的算子族特性。

Result: 证明满足五个公理的算子族存在且不唯一，建立算子无关的理论保证：多教师聚合能减少随机方差和系统性监督偏差，提供Jensen型边界、对数损失保证和安全衰减特性；对于权重线性的聚合算子，在标准独立性假设下获得经典集成方差减少结果。

Conclusion: 该框架为从多样化前沿模型进行多教师知识蒸馏提供了理论基础，同时允许多种有效的实现策略，为知识蒸馏的理论研究和实践应用建立了统一的数学基础。

Abstract: Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.
  Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.

</details>


### [23] [DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix](https://arxiv.org/abs/2601.09166)
*Sidhant R. Nair,Tanmay Sen,Mrinmay Sen*

Main category: cs.LG

TL;DR: DP-FedSOFIM：一种服务器端二阶优化框架，通过Fisher信息矩阵作为自然梯度预处理器，在差分隐私联邦学习中实现O(d)内存和计算复杂度，显著提升收敛速度


<details>
  <summary>Details</summary>
Motivation: 差分隐私联邦学习在严格隐私预算下收敛缓慢，现有二阶方法需要O(d^2)内存维护局部特征协方差矩阵，不适用于高维模型

Method: 提出DP-FedSOFIM框架，利用Fisher信息矩阵作为自然梯度预处理器，采用Sherman-Morrison公式实现高效矩阵求逆，客户端仅需O(d)内存，服务器端进行预处理器更新

Result: 理论分析证明服务器端预处理通过后处理定理保持(ε,δ)-差分隐私；在CIFAR-10上的实验表明，DP-FedSOFIM在多种隐私机制下相比一阶基线获得更优的测试准确率

Conclusion: DP-FedSOFIM在保持差分隐私的同时，通过服务器端二阶优化实现了高效收敛，解决了现有方法在高维模型中的内存瓶颈问题

Abstract: Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.

</details>


### [24] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: 论文提出几何稳定性作为表示分析的新维度，与相似性正交，量化表示在扰动下的鲁棒性，并开发Shesha框架进行测量


<details>
  <summary>Details</summary>
Motivation: 现有表示分析主要关注相似性（表示与外部参考的对齐程度），但相似性只能揭示表示了什么，无法评估表示结构的鲁棒性。需要一个新的维度来量化表示几何在扰动下的可靠性

Method: 引入几何稳定性概念，开发Shesha框架进行测量。在7个领域的2463个配置中进行实验，比较稳定性与相似性的关系，分析两者在机制上的差异（如移除主成分后的表现）

Result: 稳定性与相似性在经验上不相关（ρ≈0.01），机制上不同：相似性度量在移除主成分后崩溃，而稳定性对细粒度流形结构保持敏感。稳定性在安全监控中作为几何预警器，比CKA敏感近2倍；在可控性中预测线性可操纵性（ρ=0.89-0.96）；在模型选择中揭示迁移优化带来的几何代价

Conclusion: 几何稳定性量化了系统如何可靠地维持结构，是相似性的必要补充，为生物和计算系统的表示审计提供了新工具，在安全监控、可控性预测、模型选择等方面具有实际应用价值

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>


### [25] [$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness](https://arxiv.org/abs/2601.09176)
*Lang Xiong,Ning Liu,Ao Ren,Yuheng Bai,Haining Fang,BinYan Zhang,Zhe Jiang,Yujuan Tan,Duo Liu*

Main category: cs.LG

TL;DR: D²Prune是一种针对大语言模型的新型剪枝方法，通过双泰勒展开联合建模权重和激活扰动进行精确误差估计，并提出注意力感知的动态更新策略来保持注意力长尾分布，在多种LLM和ViT模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法存在两个关键限制：1) 忽略校准数据和测试数据之间的激活分布偏移，导致误差估计不准确；2) 忽视注意力模块中激活的长尾分布特性。这些限制影响了剪枝效果和模型性能。

Method: 提出D²Prune方法：1) 使用双泰勒展开方法联合建模权重和激活扰动，实现精确误差估计，指导剪枝掩码选择和权重更新；2) 提出注意力感知的动态更新策略，通过联合最小化注意力分布的KL散度和重构误差来保持长尾注意力模式。

Result: 在多种LLM（OPT-125M、LLaMA2/3、Qwen3）上的实验表明，D²Prune始终优于最先进方法。注意力动态更新机制也很好地泛化到ViT视觉模型（如DeiT），在ImageNet-1K上实现了优越的准确率。

Conclusion: D²Prune通过解决现有剪枝方法的两个关键限制，提供了一种更精确、更有效的模型压缩方法，在语言和视觉模型上都表现出优越性能，为LLM部署提供了有前景的解决方案。

Abstract: Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

</details>


### [26] [From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences](https://arxiv.org/abs/2601.09220)
*Xinzi Tan,Kejian Zhang,Junhan Yu,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出Hawkes Attention，一种基于多元Hawkes过程理论的新型注意力机制，用于标记时序点过程，通过可学习的类型特定神经核来调制查询、键和值投影，统一事件时序和内容交互。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法主要通过位置编码注入时间信息，依赖共享或参数化衰减结构，限制了捕捉异构和类型特定时间效应的能力。

Method: 从多元Hawkes过程理论推导出Hawkes Attention，使用可学习的每类型神经核调制查询、键和值投影，替代传统注意力中的相应部分，统一事件时序和内容交互。

Result: 实验结果显示该方法相比基线获得更好性能，除了通用标记时序点过程外，该注意力机制也能轻松应用于特定时间结构如时间序列预测。

Conclusion: Hawkes Attention通过基于Hawkes过程理论的神经核设计，有效捕捉了时间相关行为和类型特定激励模式，在标记时序点过程任务中表现出优越性能。

Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.

</details>


### [27] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: 提出GIFT方法解决大推理模型后训练中的优化不匹配问题，通过有限温度吉布斯初始化替代传统SFT，为后续RL提供更好的探索空间。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型的后训练范式（SFT+RL）存在内在优化不匹配：SFT的刚性监督导致分布坍缩，耗尽后续RL所需的探索空间。

Method: 将SFT重新表述为统一后训练框架，提出GIFT方法。将标准SFT视为抑制基础先验的零温度极限退化情况，而GIFT将监督作为有限温度能量势，建立分布桥梁确保整个后训练流程的目标一致性。

Result: 实验表明GIFT在用于RL初始化时显著优于标准SFT和其他竞争基线，为后训练实现全局最优提供了数学原理上的路径。

Conclusion: GIFT通过有限温度吉布斯初始化解决了SFT-RL后训练范式中的优化不匹配问题，为后续RL保留了必要的探索空间，实现了更好的全局优化性能。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [28] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: R4是一种基于评分的强化学习方法，使用排序均方误差损失函数，将教师评分作为序数目标，从轨迹-评分对中学习，在保证形式化理论保证的同时，相比现有方法需要更少的人类反馈。


<details>
  <summary>Details</summary>
Motivation: 强化学习在实际应用中面临奖励函数设计的瓶颈。虽然奖励学习通过人类反馈推断奖励函数是一种替代方案，但传统二元偏好反馈信息有限且认知负担重。基于评分的反馈能提供更丰富、认知要求更低的监督信息，但现有评分方法缺乏理论保证。

Method: R4采用排序均方误差损失函数，从轨迹-评分对数据集中学习。每个训练步骤中，采样一组轨迹，预测其回报，使用可微分排序算子生成软排名，然后优化软排名与教师评分之间的均方误差损失。该方法将评分视为序数目标而非基数目标。

Result: 在模拟人类反馈实验中，R4在OpenAI Gym和DeepMind Control Suite的机器人运动基准测试中，始终匹配或优于现有的评分和偏好强化学习方法，同时需要显著更少的反馈数据。

Conclusion: R4提供了一种有效的基于评分的强化学习方法，具有形式化理论保证（在温和假设下证明解集最小且完备），在实际应用中能减少人类反馈需求，提高学习效率。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [29] [XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs](https://arxiv.org/abs/2601.09237)
*Xinyang Chen,Huidong Jin,Yu Huang,Zaiwen Feng*

Main category: cs.LG

TL;DR: XLinear：一种基于MLP的轻量级时间序列预测模型，利用全局令牌作为枢纽整合外生变量信息，通过sigmoid激活的MLP提取时空模式，在保持高效的同时提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测模型通常假设变量重要性均匀，但现实应用中存在不对称因果关系和不同数据获取成本。低成本外生数据（如天气）可单向影响内生变量（如湖泊表面温度），利用这些关系可在数据可用时实现更有效预测。Transformer模型计算成本高且存在排列不变性问题，基于patch的变体虽提升效率但可能遗漏局部时间模式。

Method: 提出XLinear模型，基于多层感知机构建轻量级架构。使用内生变量衍生的全局令牌作为枢纽与外生变量交互，采用sigmoid激活的MLP同时提取时间模式和变量间依赖关系。预测头整合这些信号来预测内生序列。

Result: 在7个标准基准数据集和5个带外生输入的真实世界数据集上评估。相比最先进模型，XLinear在多元预测和受外生输入影响的单变量预测方面均提供更优的准确性和效率。

Conclusion: XLinear通过轻量级MLP架构有效利用外生变量信息，在保持计算效率的同时显著提升时间序列预测性能，特别适用于存在不对称因果关系和不同数据获取成本的实际应用场景。

Abstract: Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.

</details>


### [30] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: RIFT是一种奖励感知的微调框架，利用所有自生成样本（包括负样本），通过奖励加权损失实现数据高效的对齐方法


<details>
  <summary>Details</summary>
Motivation: 传统SFT依赖昂贵的专家数据，而RFT丢弃了有价值的负样本，导致数据效率低下。需要一种能够利用所有自生成样本（包括负样本）的微调方法

Method: 提出奖励感知微调(RIFT)框架，重新利用负轨迹，通过标量奖励对损失进行加权，从模型输出的正负轨迹中学习。引入稳定化损失公式解决朴素奖励集成导致的训练崩溃问题

Result: 在多个基础模型的数学基准测试中，RIFT始终优于RFT。证明RIFT是使用混合质量自生成数据进行对齐的鲁棒且数据高效的方法

Conclusion: RIFT提供了一种简单有效的框架，能够利用所有自生成样本进行对齐，解决了传统方法的数据效率问题，是使用混合质量自生成数据的鲁棒替代方案

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [31] [Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability](https://arxiv.org/abs/2601.09261)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 提出EIUR问题：在不可观测反馈可靠性下学习，标准鲁棒学习可能形成高置信度的系统错误信念。提出元认知调节作为解决方案，通过Monitor-Trust-Regulator分解和自我诊断机制来推断经验可信度。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒学习在不可观测反馈可靠性下存在根本局限：即使收敛稳定，也可能形成高置信度的系统错误信念。需要解决如何判断是否应该从某个经验中学习，而不仅仅是稳定学习的问题。

Method: 提出元认知调节框架，采用Monitor-Trust-Regulator分解。具体实例化为自我诊断机制，维护缓慢变化的经验信任变量，软调制学习更新，无需外部可靠性标签或显式腐败模型。

Result: 在EIUR机制下，自我诊断与改进的认知可识别性相关。在强化学习中实现校准的怀疑和系统腐败奖励下的恢复。在监督学习中揭示关键分离：性能恢复不意味着认知恢复，准确性可能反弹而内部信念动态仍被早期误导数据锁定。

Conclusion: MTR和自我诊断为不可观测可靠性下的自主学习提供了组织抽象和具体设计模板，用于内在可靠性评估，能够检测仅通过内省诊断才能发现的失败模式。

Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.

</details>


### [32] [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)
*Mianzhi Pan,JianFei Li,Peishuo Liu,Botian Wang,Yawen Ouyang,Yiming Rong,Hao Zhou,Jianbing Zhang*

Main category: cs.LG

TL;DR: MOF-LLM：首个基于大语言模型的块级MOF结构预测框架，通过空间感知预训练、监督微调和强化学习优化，显著提升MOF三维结构预测精度


<details>
  <summary>Details</summary>
Motivation: 金属有机框架（MOFs）在碳捕获和药物递送等领域有广泛应用，但准确预测其三维结构仍面临挑战。现有大语言模型在晶体生成方面虽有潜力，但难以处理MOFs的高原子复杂度。受深度生成模型中块级范式的启发，需要开发专门针对MOF结构预测的LLM框架

Method: 提出MOF-LLM框架，包含三个关键训练阶段：1）空间感知持续预训练（CPT）以融入空间先验知识；2）结构监督微调（SFT）进行精确结构学习；3）匹配驱动的强化学习（RL）采用软自适应策略优化（SAPO）提升结构稳定性。基于Qwen-3 8B模型进行适配

Result: MOF-LLM在综合实验中优于最先进的去噪基和LLM基方法，展现出卓越的采样效率。该框架显著增强了模型的空间推理能力，能够准确预测MOF结构

Conclusion: MOF-LLM成功将大语言模型应用于MOF结构预测领域，通过创新的块级训练范式解决了MOF高原子复杂度的挑战，为MOF设计和发现提供了高效准确的计算工具

Abstract: Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.

</details>


### [33] [Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data](https://arxiv.org/abs/2601.09304)
*Sota Sugawara,Yuji Kawamata,Akihiro Toyoda,Tomoru Nakayama,Yukihiko Okada*

Main category: cs.LG

TL;DR: DC-CFL是一种单轮通信的聚类联邦学习框架，通过数据协作分析实现客户端聚类和集群模型训练，在非IID数据下达到与多轮方法相当的精度


<details>
  <summary>Details</summary>
Motivation: 现有聚类联邦学习方法需要多轮通信进行集群估计和模型更新，在通信轮次受限的实际场景中实用性受限，需要开发单轮通信的解决方案

Method: 提出DC-CFL框架：1) 通过标签分布的总变差距离量化客户端间相似性；2) 使用层次聚类估计客户端集群；3) 通过数据协作分析进行集群内学习；所有步骤仅需一轮通信

Result: 在多个公开数据集和代表性非IID条件下，DC-CFL仅需一轮通信即可达到与多轮基线方法相当的准确率，验证了其在实际通信受限场景中的有效性

Conclusion: DC-CFL为通信轮次受限的协作AI模型开发提供了实用的单轮解决方案，在保持性能的同时显著降低了通信开销

Abstract: Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.

</details>


### [34] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: GeoRA是一种几何感知的低秩适配方法，专门针对强化学习可验证奖励的优化动态设计，通过SVD提取主方向并冻结残差分量，解决了现有方法在RLVR中的几何失配和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效方法（如PiSSA和MiLoRA）专为监督微调设计，未考虑RLVR特有的优化动态和几何结构，直接应用会导致谱崩溃和优化不稳定。同时，利用更新稀疏性的方法在现代硬件上因非结构化计算而效率低下。

Method: GeoRA利用RL更新子空间的各向异性和可压缩性，通过奇异值分解在几何约束子空间中提取主方向来初始化适配器，同时冻结残差分量。该方法保持预训练几何结构，并通过密集算子实现高效GPU计算。

Result: 在Qwen和Llama上的实验表明，GeoRA缓解了几何失配导致的优化瓶颈，在关键数学基准测试中持续优于现有低秩基线，达到SOTA结果。此外，在领域外任务中表现出优异的泛化能力和对灾难性遗忘的鲁棒性。

Conclusion: GeoRA通过几何感知的低秩适配有效解决了RLVR中的优化挑战，为大规模推理模型的强化学习微调提供了高效稳定的解决方案，在性能和泛化方面均优于现有方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [35] [Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay](https://arxiv.org/abs/2601.09400)
*Olgierd Unold,Stanisław Franczyk*

Main category: cs.LG

TL;DR: ACS2HER将Anticipatory Classifier System (ACS2)与Hindsight Experience Replay (HER)结合，通过后见之明学习在稀疏奖励环境中提升性能，但增加了计算开销和分类器数量。


<details>
  <summary>Details</summary>
Motivation: ACS2在构建认知地图方面非常有效，但在稀疏奖励环境中性能常常停滞不前。需要一种方法来增强ACS2在稀疏奖励环境中的学习效率。

Method: 提出ACS2HER架构，将ACS2与HER机制集成。当智能体未能达到主要目标时，触发后见之明学习，将访问过的状态重新标记为虚拟目标，从而增加学习信号的密度。

Result: 在确定性环境Maze 6和随机环境FrozenLake上评估，结果显示ACS2HER相比标准ACS2显著加速了知识获取和环境掌握。但效率提升伴随着计算开销增加和分类器数量大幅扩展。

Conclusion: 这是首次将预期机制与回顾性目标重标记在学习分类器系统中结合的分析，展示了ACS2HER在稀疏奖励环境中的有效性，但也指出了计算成本和分类器数量增加的问题。

Abstract: This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \texttt{Maze 6} and the stochastic \texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.

</details>


### [36] [On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI](https://arxiv.org/abs/2601.09455)
*André Artelt,Martin Olsen,Kevin Tierney*

Main category: cs.LG

TL;DR: 本文综述了机器学习模型反事实和半事实解释的计算复杂性研究，发现生成这些解释通常是计算困难的，并进一步贡献了不可近似性结果，讨论了这些复杂性结果对XAI社区和AI监管政策的影响。


<details>
  <summary>Details</summary>
Motivation: 为机器学习模型的决策提供清晰解释对于其在关键应用中的部署至关重要。反事实和半事实解释已成为向用户提供模型输出洞察的两种机制，但生成这些解释的计算复杂性需要系统研究。

Method: 本文采用文献综述方法，系统梳理了反事实和半事实解释生成的计算复杂性研究结果，并进一步贡献了作者自己的不可近似性理论结果，分析了在特定假设下这些解释不仅难以生成，甚至难以近似。

Result: 研究发现，在许多情况下生成反事实和半事实解释是计算困难的。作者进一步证明，在某些假设下，这些解释不仅难以生成，而且是不可近似的，这显著强化了现有文献中的复杂性论点。

Conclusion: 反事实和半事实解释的计算复杂性结果对XAI社区和寻求监管AI解释的政策制定者具有重要启示。这些发现表明，在设计和实施解释性AI系统时，必须充分考虑计算可行性限制，并为监管框架提供理论基础。

Abstract: Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.

</details>


### [37] [Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps](https://arxiv.org/abs/2601.09428)
*Siyi Li,Joseph G. Lambourne,Longfei Zhang,Pradeep Kumar Jayaraman,Karl. D. D. Willis*

Main category: cs.LG

TL;DR: 提出一种通过几何构造序列生成CAD轮廓的新方法，使用曲线偏移、旋转和交点等简单构造步骤，从设计师输入几何逐步构建最终轮廓


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模需要精确参数化，而现有生成方法缺乏可编辑性和精度。本文旨在开发一种既能保持CAD精度又能支持参数化编辑的生成方法，类似于语言模型中的思维链概念

Method: 采用几何构造序列方法，从设计师输入几何开始，通过曲线偏移、旋转和交点等简单构造步骤逐步构建CAD轮廓。引入强化学习优化构造序列，类似于思维链在语言模型中的作用

Result: 构造步骤显著提高生成质量，类似思维链在语言模型中的效果。构造序列将形状自由度减少到少量可调整参数，支持浮点精度的参数化编辑。强化学习进一步提升了多种指标的性能，包括未明确优化的指标

Conclusion: 几何构造序列方法为CAD轮廓生成提供了新范式，结合了生成模型的灵活性和参数化CAD的精度与可编辑性，强化学习的应用进一步增强了方法的性能

Abstract: We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.

</details>


### [38] [Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting](https://arxiv.org/abs/2601.09467)
*Tianye Li,Qi Liu,Hao Li,Lei Chen,Wencong Cheng,Fei Zheng,Xiangao Xia,Ya Wang,Gang Huang,Weiwei Wang,Xuan Tong,Ziqing Zu,Yi Fang,Shenming Fu,Jiang Jiang,Haochen Li,Mingxing Li,Jiangjiang Xia*

Main category: cs.LG

TL;DR: 提出Searth Transformer架构和RAR微调策略，开发YanTian全球中期天气预报模型，在精度和计算效率上超越传统方法


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的天气预报模型忽视地球球面几何和纬向周期性，且传统自回归训练计算成本高、误差累积限制预报时效

Method: 提出Searth Transformer架构，将纬向周期性和经向边界融入窗口自注意力机制；引入Relay Autoregressive微调策略，在有限计算资源下学习长程大气演化

Result: YanTian模型精度超过ECMWF高分辨率预报，与最先进AI模型竞争，计算成本比标准自回归微调低约200倍，Z500有效预报时效达10.3天（HRES为9天）

Conclusion: 该工作为复杂全球尺度地球物理环流系统的预测建模建立了稳健算法基础，为地球系统科学提供了新途径

Abstract: Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.

</details>


### [39] [SimMerge: Learning to Select Merge Operators from Similarity Signals](https://arxiv.org/abs/2601.09473)
*Oliver Bolton,Aakanksha,Arash Ahmadian,Sara Hooker,Marzieh Fadaee,Beyza Ermis*

Main category: cs.LG

TL;DR: SimMerge：基于模型相似性信号预测合并性能的合并选择方法，无需昂贵的合并-评估循环


<details>
  <summary>Details</summary>
Motivation: 模型合并是LLM开发的重要工具，但大规模合并面临挑战：需要选择合适的合并算子、模型和合并顺序，通常需要运行昂贵的合并-评估搜索。需要一种更高效的合并选择方法。

Method: 提出SimMerge方法，使用少量未标记探针计算模型间的功能和结构特征，基于这些相似性信号预测给定双向合并的性能。通过预测选择最佳合并算子、模型子集和合并顺序，避免昂贵的合并-评估循环。

Result: 在7B参数LLM的双向合并中超越标准合并算子性能；SimMerge无需重新训练即可推广到多向合并和111B参数LLM合并；提出支持动态添加新任务、模型和算子的bandit变体。

Conclusion: 学习如何合并是当检查点目录庞大且评估预算紧张时，实现可扩展模型组合的实用途径。SimMerge提供了一种基于预测的合并选择方法，显著降低了合并成本。

Abstract: Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \simmerge{}, \emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.

</details>


### [40] [Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric](https://arxiv.org/abs/2601.09624)
*Jiali Cheng,Ziheng Chen,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 该论文提出了基于模型电路的遗忘难度度量CUD，用于预测样本在机器遗忘中的难易程度，发现难易样本在电路结构上存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘在构建可信赖和合规的语言模型中变得至关重要，但现有方法在不同样本上的遗忘效果差异很大。作者认为这种差异不仅是数据侧现象，更反映了模型内部编码和保护记忆信息的机制。

Method: 提出基于电路引导的遗忘难度度量CUD，这是一种预遗忘指标，利用电路级信号为每个样本分配连续难度分数。通过分析模型电路（控制预测形成的结构化交互路径）来识别难易样本的机制特征。

Result: CUD能可靠地区分内在易遗忘和难遗忘样本，且在不同遗忘方法中保持稳定。易遗忘样本与较短、较浅的交互路径相关，集中在模型的早期到中期部分；而难遗忘样本依赖于更长、更深的路径，更接近后期计算。

Conclusion: 相比现有定性研究，CUD朝着原则性、细粒度和可解释的遗忘难度分析迈出了第一步，并激励开发基于模型机制的遗忘方法。

Abstract: Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.

</details>


### [41] [Terminally constrained flow-based generative models from an optimal control perspective](https://arxiv.org/abs/2601.09474)
*Weiguo Gao,Ming Li,Qianxiao Li*

Main category: cs.LG

TL;DR: 提出TOCFlow方法，通过最优控制框架解决预训练流模型在终端约束分布采样问题，结合几何感知的采样时间指导，在保持生成质量的同时提升约束满足度。


<details>
  <summary>Details</summary>
Motivation: 预训练的基于流的生成模型在采样时难以满足特定的终端约束条件，需要开发能够在保持模型生成质量的同时精确满足约束的采样方法。

Method: 提出TOCFlow方法：1) 建立最优控制理论框架，将约束采样问题表述为Hamilton-Jacobi-Bellman方程；2) 在终端共动坐标系中求解控制问题，得到闭式标量阻尼因子；3) 沿黎曼梯度方向施加几何感知的采样时间指导，无需矩阵求逆即可捕捉二阶曲率效应。

Result: 在Darcy流、约束轨迹规划、具有Kolmogorov谱标度的湍流快照生成三个高维科学任务上评估，TOCFlow在等式、不等式和全局统计约束下均优于欧几里得指导和投影基线，显著提升约束满足度同时保持参考模型的生成质量。

Conclusion: TOCFlow通过最优控制框架和几何感知的采样时间指导，为预训练流模型提供了有效的终端约束采样解决方案，在计算成本与标准梯度指导相当的情况下实现了与高斯-牛顿更新相当的几何一致性。

Abstract: We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.

</details>


### [42] [Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection](https://arxiv.org/abs/2601.09684)
*Ziyu Yang,Guibin Chen,Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: 提出Ortho-LoRA方法，通过梯度投影解决多任务学习中LoRA适配器的任务干扰问题，在GLUE基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习结合LoRA虽然能减少存储开销，但存在负迁移问题，不同任务的梯度更新相互冲突，导致性能下降。LoRA的低秩约束进一步限制了优化空间容纳多样化任务需求的能力。

Method: 提出Ortho-LoRA梯度投影方法，针对LoRA的双层结构特点，动态将冲突的任务梯度投影到彼此的正交补空间中，从而在LoRA固有子空间内减少任务干扰。

Result: 在GLUE基准上的大量实验表明，Ortho-LoRA能有效缓解任务干扰，优于标准联合训练，恢复了多任务与单任务基线之间95%的性能差距，且计算开销可忽略不计。

Conclusion: Ortho-LoRA为多任务学习中LoRA适配器的负迁移问题提供了有效的解决方案，在保持参数效率的同时显著提升了多任务学习性能。

Abstract: Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.

</details>


### [43] [Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions](https://arxiv.org/abs/2601.09491)
*Beatrice Ceccanti,Mattia Galanti,Ivo Roghair,Martin van Sint Annaland*

Main category: cs.LG

TL;DR: DeepONets被应用于吸附过程建模，作为循环吸附过程模拟和优化的替代模型，以加速需要重复求解瞬态PDE的温度-真空变压吸附等过程的收敛。


<details>
  <summary>Details</summary>
Motivation: 循环吸附过程（如TVSA）需要重复求解计算昂贵的瞬态偏微分方程，每个步骤从前一步的最终状态开始，需要广泛的初始条件泛化能力。控制方程具有陡峭的行进前沿，为算子学习提供了具有挑战性的基准。

Method: 构建包含异质初始条件的混合训练数据集，训练DeepONets来近似相应的解算子。在训练参数范围之外的初始条件以及完全未见过的函数形式上测试训练好的模型。

Result: DeepONets在训练分布内外都能做出准确预测，展示了其作为加速循环吸附模拟和优化工作流程的高效替代模型的潜力。

Conclusion: DeepONets在具有挑战性的吸附过程建模场景中表现出良好的泛化能力，可作为加速循环吸附过程模拟和优化的有效替代模型。

Abstract: Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, since the solution of a partial differential equation can be interpreted as an operator mapping an initial condition to its corresponding solution field. In this work, we applied DeepONets in the context of process modeling for adsorption technologies, to assess their feasibility as surrogates for cyclic adsorption process simulation and optimization. The goal is to accelerate convergence of cyclic processes such as Temperature-Vacuum Swing Adsorption (TVSA), which require repeated solution of transient PDEs, which are computationally expensive. Since each step of a cyclic adsorption process starts from the final state of the preceding step, effective surrogate modeling requires generalization across a wide range of initial conditions. The governing equations exhibit steep traveling fronts, providing a demanding benchmark for operator learning. To evaluate functional generalization under these conditions, we construct a mixed training dataset composed of heterogeneous initial conditions and train DeepONets to approximate the corresponding solution operators. The trained models are then tested on initial conditions outside the parameter ranges used during training, as well as on completely unseen functional forms. The results demonstrate accurate predictions both within and beyond the training distribution, highlighting DeepONets as potential efficient surrogates for accelerating cyclic adsorption simulations and optimization workflows.

</details>


### [44] [Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels](https://arxiv.org/abs/2601.09579)
*Fiona Murphy,Alessio Benavoli*

Main category: cs.LG

TL;DR: 该论文提出了一种基于核主成分回归的统一框架，将两种最先进的核基格兰杰因果关系方法统一起来，并引入了基于高斯过程的评分模型和同步因果识别算法，显著提升了非线性时间序列因果发现的性能。


<details>
  <summary>Details</summary>
Motivation: 现有核基格兰杰因果关系方法虽然能识别非线性因果关系，但缺乏统一的理论框架，且性能有待提升。同时，需要更有效的同步因果识别方法。

Method: 1) 在核主成分回归框架下统一两种核基GC方法；2) 提出基于高斯过程的评分模型，采用平滑信息准则惩罚边际似然；3) 提出完全基于GC的同步因果识别算法，使用GP_SIC方法。

Result: 1) 统一框架能改进因果识别；2) GP_SIC方法在性能上超越了现有最先进的非线性时间序列因果发现方法；3) 提出的同步因果识别算法与现有最先进的同步时间序列因果发现算法相比表现出色。

Conclusion: 通过核主成分回归框架统一核基GC方法，结合高斯过程评分模型和平滑信息准则惩罚，以及基于GC的同步因果识别算法，显著提升了非线性时间序列因果发现的准确性和性能。

Abstract: Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.

</details>


### [45] [Energy-Entropy Regularization: The True Power of Minimal Looped Transformers](https://arxiv.org/abs/2601.09588)
*Wai-Lun Lam*

Main category: cs.LG

TL;DR: 提出基于Tsallis熵和哈密顿动力学的新训练框架，成功训练单头循环Transformer解决归纳头任务，揭示了其优越推理能力的内部机制。


<details>
  <summary>Details</summary>
Motivation: 当前单头循环Transformer在基准任务上训练经常失败或性能不佳，原因是高度非凸且不规则的损失景观导致优化停滞在局部极小值和鞍点，无法发现全局最优点。这些模型的内部机制仍不清楚，从头训练具有重大挑战。

Method: 提出新颖的训练框架，利用Tsallis熵和哈密顿动力学来变换损失景观的几何形状。通过将参数更新视为物理流动，成功训练了模型维度d=8的单头循环Transformer，能够处理1000个令牌的输入序列长度来解决归纳头任务。

Result: 成功训练单头循环Transformer解决归纳头任务，输入序列长度达1000个令牌。这一成功揭示了循环Transformer优越推理能力背后的内部机制。

Conclusion: 通过Tsallis熵和哈密顿动力学变换损失景观几何形状的方法，能够有效克服单头循环Transformer训练中的优化困难，为理解其内部推理机制提供了重要突破。

Abstract: Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.

</details>


### [46] [Exploring Fine-Tuning for Tabular Foundation Models](https://arxiv.org/abs/2601.09654)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 对表格基础模型微调策略的首次全面研究，发现零样本性能已很强，微调效果高度依赖模型和数据，全监督微调常降低准确性或校准质量


<details>
  <summary>Details</summary>
Motivation: 表格基础模型已展现出强大的上下文学习能力，但微调策略的效果尚不明确，需要系统研究不同微调方法在不同数据集条件下的表现，为实际应用提供指导

Method: 在TALENT、OpenML-CC18和TabZilla等基准测试上，系统比较了零样本、元学习、全监督微调和参数高效微调四种方法，分析了数据集不平衡性、大小和维度等因素对结果的影响

Result: 零样本TFMs已表现出强劲性能；微调效果高度依赖模型和数据；元学习和PEFT在特定条件下提供中等增益；全监督微调常降低准确性或校准质量；数据集特性显著影响微调效果

Conclusion: 提供了关于何时微调最有益及其局限性的实用指南，强调需要根据具体模型和数据集特性谨慎选择微调策略，避免盲目使用全监督微调

Abstract: Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.

</details>


### [47] [Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design](https://arxiv.org/abs/2601.09693)
*Lisa Schneckenreiter,Sohvi Luukkonen,Lukas Friedrich,Daniel Kuhn,Günter Klambauer*

Main category: cs.LG

TL;DR: ConGLUDe是一个统一的对比几何模型，通过单模型同时支持基于结构和基于配体的计算药物设计，无需预定义结合口袋，在零样本虚拟筛选和目标追踪任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构和基于配体的计算药物设计方法使用不同的数据源和建模假设，限制了它们的大规模联合应用。需要开发能够统一这两种方法的模型，以充分利用结构信息和生物活性数据。

Method: ConGLUDe采用对比几何学习框架，包含几何蛋白质编码器（生成全蛋白表示和预测结合位点的隐式嵌入）和快速配体编码器。通过对比学习将配体与全局蛋白表示及多个候选结合位点对齐，支持配体条件化的口袋预测、虚拟筛选和目标追踪，并在蛋白-配体复合物和大规模生物活性数据上联合训练。

Result: 在多样化基准测试中，ConGLUDe在无需输入结合口袋信息的零样本虚拟筛选中达到最先进性能；在具有挑战性的目标追踪任务上显著优于现有方法；在配体条件化口袋选择方面表现具有竞争力。

Conclusion: ConGLUDe展示了统一结构和配体训练的优势，是迈向药物发现通用基础模型的重要一步，为计算药物设计提供了更灵活和强大的工具。

Abstract: Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 该研究针对LLM在教育应用中直接揭示答案而非支持对话式学习的局限性，创建了ConvoLearn数据集，通过微调使LLM转向知识建构教学策略，显著提升了教学效果。


<details>
  <summary>Details</summary>
Motivation: LLM在教育应用中存在根本性教学局限，倾向于直接揭示解决方案而非支持对话式学习。需要开发能够促进知识建构的AI导师，这需要基于教学理论的数据集来指导LLM行为。

Method: 1. 基于知识建构理论创建ConvoLearn数据集，包含六个核心教学维度；2. 通过人类教师与模拟学生受控交互构建半合成数据集（1250个师生对话，每个20轮）；3. 使用QLoRA对Mistral 7B进行微调；4. 由31名教师进行人工评估。

Result: 微调后的Mistral 7B（M=4.10, SD=1.03）在整体表现上显著优于其基础版本（M=2.59, SD=1.11）和Claude Sonnet 4.5（M=2.87, SD=1.29）。训练使LLM行为有意义地转向知识建构策略。

Conclusion: 这项工作为未来建构主义AI导师的开发和评估建立了一个潜在框架，证明了基于教学理论的数据集微调能够有效改善LLM的教学行为。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [49] [The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments](https://arxiv.org/abs/2601.09032)
*Logan Ritchie,Sushant Mehta,Nick Heiner,Mason Yu,Edwin Chen*

Main category: cs.AI

TL;DR: 该研究评估了前沿AI模型在电商RL环境中完成150个职场任务的能力，揭示了模型必须掌握的五个层次能力，发现即使最佳模型仍有约40%的任务失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体发展，AI评估需要从单轮响应评估转向交互环境中的多步骤任务完成评估。研究者旨在通过实证研究了解当前前沿模型在真实职场环境中的实际表现和能力局限。

Method: 研究在Surge提供的真实电商RL环境中评估前沿AI模型，设计了150个职场任务。采用任务中心化设计方法，强调任务多样性和领域专家贡献，并对失败案例进行详细分析。

Result: 研究发现了经验推导的智能体能力层次结构：1)工具使用、2)规划与目标形成、3)适应性、4)基础性、5)常识推理。最佳模型仍有约40%的任务失败率，失败模式沿能力层次可预测分布：较弱模型在基础工具使用和规划上挣扎，较强模型主要在需要超越显式指令的上下文推理任务上失败。

Conclusion: 当前前沿模型虽能展示连贯的多步骤行为，但在真实职场环境中实现人类水平任务完成仍存在显著能力差距。研究提出的能力层次结构和任务设计方法为智能体开发提供了重要指导。

Abstract: The advancement of large language model (LLM) based agents has shifted AI evaluation from single-turn response assessment to multi-step task completion in interactive environments. We present an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge. Our analysis reveals an empirically-derived \emph{hierarchy of agentic capabilities} that models must master for real-world deployment: (1) tool use, (2) planning and goal formation, (3) adaptability, (4) groundedness, and (5) common-sense reasoning. Even the best-performing models fail approximately 40\% of the tasks, with failures clustering predictably along this hierarchy. Weaker models struggle with fundamental tool use and planning, whereas stronger models primarily fail on tasks requiring contextual inference beyond explicit instructions. We introduce a task-centric design methodology for RL environments that emphasizes diversity and domain expert contributions, provide detailed failure analysis, and discuss implications for agent development. Our findings suggest that while current frontier models can demonstrate coherent multi-step behavior, substantial capability gaps remain before achieving human-level task completion in realistic workplace settings.

</details>


### [50] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行，解决了多约束规划中LLM方法的局限性，实现了高效、可复用的求解器函数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本性局限：纯推理范式易产生不一致性和错误累积，成本高昂；而结合编码或求解器的方法缺乏灵活性，无法捕获跨问题的通用逻辑。

Method: 提出SCOPE框架，将查询特定的推理与通用的代码执行解耦，生成一致、确定且可跨查询复用的求解器函数，仅需最小化输入参数调整。

Result: SCOPE在GPT-4o上达到93.1%的TravelPlanner成功率，比最佳基线（CoT）提升61.6%，同时降低推理成本1.4倍，减少时间约4.67倍。

Conclusion: SCOPE通过分离推理与执行，解决了多约束规划中LLM方法的局限性，实现了高效、低成本、可复用的求解方案，达到最先进性能。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [51] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: DScheLLM：基于双系统（快慢）推理架构的微调大语言模型动态调度方法，用于处理不同规模的扰动


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动（如加工时间变化、机器可用性变化、意外任务插入）高度敏感，通常依赖事件特定模型和显式分析公式，限制了其适应性和对未见扰动的泛化能力

Method: 提出DScheLLM动态调度方法，在双系统（快慢）推理架构中利用微调的大语言模型处理不同规模的扰动。构建统一的大语言模型框架处理动态事件，使用运筹学求解器获得的精确调度生成快慢推理模式的训练数据集，基于LoRA对华为OpenPangu Embedded-7B模型进行混合推理范式微调

Result: 在标准作业车间调度基准测试中，快速思维模式能高效生成高质量调度，慢速思维模式能生成与求解器兼容且格式良好的决策输入

Conclusion: 这是最早将大语言模型应用于动态环境作业车间调度的研究之一，突显了其在智能自适应调度优化方面的巨大潜力

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [52] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 提出AviationLMM愿景：一个面向民航的大型多模态基础模型，旨在统一民航异构数据流，实现理解、推理、生成和智能体应用，以解决现有AI方案孤立、单模态的局限性。


<details>
  <summary>Details</summary>
Motivation: 民航是全球交通和商业的基石，确保其安全、效率和客户满意度至关重要。现有AI解决方案在民航领域呈现孤岛化和狭窄化，专注于孤立任务或单一模态，难以整合语音通信、雷达轨迹、传感器流和文本报告等异构数据，限制了态势感知、适应性和实时决策支持能力。

Method: 首先识别现有AI解决方案与需求之间的差距；其次描述模型架构，该架构能够接收空地语音、监视数据、机载遥测、视频和结构化文本等多模态输入，执行跨模态对齐和融合，并产生从态势摘要、风险预警到预测性诊断和多模态事件重建的灵活输出。

Result: 提出了AviationLMM的完整愿景和设计框架，识别了实现该愿景需要解决的关键研究机遇，包括数据获取、对齐与融合、预训练、推理、可信性、隐私、缺失模态鲁棒性以及合成场景生成等挑战。

Conclusion: 通过阐述AviationLMM的设计和挑战，旨在推动民航基础模型的进展，并催化协调研究努力，朝着集成、可信和隐私保护的民航AI生态系统发展。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [53] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 该立场论文批判现有LLM自动生成审稿的方法，提出将LLM定位为辅助和教育人类审稿人的工具，并设计了两个互补系统来培养审稿人能力和提升审稿质量。


<details>
  <summary>Details</summary>
Motivation: AI研究的快速扩张加剧了"审稿人缺口"，威胁同行评审的可持续性并导致低质量评审的恶性循环。现有LLM自动生成审稿的方法存在缺陷，需要范式转变来构建更可持续的学术生态系统。

Method: 提出以人为中心的LLM辅助方法：1) 定义高质量同行评审的核心原则；2) 基于这些原则设计两个互补系统：LLM辅助的导师系统（培养审稿人长期能力）和LLM辅助的反馈系统（帮助审稿人完善评审质量）。

Result: 论文提出了一个理论框架和系统设计方案，将LLM从自动生成工具转变为人类审稿人的辅助和教育工具，旨在通过培养审稿人专业能力来提升评审质量。

Conclusion: 采用以人为中心的LLM辅助方法能够加强审稿人专业知识，有助于构建更可持续的学术生态系统，这是解决审稿人缺口和提升同行评审质量的有效途径。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [54] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM智能体的元自适应推理框架，通过前瞻策略和轨迹收敛机制解决智能体推理中的局部短视和轨迹不稳定问题，平衡全局有效性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法存在两个主要问题：(1) 局部短视生成，由于缺乏前瞻性；(2) 轨迹不稳定性，早期微小错误会导致推理路径发散。这些问题使得难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略扩展推理路径几步，估计工具使用的优势值，结合步骤一致性方差和步骤间趋势斜率联合选择稳定、一致、高价值的推理步骤。引入轨迹收敛机制，在路径一致性达成时停止进一步展开，控制计算成本。

Result: 在三个基础模型（MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B）和五个数据集上的广泛实证研究表明，MAXS在性能和推理效率方面均优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM智能体推理中的局部短视和轨迹不稳定问题，实现了全局有效性和计算效率的良好平衡，为多工具推理提供了有效的解决方案。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [55] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow：将离散推理步骤重构为连续概率流，量化每个步骤对最终答案的贡献，实现流引导解码和流强化学习，提升推理效率和性能


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制，导致推理效率低下（冗余探索）和优化困难（稀疏监督或昂贵验证器）

Method: 提出CoT-Flow框架，将离散推理步骤重构为连续概率流，量化每个步骤对真实答案的贡献。基于此实现两种方法：1）流引导解码，采用贪婪的基于流的解码策略提取信息高效的推理路径；2）流强化学习，构建无需验证器的密集奖励函数

Result: 在具有挑战性的基准测试中，CoT-Flow在推理效率和推理性能之间实现了优越的平衡

Conclusion: CoT-Flow通过量化推理步骤的信息增益，解决了当前思维链方法的局限性，为高效、可优化的推理提供了新框架

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [56] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体决策框架，通过模拟跨区域协调政策制定，实现更有效的疫情控制


<details>
  <summary>Details</summary>
Motivation: 传统疫情控制政策存在碎片化和反应式的问题，各地区政策制定孤立且调整滞后，缺乏协调性和前瞻性，影响全球疫情防控效果

Method: 为每个行政区分配一个LLM智能体作为AI决策助手，智能体基于区域特定流行病学动态进行推理，同时与其他智能体通信以考虑跨区域相互依赖关系，整合真实世界数据、疫情演化模拟器和结构化智能体间通信

Result: 使用美国2020年4月至12月州级COVID-19数据验证，相比真实世界结果，在单个州层面分别减少63.7%的累计感染和40.1%的死亡，跨州汇总分别减少39.0%和27.0%

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情控制，为公共卫生决策提供新的技术路径

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [57] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER是一个基于路由器的自适应激活干预框架，通过强化学习动态组合可重用推理向量，在零样本设置下提升LLM推理性能，同时保持高令牌效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活干预的方法采用静态、手动干预，无法适应复杂推理的动态特性，而训练密集型方法需要参数更新，不够高效。需要一种参数高效且能自适应动态推理的干预方法。

Method: RISER框架包含：1)构建可重用推理向量库；2)轻量级路由器动态组合这些向量；3)通过强化学习在任务级奖励下优化路由器，以涌现和组合方式激活潜在认知原语。

Result: 在七个多样化基准测试中，RISER相比基础模型平均零样本准确率提升3.4-6.5%，超越思维链推理，令牌效率提高2-3倍，且保持稳健的准确率增益。分析显示RISER能自主组合多个向量形成可解释的精确控制策略。

Conclusion: RISER通过自适应激活干预实现了更可控和高效的LLM推理，为参数高效的动态推理干预提供了有前景的方向。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [58] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 该论文提出了A³-Bench基准，用于评估科学推理中的记忆驱动机制，重点关注锚点和吸引子的激活与利用。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步推理的连贯性，但忽视了人类推理中基于记忆驱动的机制，特别是锚点和吸引子的激活与整合过程。

Method: 1. 使用SAPM过程（主题、锚点与吸引子、问题、记忆发展）对2,198个跨领域科学推理问题进行标注；2. 提出基于锚点和吸引子的双尺度记忆评估框架；3. 引入AAUI指标衡量记忆激活率；4. 通过多种基础模型和范式进行实验验证。

Result: 通过实验验证了A³-Bench基准的有效性，分析了记忆激活对推理性能的影响，为理解记忆驱动的科学推理提供了见解。

Conclusion: A³-Bench填补了现有基准在评估记忆驱动推理机制方面的空白，为科学推理研究提供了新的评估工具和视角。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [59] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR是一个无需参数的推理时遗忘框架，专门针对大型推理模型，通过语义检测、安全提示前缀、轨迹感知抑制和自适应过滤，在整个推理链中实现隐私保护，并引入新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成复杂思维链轨迹时存在严重隐私风险，敏感信息可能深嵌于推理过程中。现有的LLM遗忘方法通常只修改最终答案，无法从中间步骤移除敏感内容，导致持续隐私泄露和安全退化。

Method: 提出Sensitive Trajectory Regulation (STaR)框架：1) 语义感知检测识别敏感内容；2) 通过安全提示前缀注入全局安全约束；3) 轨迹感知抑制动态阻断整个推理链中的敏感内容；4) 令牌级自适应过滤防止生成精确和转述的敏感令牌。

Result: 在R-TOFU基准测试中，STaR实现了全面稳定的遗忘，同时保持最小效用损失，为LRMs中的隐私保护推理设立了新标准。

Conclusion: STaR是一个有效的参数无关推理时遗忘框架，能够保护大型推理模型在整个推理过程中的隐私，并引入了更全面的评估指标来量化隐私保护效果。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [60] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种基于自然语言处理的语义化、意图驱动的集群调度范式，通过LLM解析自然语言分配提示注释来实现软亲和性偏好，在Kubernetes调度器扩展器中集成原型系统，验证了语义化软亲和性简化工作负载编排的可行性。


<details>
  <summary>Details</summary>
Motivation: 集群工作负载分配通常需要复杂配置，存在可用性差距。传统调度系统配置复杂，用户需要掌握专业技术知识，这限制了集群系统的易用性和可访问性。

Method: 采用语义化、意图驱动的调度范式，利用自然语言处理技术。系统集成大型语言模型（LLM）到Kubernetes调度器扩展器中，解析自然语言分配提示注释以实现软亲和性偏好。开发了包含集群状态缓存和意图分析器（使用AWS Bedrock）的原型系统。

Result: LLM解析准确率超过95%（在评估基准数据集上的子集准确率），Amazon Nova Pro/Premier和Mistral Pixtral Large等顶级模型显著优于基线引擎。在六个场景的调度质量测试中，原型系统相比标准Kubernetes配置实现了更优或相当的放置效果，尤其在复杂和定量场景以及处理冲突软偏好方面表现出色。

Conclusion: 验证了使用LLM实现可访问调度的可行性，确认了语义化软亲和性简化工作负载编排的潜力。但指出了同步LLM延迟等限制，建议采用异步处理以实现生产就绪。该工作为简化集群工作负载编排提供了新途径。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [61] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 提出了一种面向任务的主动式智能体交互范式，通过意图条件监控和事件触发跟进能力，在动态环境中实现长期用户意图维护，并构建了ChronosBench基准和高质量数据合成流程。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体主要采用被动响应范式，只能在短期会话中响应用户即时查询，无法维护长期用户意图和适应动态变化的外部环境，这限制了其在复杂任务导向交互中的应用。

Method: 1) 形式化主动性的两个关键能力：意图条件监控（基于对话历史自主制定触发条件）和事件触发跟进（检测到有用环境更新时主动与用户互动）；2) 引入高质量数据合成管道构建动态环境中的复杂多轮对话数据；3) 提出ChronosBench基准评估动态环境中任务导向交互；4) 使用合成数据进行监督学习微调模型。

Result: 1) 评估了当前领先的闭源和开源模型，揭示了它们在长期任务导向交互中的缺陷；2) 使用合成数据微调的模型在包含用户意图转变的复杂任务中达到85.19%的任务完成率，优于其他测试模型；3) 验证了数据驱动策略的有效性。

Conclusion: 提出的主动式任务导向智能体交互范式能够有效弥合相对静态的用户需求与动态环境之间的差距，通过意图条件监控和事件触发跟进实现长期意图维护，数据合成和微调方法显著提升了复杂动态任务中的性能。

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [62] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 论文提出T2Q评估范式，通过将任务执行与环境理解解耦来评估LLM智能体的泛化能力，发现任务成功不能准确反映环境理解水平。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂决策和工具使用任务中表现出色，但其在不同环境中的泛化能力尚未得到充分研究。现有评估主要依赖基于轨迹的任务成功率指标，无法评估智能体是否真正掌握了可迁移的环境模型。

Method: 提出Task-to-Quiz (T2Q)评估范式，将任务执行与环境理解解耦。具体实现为T2QBench基准套件，包含30个环境和1,967个基于真实状态的问答对，涵盖多个难度级别。

Result: 实验表明：1) 任务成功率不能准确反映环境理解能力；2) 现有记忆机制无法有效帮助智能体获取真实的环境模型；3) 主动探索和细粒度状态表示是主要瓶颈。

Conclusion: T2Q为评估LLM智能体的环境理解能力提供了新范式，揭示了当前方法的局限性，为开发更具泛化能力的自主智能体指明了方向。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [63] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: Omni-R1提出统一生成式多模态推理框架，通过生成中间图像统一多种多模态推理技能，包含有监督和无监督两种实现方式。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在推理过程中要么仅使用文本，要么采用单一任务特定的推理模式，缺乏跨多种多模态任务的通用性。不同多模态任务需要多样化的推理技能（如放大特定区域、标记图像对象等），需要统一的推理框架。

Method: 提出统一生成式多模态推理范式，通过在推理过程中生成中间图像来统一多种推理技能。具体实现包括：1) Omni-R1：两阶段SFT+RL框架，包含感知对齐损失和感知奖励，实现功能性图像生成；2) Omni-R1-Zero：通过从纯文本推理数据中引导逐步可视化，无需多模态标注。

Result: 实验结果表明，Omni-R1能够在广泛的多模态任务上实现统一生成式推理，而Omni-R1-Zero在平均性能上能够匹配甚至超越Omni-R1，显示了生成式多模态推理的潜力。

Conclusion: 统一生成式多模态推理通过生成中间图像有效统一了多种推理技能，为多模态大语言模型提供了更通用的推理框架，无监督方法展示了从纯文本数据学习多模态推理的可能性。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [64] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL框架通过测试时强化学习将结构化文本经验注入多智能体推理过程，无需训练即可提升多智能体系统的决策性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在实际应用中展现出优势，但多智能体强化学习训练资源消耗大且不稳定，存在非平稳性和稀疏高方差奖励问题，需要更高效的解决方案

Method: 提出MATTRL框架：1) 组建多专家团队进行多轮讨论；2) 检索并整合测试时经验；3) 通过共识机制进行最终决策；4) 研究信用分配机制构建回合级经验池并重新注入对话

Result: 在医学、数学和教育等挑战性基准测试中，MATTRL相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%；消融研究分析了不同信用分配方案对训练结果的影响

Conclusion: MATTRL提供了一种稳定、有效且高效的路径，无需调优即可实现分布偏移鲁棒的多智能体推理，解决了传统MARL训练的资源密集和不稳定问题

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [65] [Assessment of the Rankine Vortex Model for Modeling the Gas Entrainment in a Representative Mock-up of a Sodium Fast Reactor](https://arxiv.org/abs/2601.08861)
*David Guenadou,Philippe Aubert,Jean-Philippe Descamps*

Main category: physics.flu-dyn

TL;DR: 本文通过比较Rankine涡模型与实验数据，改进钠冷快堆上腔室气体夹带预测准则


<details>
  <summary>Details</summary>
Motivation: 钠冷快堆中气体夹带会引发中子效应导致反应性变化，现有基于Rankine涡模型的预测准则在某些情况下与实验结果不符，需要改进

Method: 使用PIV方法测量速度场确定Rankine模型参数，通过原始PIV图像计算涡旋直径，将模型计算结果与实验数据进行对比分析

Result: 通过实验数据验证Rankine涡模型的准确性，识别模型与实验结果之间的差异，为改进气体夹带预测准则提供依据

Conclusion: 通过对比Rankine涡模型与实验数据，可以改进气体夹带预测准则，提高对钠冷快堆上腔室涡旋现象的数值模拟精度

Abstract: Since decades, the CEA has been involved in the development of 4th generation reactors cooled by sodium. The dedicated experimental platform PLATEAU was erected to study the different issues and built a results database for the code validation. As experiments with sodium are complex, the tests are led in mock-ups using water as a simulant fluid. The MICAS mock-up, model of the upper plenum, aims at studying the gas entrainment at the free surface. Gas is prohibited in the reactor core due to neutron effects, which may induce reactivity variations. Since vortices are difficult to handle by numerical codes in complex geometries such as reactor ones, criteria have been developed based on the Rankine vortex model to forecast the air entrainment. However, in some case, numerical results do not correlate the experimental ones and those discrepancies are may be due to the vortex model. This article aims at comparing the vortices characteristics calculated using the Rankine vortex model with experimental results in order to improve the gas entrainment criteria. The parameters of the Rankine model are determined using the velocity fields measured by the Particle Image Velocity (PIV) method and the vortices diameter is calculated using the raw PIV images.

</details>


### [66] [Lagrangian Phase-Lag and Geometric Precedence in Turbulent Vortex Stretching](https://arxiv.org/abs/2601.08862)
*Khalid Saqr*

Main category: physics.flu-dyn

TL;DR: 研究通过拉格朗日跟踪发现，在高雷诺数湍流中，涡拉伸存在因果时间顺序：压力场拓扑结构作为几何前兆，先于峰值涡量放大发生，形成相空间滞后现象。


<details>
  <summary>Details</summary>
Motivation: 经典湍流理论通常假设应变与涡量瞬时对齐，但实际涡拉伸过程的时间因果顺序尚不明确。本研究旨在揭示高雷诺数湍流中涡拉伸的精确时间演化机制，特别是压力场在组织流动结构中的前驱作用。

Method: 采用1024^3直接数值模拟，对高雷诺数湍流（Re_λ≈433）进行拉格朗日跟踪。通过条件平均流体微团的动力学，分析相空间滞后现象。研究压力场鞍点拓扑（λ_min^p < 0）与峰值涡量放大之间的时间顺序，并在磁流体动力学湍流中验证机制的鲁棒性。

Result: 发现系统性的拉格朗日相位滞后控制着强耗散的发生。流体微团轨迹在经历峰值涡量放大之前，先被压力场鞍点拓扑捕获。时间顺序（τ>0）表明压力拓扑作为确定性几何前兆，在爆发事件发生前组织流动结构。在MHD湍流中，洛伦兹力抑制了滞后环，迫使从因果优先转变为同步锁定。

Conclusion: 涡拉伸不是瞬时过程，而是受压力场拓扑结构因果调控的演化序列。压力鞍点作为几何前兆，先于涡量爆发组织流动，这一机制在普通湍流中表现为相空间滞后，在MHD湍流中因洛伦兹力作用而转变为同步锁定。

Abstract: This study investigates the causal timeline of vortex stretching in high-Reynolds-number turbulence ($Re_λ\approx 433$) using Lagrangian tracking in $1024^3$ direct numerical simulations. While classical theories often assume an instantaneous alignment between strain and vorticity, the present analysis identifies a systematic Lagrangian phase lag governing the onset of intense dissipation. By conditionally averaging the dynamics of fluid parcels, a distinct phase-space hysteresis is revealed. Trajectories are captured by the saddle-point topology of the pressure field ($λ_{min}^p < 0$) prior to experiencing peak enstrophy amplification. This temporal ordering ($τ> 0$) demonstrates that the pressure topology acts as a deterministic geometric precursor, organizing the flow structure before the bursting event occurs. The robustness of this mechanism is verified in magnetohydrodynamic (MHD) turbulence, where the Lorentz force is found to suppress the hysteresis loop, forcing a transition from causal precedence to simultaneous locking.

</details>


### [67] [Dynamic Behavior of Tandem Perforated Elastic Vortex Generators Using Two-Way Coupled Fluid-Structure Interaction Simulations](https://arxiv.org/abs/2601.08966)
*Karan Kakroo,Hamid Sadat*

Main category: physics.flu-dyn

TL;DR: 该研究通过双向耦合流固相互作用模拟，研究了串联穿孔弹性涡流发生器在不同弯曲刚度、质量比和孔隙率下的动态行为，揭示了孔隙率如何改变动态响应模式并抑制腔体振荡。


<details>
  <summary>Details</summary>
Motivation: 研究穿孔弹性涡流发生器在串联配置中的动态行为，探索孔隙率如何影响其流固相互作用、动态响应模式和尾流动力学，为被动调制涡流发生器系统提供理论基础。

Method: 采用高保真双向耦合流固相互作用模拟，在固定雷诺数和间距条件下，对串联穿孔弹性涡流发生器进行参数化研究，涵盖宽范围的弯曲刚度、质量比和孔隙率，并与非穿孔配置进行对比分析。

Result: 观察到三种响应模式（驻留、涡激振动和静态重构），非穿孔配置出现独特的腔体振荡模式，但穿孔完全抑制了该模式。频率分析显示涡激振动锁定在第二固有频率，腔体振荡锁定在第一固有频率并与第一Rossiter模式一致。穿孔改变了固有频率，使锁定和模式转换向更低弯曲刚度和更高质量比偏移，并降低振荡幅度。阻力分析表明上游阻力始终较高，孔隙率降低上游阻力但增加下游阻力。

Conclusion: 孔隙率从根本上改变了串联弹性涡流发生器的动态机制，抑制了腔体驱动的不稳定性，并实现了尾流动力学的被动调制，穿孔配置产生更小、更耗散的涡结构，为涡流发生器系统的优化设计提供了新途径。

Abstract: This study presents high-fidelity, two-way coupled fluid-structure interaction simulations to investigate the dynamic behavior of tandem perforated elastic vortex generators across a wide range of bending rigidity, mass ratio, and porosity, at a fixed Reynolds number and interspacing. Comparative simulations with non-perforated EVGs are also performed. Three response modes, lodging, vortex-induced vibration, and static reconfiguration, are observed in both configurations, while a distinct cavity oscillation mode emerges exclusively in non-perforated tandem EVGs. This mode is entirely suppressed with porosity due to disruption of the low-pressure cavity and increased flow transmission through pores. Frequency analyses reveal that vortex-induced vibration is consistently locked onto the second natural frequency, whereas the cavity oscillation mode is locked onto the first natural frequency and closely aligns with the first Rossiter mode, underscoring its distinct physical origin. Perforation modifies the natural frequency of the EVGs, shifting the lock-in and mode transitions toward lower bending rigidity and higher mass ratio values, and reducing oscillation amplitudes due to motion damping. Drag analysis shows consistently higher upstream drag due to wake shielding, while porosity reduces upstream drag and increases downstream drag by restoring streamwise momentum. Flow visualizations demonstrate that vortex shedding originates at the EVG tips, with perforated configurations producing smaller, more dissipative vortical structures. These results establish that porosity fundamentally alters dynamic regimes, suppresses cavity-driven instabilities, and enables passive modulation of wake dynamics in tandem EVG systems.

</details>


### [68] [Emergence of Purely Elasto-Plastic Turbulence in Shear Flows](https://arxiv.org/abs/2601.08992)
*Muhammad Abdullah,Shravan Pradeep,Doug J. Jerolmack,Becca Thomases,Paulo E. Arratia*

Main category: physics.flu-dyn

TL;DR: 在无惯性条件下，屈服应力流体中出现弹性驱动的流动状态，表现出弹性-塑性湍流特征，塑性增强而非阻碍动量输运


<details>
  <summary>Details</summary>
Motivation: 研究屈服应力流体在简单剪切流中弹性与塑性的复杂相互作用，探索塑性对流动行为的影响机制

Method: 通过数值模拟研究无惯性条件下屈服应力流体的流动状态，分析速度场和应力场的波动谱特征

Result: 发现弹性驱动的弹性-塑性湍流状态，未屈服流动体积分数与塑性呈非单调关系，塑性超过临界值后系统可流体化

Conclusion: 塑性可以增强而非阻碍动量输运，揭示了简单剪切流中弹性与塑性相互作用的复杂性

Abstract: We observe the emergence of a distinct, elasticity-driven flow state in a yield-stress fluid in the absence of inertia. Numerical simulations show that this elasto-plastic turbulent state is characterized by a broad spectrum of fluctuations in velocity and stress. Results show a non-monotonic relationship between the volume fraction of the unyielded flow and plasticity. Surprisingly, we find that above a critical value of plasticity, the system can fluidize. Our results reveal the complex interplay between elasticity and plasticity in simple shear flows, indicating that plasticity can enhance rather than hinder momentum transport.

</details>


### [69] [Generalized Formulation to Predict Rossiter Modes for Subsonic to Hypersonic Flow](https://arxiv.org/abs/2601.09099)
*Jeremy P. Redding,Luis Bravo,Prashant Khare*

Main category: physics.flu-dyn

TL;DR: 开发了广义物理模型，无需先验流动物理知识即可准确预测亚音速到高超音速范围内矩形空腔流动的Rossiter模态，改进模型与DNS数据吻合良好（误差<10%）。


<details>
  <summary>Details</summary>
Motivation: 现有Heller-Bliss模型在高马赫数下与直接数值模拟结果存在显著偏差，需要建立更精确的广义物理模型来预测宽马赫数范围内的空腔流动Rossiter模态。

Method: 采用基于物理的建模方法，引入有效温度概念来修正声速计算，并基于能量模态进行物理推理，建立了适用于亚音速到高超音速范围的广义模型。

Result: 改进模型在高马赫数下与DNS数据吻合良好（误差在10%以内），而Heller-Bliss模型则出现显著偏差；同时建立了Strouhal数的渐近极限。

Conclusion: 提出的广义物理模型能够准确预测宽马赫数范围内的空腔流动Rossiter模态，为工程应用提供了更可靠的预测工具，并建立了相关的渐近理论框架。

Abstract: This paper describes the development of a generalized physics-based model to accurately estimate Rossiter modes for flow over rectangular cavities for regimes ranging from subsonic to hypersonic without the a priori knowlege of flow physics. The Heller-Bliss model is shown to diverge from direct numerical simulation (DNS) results, while the adapted model shows close alignment (within 10\%) with the DNS data at higher Mach numbers, and is physically reasoned on the basis of energy modes. Using an effective temperature to evaluate the speed of sound calculations and then using it to calculate the Strouhal number yields closer predictions to DNS data. The present work also establishes asymptotic limits for Strouhal numbers.

</details>


### [70] [Progressive Mixture-of-Experts with autoencoder routing for continual RANS turbulence modelling](https://arxiv.org/abs/2601.09305)
*Haoyu Ji,Yinhang Luo,Hanyu Zhou,Yaomin Zhao*

Main category: physics.flu-dyn

TL;DR: 提出渐进式专家混合框架，通过模块化路由器和专家系统实现RANS湍流模型的持续学习，避免灾难性遗忘，提高跨流态预测精度


<details>
  <summary>Details</summary>
Motivation: 开发能够适应多种流态的RANS湍流模型是一个长期挑战，现有模型难以在保持先前学习能力的同时适应新流态

Method: 提出渐进式专家混合框架，使用基于自动编码器的模块化路由器将不同流态与专门化湍流模型（专家）关联，当遇到新流态时以低成本添加新专家而不影响已有专家

Result: PMoE模型有效整合多个专家，在翼型尾迹、壁面附着流、分离流和角部诱导二次流等不同物理特性的测试案例中，对已见和未见流态均获得改进的预测精度

Conclusion: 该框架为工业计算流体力学提供了可扩展的终身学习湍流模型路径，稀疏激活机制确保推理时模型扩展不增加计算成本

Abstract: Developing Reynolds-averaged Navier-Stokes (RANS) turbulence models that remain accurate across diverse flow regimes remains a long-standing challenge. In this work, we propose a novel framework, termed the progressive mixture-of-experts (PMoE), designed to enable continual learning for RANS turbulence modelling. The framework employs a modular autoencoder-based router to associate each flow scenario with a specialised turbulence model, referred to as an expert. When an unseen flow regime cannot be adequately represented by the existing router and expert set, a new expert together with its routing component can be introduced at low cost, without modifying or degrading previously trained ones, thereby naturally avoiding catastrophic forgetting. The framework is applied to a range of flows with distinct physical characteristics, including baseline airfoil wakes, wall-attached flows, separated flows and corner-induced secondary flows. The resulting PMoE model effectively integrates multiple experts and achieves improved predictive accuracy across both seen and unseen test cases. Owing to sparse activation, model expansion does not incur additional computational cost during inference. The proposed framework therefore provides a scalable pathway towards lifelong-learning turbulence models for industrial computational fluid dynamics.

</details>


### [71] [Slip viscosity and strain-rate viscosity in Taylor-Couette laminar flows: Experimental falsification and end-wall effects](https://arxiv.org/abs/2601.09319)
*Jian He,Jin Wang,Qiaocong Kong,Penglong Zhao,Xiaoshu Cai,Xiaohang Zhang,Wennan Zou*

Main category: physics.flu-dyn

TL;DR: 通过泰勒-库埃特层流实验验证两种粘度模型的物理真实性，并数值研究滑移模型中相对间隙和转速对周向速度的影响


<details>
  <summary>Details</summary>
Motivation: 验证应变率粘度模型和滑移粘度模型的物理真实性，澄清两者在剪切力共轭关系上的差异，研究滑移模型中端壁效应导致的复杂流动现象

Method: 采用内外圆柱同角速度旋转的泰勒-库埃特层流实验，使用LDV测量周向速度；数值模拟研究相对间隙和转速对周向速度分布的影响，对比两种粘度模型

Result: 实验结果显示相对间隙0.3时最大偏离刚体旋转约0.86%，与滑移粘度模型理论预测一致；数值模拟表明应变率粘度模型无端壁效应，而滑移模型导致周向速度沿轴向变化，形成受相对间隙和角速度影响的三维螺旋流线

Conclusion: 实验验证了滑移粘度模型的物理真实性，揭示了滑移模型中端壁效应引起的三维流动特征，为相关粘度模型的应用提供了实验和数值依据

Abstract: The viscous force should be shear force, the difference between the strain-rate viscosity and the slip viscosity is that the former has conjugate shear force, while the latter does not. The study in this paper verifies the physical authenticity of two viscosity models through Taylor Couette laminar flow experiments with inner and outer cylinders rotating at the same angular velocity, and numerically investigate the influence of relative cylinder spacing and rotational speed on the circumferential velocity under the slip model. The experimental results of LDV measurement with a relative cylinder spacing of 0.3 indicate that the maximum deviation from rigid-body rotation is about 0.86%, which is consistent with the theoretical prediction of slip viscosity model. The numerical simulations show that the end-walls have no effect under the strain-rate viscosity model; but when the slip viscosity model is introduced, the end-walls inevitably bring about the circumferential velocity profile changing along the axial direction, and result in a three-dimensional (3D) spiral streamline pattern influenced by the relative cylinder spacing and angular speed of cylinders.

</details>


### [72] [Finite-system size effects in gravity-capillary wave turbulence](https://arxiv.org/abs/2601.09355)
*Tanu Singla,Jean-Baptiste Gorce,Eric Falcon*

Main category: physics.flu-dyn

TL;DR: 实验研究有限系统尺寸对弱非线性随机重力-毛细表面波动力学的影响，通过改变容器纵横比和波陡度，观察到从离散波湍流向连续波湍流的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 研究有限系统尺寸对波动力学的影响，特别是在弱非线性随机重力-毛细表面波系统中，避免传统水平振荡容器中全局强迫的主导影响。

Method: 在具有不同纵横比的矩形水箱中进行实验，使用部分浸没的小磁铁在振荡垂直电磁场驱动下局部扰动流体表面，产生统计均匀且各向同性的随机波场，进行时空测量和高阶相关分析。

Result: 波能谱沿非受限方向显示多个分支，对应受限方向的晃动模式；观察到受限方向的离散波湍流和非受限方向的介观连续波湍流；随着限制逐渐放松，实现从离散到连续波湍流的平滑过渡；有限尺寸效应通过消耗受限方向的二维三波共振相互作用改变波动力学。

Conclusion: 有限系统尺寸显著影响弱非线性随机重力-毛细表面波的动力学，通过调整波陡度或限制可以调谐模式的光谱特性，并观察到从离散到连续波湍流的过渡，为理解有限尺寸系统中的波湍流提供了实验证据。

Abstract: We experimentally investigate the effects of finite-system size on the dynamics of weakly nonlinear random gravity-capillary surface waves. Experiments are conducted in rectangular tanks with varying aspect ratios, in which the fluid surface is perturbed locally and erratically by small, partially submerged magnets. Driven by an oscillating vertical electromagnetic field, these magnets generate a statistically homogeneous and isotropic random wave field. This setup enables us to probe finite-size effects without the dominant influence of global forcing present in horizontally oscillated tanks. Spatiotemporal measurements of the wave field reveal multiple branches in the wave-energy spectrum along the unconfined direction, corresponding to sloshing modes in the confined direction. We show that the spectral properties of these modes can be tuned by varying either the wave steepness or the confinement. Signatures of discrete wave turbulence in the confined direction and mesoscopic continuous wave turbulence in the unconfined direction are observed. As the confinement is gradually relaxed, we further demonstrate a smooth transition from discrete to continuous wave turbulence, consistent with the nonlinear-to-discreteness timescale ratio. Using high-order correlation analysis, we also show that finite-size effects alter wave dynamics by depleting two-dimensional three-wave resonant interactions along the confined direction.

</details>


### [73] [Semi-convection in rotating spherical shells: flows, layers and dynamos](https://arxiv.org/abs/2601.09358)
*Paul Pružina,Nathanaël Schaeffer,David Cébron*

Main category: physics.flu-dyn

TL;DR: 该研究通过旋转球壳模拟半对流现象，揭示了分层结构与行星磁场生成机制，特别是与土星磁场的相似性。


<details>
  <summary>Details</summary>
Motivation: 研究气态行星中半对流区域的动力学行为及其对行星磁场生成的潜在作用，弥补先前局部笛卡尔模型研究的不足。

Method: 采用旋转球壳模型进行线性和非线性模拟，包括磁流体动力学模拟，研究半对流在旋转条件下的行为及其对磁场生成的影响。

Result: 系统演化形成同心密度层结构，最终达到统计稳态；磁流体模拟显示自持发电机作用，在特定条件下产生强偶极轴对称磁场，与土星观测磁场高度一致。

Conclusion: 旋转球壳中的半对流能够产生与行星磁场相似的结构，特别是土星的偶极轴对称磁场；罗斯贝数和稳定分层层厚度可由单一参数组合预测，为行星发电机参数识别提供依据。

Abstract: Large regions of gaseous planets are thought to be stratified with an unstable thermal gradient, but a stabilising gradient of heavy element composition. Fluid in these regions is unstable to semi-convection, with motions driven by differences in the molecular diffusivity of temperature and composition, and could play a role in supporting planetary magnetic fields. Previous studies focus largely on local models in Cartesian boxes; here, we investigate semi-convection in rotating spherical shells. The onset of linear instability shows a transition between the two limits of rotating convection and non-rotating semi-convection. Non-linear simulations evolve into a system of concentric layers of relatively constant density, separated by narrow high-gradient regions. These layers gradually merge, resulting in a statistically steady state dominated by either a single convection region or a narrower convective zone beneath a stably stratified layer (SSL), depending on the strength of the thermal forcing compared to the rotation. When magnetic field generation is considered, our magnetohydrodynamic simulations exhibit self-sustained dynamo action. In cases where the turbulent convective region generates magnetic fields that are smoothed by zonal flows within the overlying SSL, the resulting field is strongly dipolar and axisymmetric, in encouraging agreement with Saturn's observed magnetic field. Within the regimes explored, both the Rossby number and the thickness of the SSL are well predicted by a single combination of control parameters. This enables the identification of a parameter range in which the generated magnetic fields resemble those of planetary dynamos.

</details>


### [74] [Analysis and simulations of droplet generation regimes in a coaxial microfluidic device](https://arxiv.org/abs/2601.09483)
*Alessio Innocenti,Andrea Poggi,Simone Camarri,Maria Vittoria Salvetti*

Main category: physics.flu-dyn

TL;DR: 使用开源软件Basilisk评估微流控设备中微滴生成的数值模拟准确性，验证工具后分析同轴装置的滴落和喷射机制，提出考虑粘度比等物理效应的新标度律


<details>
  <summary>Details</summary>
Motivation: 微流控设备中的微滴生成在生化和制药等领域有广泛应用，相比传统批量生成方法能更好地控制液滴尺寸、均匀性和生成速率。需要验证开源数值工具Basilisk在预测微滴生成方面的准确性，并改进现有标度律以更好地预测液滴直径

Method: 1. 使用Bretherton理论验证Basilisk软件对受限流中液滴动力学的预测准确性；2. 与文献实验数据进行对比验证；3. 对同轴微流控装置进行多组数值模拟，分析不同液滴生成机制（滴落和喷射）；4. 分析物理参数和流动参数对生成机制和液滴直径的影响；5. 提出考虑粘度比等物理效应的新标度律

Result: 1. Basilisk软件能准确预测微滴生成；2. 验证了数值工具与Bretherton理论和实验数据的一致性；3. 分析了不同参数对液滴生成机制的影响；4. 提出了改进的标度律，能更准确地预测滴落和喷射机制下的液滴直径，特别是考虑了粘度比等先前被忽略的物理效应

Conclusion: Basilisk是预测微流控设备中微滴生成的有效数值工具。通过数值模拟分析，提出了考虑粘度比等物理效应的新标度律，改进了现有预测模型，为微滴生成的精确控制和优化提供了理论基础

Abstract: The generation of microdroplets via segmentation in microfluidic devices is of interest in many applications, from biochemical to pharmaceutical. This technique permits indeed much higher control on the droplet size, uniformity and generation rate than in standard batch generation processes. In this work we have evaluated the suitability of the open-source software Basilisk to accurately predict microdroplet generation by segmentation. We have validated the numerical tool with analytical solutions for the dynamics of droplets in confined flows, namely with Bretherton theory, and by comparison with literature experimental results. We have then performed several campaigns of numerical simulations for a coaxial device, analyzing the different regimes of droplet generation, and evaluating how the physical and flow parameters affect the production mechanisms and {the diameters of the generated droplets}. Finally we have proposed new scaling laws for the prediction of droplet diameters in the dripping and jetting regimes, refining existing ones by taking into account additional physical effects, like the viscosity ratio.

</details>


### [75] [Effects of correlated collisions and intermittency on the growth of lucky droplets](https://arxiv.org/abs/2601.09545)
*Tobias Bätge,Johannes Zierenberg,Michael Wilczek*

Main category: physics.flu-dyn

TL;DR: 该研究探讨湍流如何通过相关碰撞和间歇性加速云滴生长，解决"尺寸间隙问题"，发现相关碰撞在早期加速幸运液滴生长，而间歇性能显著缩短液滴跨越尺寸间隙的时间。


<details>
  <summary>Details</summary>
Motivation: 基于凝结和重力碰撞的理论估计无法解释观测到的降水起始时间尺度，存在所谓的"尺寸间隙问题"，需要研究湍流介导的碰撞生长机制来解决这一矛盾。

Method: 使用湍流中液滴的直接数值模拟，约束非马尔可夫随机框架评估连续碰撞间的相关性记忆效应；然后忽略碰撞相关性，建立云团集合模型，用线性主方程描述每个云团中的液滴尺寸分布，碰撞速率根据体积平均耗散率随时间变化。

Result: 相关碰撞加速幸运液滴的初始生长，但在后期阶段影响较小；间歇性能显著减少幸运液滴跨越尺寸间隙所需的时间。

Conclusion: 湍流通过相关碰撞和间歇性效应共同促进云滴生长，其中间歇性对解决尺寸间隙问题起关键作用，显著缩短降水起始时间。

Abstract: To trigger precipitation, water droplets in warm clouds need to attain a sufficient size. Theoretical estimates based on condensation and gravitational collisions alone fail to explain the observed timescales for the onset of precipitation for a range of droplet sizes. This suggests the involvement of collisional growth mediated by turbulence to resolve the so-called ``size-gap problem''. For the onset of rain, it is sufficient that statistical outliers, coined ``lucky droplets'', cross the size gap. In this study, we explore the influence of turbulence on droplet growth, focusing on correlated collisions and intermittency. Using direct numerical simulations of droplets in turbulent flow, we constrain a non-Markovian stochastic framework that allows us to assess memory effects on the droplet-size distribution arising from correlations between consecutive collisions. Using our framework, we find that correlated collisions accelerate the initial growth of lucky droplets but have sub-leading effect at later stages. Consequently, we neglect correlations from collisions and model an ensemble of cloud parcels representing fluctuations in the volume-averaged dissipation rate. Here, the distribution of droplet sizes in each parcel is described by a linear master equation with a time-dependent collision rate according to the volume-averaged dissipation rate. Our analyses of this toy model show that intermittency can significantly reduce the time required by lucky droplets to cross the size gap.

</details>


### [76] [Mathematical Theorems on Turbulence](https://arxiv.org/abs/2601.09619)
*Theodore D. Drivas*

Main category: physics.flu-dyn

TL;DR: 该论文强调湍流运动中的定理而非理论，这些定理对20世纪伟大科学家的理论预测构成了约束


<details>
  <summary>Details</summary>
Motivation: 强调湍流研究中定理的重要性，这些定理为Onsager、Kolmogorov、Landau、Richardson等杰出科学家的理论预测提供了约束和验证框架

Method: 通过整理和分析湍流运动中的数学定理，将这些定理作为检验和约束理论预测的工具

Result: 建立了湍流理论中定理的集合，这些定理对经典湍流理论（如Onsager猜想、Kolmogorov标度律、Landau异议、Richardson级联等）构成了数学约束

Conclusion: 定理在湍流研究中具有核心地位，它们为理论预测提供了严格的数学约束，有助于验证和完善20世纪伟大科学家提出的湍流理论

Abstract: In these notes, we emphasize Theorems rather than Theories concerning turbulent fluid motion. Such theorems can be viewed as constraints on the theoretical predictions and expectations of some of the greatest scientific minds of the 20th century: Lars Onsager, Andrey Kolmogorov, Lev Landau, Lewis Fry Richardson among others.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [Fairness risk and its privacy-enabled solution in AI-driven robotic applications](https://arxiv.org/abs/2601.08953)
*Le Liu,Bangguo Yu,Nynke Vellinga,Ming Cao*

Main category: cs.RO

TL;DR: 该论文提出了一个针对机器人决策的效用感知公平性度量框架，并分析了公平性与用户数据隐私之间的相互作用，展示了隐私预算如何共同用于实现公平性目标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI驱动的自主机器决策系统存在公平性隐患，而机器人应用中缺乏能够同时捕捉用户效用和数据随机性的可实施公平性定义。在隐私保护成为法律要求的背景下，需要建立能够统一处理公平性和隐私的框架。

Method: 提出了一个效用感知的机器人决策公平性度量标准，并联合分析公平性与用户数据隐私，推导出隐私预算如何控制公平性度量的条件。建立了一个形式化和量化公平性及其与隐私相互作用的统一框架。

Result: 在机器人导航任务中测试了该框架，结果显示隐私预算可以共同用于满足公平性目标。这表明在隐私保护的法律要求下，机器人系统可以通过隐私预算同时实现公平性目标。

Conclusion: 通过创造性结合隐私考量来解决公平性问题，是迈向AI伦理使用的重要一步，增强了日常环境中部署的自主机器人的可信度。隐私预算与公平性目标的协同作用为构建更道德的自主系统提供了新途径。

Abstract: Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.

</details>


### [78] [Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object](https://arxiv.org/abs/2601.09104)
*Ko Yamamoto,Kyosuke Ishibashi,Hiroki Ishikawa,Osamu Azami*

Main category: cs.RO

TL;DR: 提出一种液压驱动软体机器人夹爪的设计方法，用于抓取10-20kg、直径20-30cm的大型重物，通过液压驱动替代传统气动方案以实现更大输出力。


<details>
  <summary>Details</summary>
Motivation: 现有软体夹爪多为气动驱动，工作压力仅数百kPa，无法产生足够力来抓取大型重物（10-20kg，直径20-30cm）。液压驱动具有数MPa压力的潜力，能产生更大功率。

Method: 开发液压驱动软体夹爪，基于数学模型确定基本设计参数（驱动压力、弯曲角度、物体质量、抓取力关系）；通过有限元分析选择适合抓取重物的材料；实现手指弯曲角的闭环控制。

Result: 成功实现了对20kg物体的抓取实验，并完成了手指弯曲角的闭环控制。

Conclusion: 液压驱动软体夹爪设计方法有效，能够抓取传统气动软体夹爪无法处理的大型重物，为重型软体机器人应用提供了可行方案。

Abstract: This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.

</details>


### [79] [CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space](https://arxiv.org/abs/2601.09163)
*Tong Wu,Shoujie Li,Junhao Gong,Changqing Guo,Xingting Li,Shilong Mu,Wenbo Ding*

Main category: cs.RO

TL;DR: CEI框架通过功能相似性度量和梯度优化实现不同机械臂与末端执行器之间的演示迁移，支持跨形态策略转移


<details>
  <summary>Details</summary>
Motivation: 现有机器人基础模型在大规模操作数据集上训练时，往往因数据集偏差而过度拟合特定视角、机械臂和平行夹爪，限制了其泛化能力

Method: 提出跨形态接口(CEI)框架，引入功能相似性概念并使用方向性Chamfer距离量化，通过梯度优化对齐机器人轨迹，为未见过的机械臂和末端执行器合成观测和动作

Result: 在仿真中将Franka Panda机器人的数据和策略迁移到16种不同形态，在真实世界实现UR5+AG95与UR5+Xhand之间的双向迁移，平均迁移率达82.4%，并展示了空间泛化和多模态运动生成能力

Conclusion: CEI框架有效解决了机器人基础模型对特定形态的过拟合问题，实现了跨不同机械臂和末端执行器的演示迁移与策略转移，提升了机器人策略的泛化能力

Abstract: Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \CEI introduces the concept of \textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \CEI transfers data and policies from a Franka Panda robot to \textbf{16} different embodiments across \textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\%. Finally, we demonstrate that \CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/

</details>


### [80] [Vision-Conditioned Variational Bayesian Last Layer Dynamics Models](https://arxiv.org/abs/2601.09178)
*Paul Brunzema,Thomas Lew,Ray Zhang,Takeru Shirasawa,John Subosits,Marcus Greiff*

Main category: cs.RO

TL;DR: 提出了一种基于视觉条件的变分贝叶斯最后一层动力学模型，通过视觉上下文预测环境变化，用于车辆赛车控制


<details>
  <summary>Details</summary>
Motivation: 机器人系统的敏捷控制需要预测环境对系统行为的影响，传统建模方法难以捕捉系统行为的突变，自适应方法本质上是反应式的，可能适应太晚而无法确保安全

Method: 提出视觉条件的变分贝叶斯最后一层动力学模型，首先学习名义车辆动力学，然后通过潜在特征的逐特征仿射变换进行微调，实现上下文感知的动力学预测，并将模型集成到最优控制器中

Result: 在雷克萨斯LC500赛车通过水坑的场景中验证，视觉条件系统在变化条件下完成了所有12圈尝试，而没有视觉上下文的所有基线方法都持续失控

Conclusion: 视觉条件动力学模型能够实现主动适应，在高性能应用中具有重要性，特别是在快速变化条件下

Abstract: Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.

</details>


### [81] [Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces](https://arxiv.org/abs/2601.09231)
*Shuoye Li,Zhiyuan Song,Yulin Li,Zhihai Bi,Jun Ma*

Main category: cs.RO

TL;DR: 提出使用多项式超曲面进行非线性分离的轨迹优化方法，避免传统凸近似方法的保守性，实现任意几何形状的碰撞避免


<details>
  <summary>Details</summary>
Motivation: 现有基于分离超平面的轨迹优化方法需要将机器人和障碍物近似为凸集，这在复杂狭窄环境中过于保守，限制了应用范围

Method: 1. 推广经典分离超平面定理，证明任意两个不相交的有界闭集都可以用多项式超曲面分离；2. 构建非线性规划问题，联合优化机器人轨迹和分离多项式系数，实现几何感知的碰撞避免

Result: 方法在仿真和真实实验中成功实现非凸机器人的平滑、无碰撞、敏捷机动，在凸近似基线方法失败的环境中表现优异

Conclusion: 通过引入多项式超曲面分离，彻底消除了轨迹优化中对凸近似的依赖，为任意几何形状的碰撞避免提供了理论保证和实用解决方案

Abstract: An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.

</details>


### [82] [Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report](https://arxiv.org/abs/2601.09318)
*Ro'i Lang,Elon Rimon*

Main category: cs.RO

TL;DR: 构建和分析用于3D工作空间中球形和圆柱形障碍物运动规划的多项式导航函数


<details>
  <summary>Details</summary>
Motivation: 为3D工作空间中的运动规划开发可靠的导航函数，特别是在存在球形和圆柱形障碍物（包括相交障碍物）的复杂环境中，需要避免局部最小值并确保目标点具有唯一非退化最小值

Method: 将工作空间建模为有界球形区域，使用平滑多项式隐函数编码障碍物，构建多项式导航函数，并进行梯度和Hessian分析以验证函数性质

Result: 建立了导航函数在目标点具有唯一非退化最小值且避免局部最小值的条件，即使在成对相交障碍物情况下也成立，并通过数值模拟在障碍物丰富的3D环境中验证了理论结果

Conclusion: 提出的多项式导航函数方法为3D工作空间中的运动规划提供了有效的数学框架，能够处理复杂障碍物配置，包括相交障碍物，确保了可靠的导航性能

Abstract: This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.

</details>


### [83] [ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving](https://arxiv.org/abs/2601.09377)
*Xuemei Yao,Xiao Yang,Jianbin Sun,Liuwei Xie,Xuebin Shao,Xiyu Fang,Hang Su,Kewei Yang*

Main category: cs.RO

TL;DR: ReflexDiffusion：一种推理阶段框架，通过反射调整增强基于扩散的轨迹规划器，专门针对高横向加速度场景（如急转弯）进行优化，在nuPlan Test14-hard基准测试中驾驶分数提升14.1%。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹规划器在长尾场景（特别是高横向加速度操作如急转弯）中存在系统性失败，这源于数据不平衡导致对车辆动力学、道路几何和环境约束建模不足，当车辆接近物理极限时产生次优或不安全的轨迹预测。

Method: ReflexDiffusion是一种推理阶段框架，在迭代去噪过程中引入基于梯度的调整机制：每次标准轨迹更新后，计算条件噪声预测和无条件噪声预测之间的梯度，显式放大关键条件信号（包括道路曲率和横向车辆动力学），强制严格遵守物理约束。

Result: 在nuPlan Test14-hard基准测试中，ReflexDiffusion在高横向加速度场景下相比最先进方法实现了14.1%的驾驶分数提升，证明推理时轨迹优化能有效补偿训练数据稀疏性，在操控极限附近动态强化安全关键约束。

Conclusion: ReflexDiffusion提供了一种架构无关的推理阶段解决方案，可直接部署到现有基于扩散的规划器中，通过动态强化安全关键约束来改善自动驾驶车辆在挑战性驾驶条件下的安全性，特别是在高横向加速度操作中。

Abstract: Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.

</details>


### [84] [Data Scaling for Navigation in Unknown Environments](https://arxiv.org/abs/2601.09444)
*Lauri Suomela,Naoki Takahata,Sasanka Kuruppu Arachchige,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 大规模数据多样性比数据量更重要，可实现零样本导航泛化


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习导航策略在未见环境中的泛化挑战，研究数据规模与多样性对真实世界端到端视觉导航的影响

Method: 使用来自35个国家161个地点的4,565小时众包数据集，训练点目标导航策略，在四个国家的125公里自主驾驶中评估闭环控制性能

Result: 大规模训练数据可实现未知环境中的零样本导航，接近环境特定演示训练的策略性能；数据多样性比数据量更重要，地理位置数量翻倍可使导航错误减少约15%

Conclusion: 数据多样性是实现导航泛化的关键因素，简单回归模型在噪声众包数据中优于生成式和序列架构

Abstract: Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.
  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.

</details>


### [85] [CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion](https://arxiv.org/abs/2601.09512)
*Ralf Römer,Yi Zhang,Angela P. Schoellig*

Main category: cs.RO

TL;DR: CLARE：一种用于视觉语言动作模型的免示例持续学习框架，通过轻量级模块化适配器和自动编码器路由机制实现高效持续适应


<details>
  <summary>Details</summary>
Motivation: 现有机器人持续学习方法需要存储先前数据、难以处理长任务序列或依赖任务标识符，而预训练VLA模型微调方法不适合需要长期适应新任务和环境同时保留已有知识的真实世界操作

Method: CLARE在选定前馈层中引入轻量级模块化适配器，通过层间特征相似性指导在必要时自主扩展模型；部署时使用基于自动编码器的路由机制动态激活最相关适配器，无需任务标签

Result: 在LIBERO基准测试中，CLARE在新任务上实现高性能且对早期任务无灾难性遗忘，显著优于基于示例的方法

Conclusion: CLARE为VLA模型提供了一种通用、参数高效的免示例持续学习框架，能够支持机器人在真实世界中长期适应新任务和环境

Abstract: To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.

</details>


### [86] [Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations](https://arxiv.org/abs/2601.09518)
*Wei-Jin Huang,Yue-Yi Zhang,Yi-Lin Wei,Zhi-Wei Xia,Juantao Tan,Yuan-Ming Li,Zhilin Zhao,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: PAIR+D-STAR框架：通过物理感知交互重定向从人类-人类交互数据生成高质量人形机器人交互数据，并结合解耦时空动作推理器学习超越模仿的协作行为


<details>
  <summary>Details</summary>
Motivation: 人形机器人与人类物理交互面临高质量人-人形交互数据稀缺的挑战，而利用丰富的人类-人类交互数据作为可扩展替代方案时，标准重定向方法会破坏关键接触点，且传统模仿学习策略缺乏交互理解

Method: 提出两阶段框架：1) PAIR（物理感知交互重定向）- 基于接触中心的管道，通过保持接触语义跨形态差异生成物理一致的人-人形交互数据；2) D-STAR（解耦时空动作推理器）- 分层策略，通过相位注意力（何时行动）和多尺度空间模块（何处行动）解耦时空推理，由扩散头融合生成同步全身行为

Result: 通过广泛严格的仿真验证，相比基线方法获得显著性能提升，展示了从人类-人类交互数据学习复杂全身交互的完整有效管道

Conclusion: PAIR+D-STAR框架成功解决了从人类-人类交互数据学习人形机器人交互的两个关键挑战：生成物理一致的数据和超越简单模仿的交互理解，为人形机器人物理交互学习提供了可扩展的解决方案

Abstract: Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.

</details>


### [87] [Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping](https://arxiv.org/abs/2601.09578)
*Jiajun Sun,Yangyi Ou,Haoyuan Zheng,Chao yang,Yue Ma*

Main category: cs.RO

TL;DR: 提出一种将热信息语义增强到3D点云地图的新方法，通过可见光与红外图像像素级融合，将LiDAR点云投影到融合图像流，分割热源特征识别高温目标，并将温度信息作为语义层添加到最终3D地图中。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，自主机器人导航和环境感知对SLAM技术提出了更高要求。需要生成不仅具有准确几何结构，还能对环境有重要语义理解的地图，这对于灾害快速评估和工业预防性维护等特定应用至关重要。

Method: 1. 对可见光和红外图像进行像素级融合；2. 将实时LiDAR点云投影到融合图像流上；3. 在热通道中分割热源特征以即时识别高温目标；4. 将温度信息作为语义层应用到最终的3D地图中。

Result: 该方法生成的地图不仅具有准确的几何结构，还具备对环境的关键语义理解能力。能够即时识别高温目标，为灾害评估和工业维护等应用提供有价值的语义信息。

Conclusion: 提出的语义增强3D点云地图方法通过融合热信息，显著提升了SLAM系统的环境感知能力，为特定应用场景如灾害快速评估和工业预防性维护提供了高度实用的解决方案。

Abstract: In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [88] [A generalization of Hartog's extension of line bundles](https://arxiv.org/abs/2601.09645)
*Youssef Alaoui*

Main category: math.CV

TL;DR: 证明了在复流形上，如果存在q-凸带角函数，则当1≤q≤n-3时，{f>c}上的全纯线丛可以唯一地延拓到整个流形上


<details>
  <summary>Details</summary>
Motivation: 推广已有的q-完备带角复流形上的全纯线丛延拓结果，从n≥3q的强条件扩展到更一般的q-凸带角情形，并放宽维数限制

Method: 利用q-凸带角函数的几何性质，通过复分析技术研究全纯线丛的延拓问题，建立相应的存在唯一性定理

Result: 证明了当1≤q≤n-3时，对于存在q-凸带角函数的复流形X，{f>c}上的任意全纯线丛都可以唯一地延拓到整个X上

Conclusion: 该结果将文献[ref5]中关于q-完备带角流形的结论推广到更一般的q-凸带角情形，扩展了全纯线丛延拓定理的适用范围

Abstract: In this article, we prove that if $X$ is a complex manifold of dimension $n\geq 4$ such that there exists a $q$-convex with corners function $f\in F_{q}(X)$, then every holomorphic line bundle over $\{f>c\}$ extends uniquely to $X$ if $1\leq q\leq n-3$. This generalizes a well-known result obtained in \cite{ref5} for $q$-complete with corners complex manifolds with a corresponding exhaustion function $f \in F_{q}(X)$, when $n \geq 3q$.

</details>
