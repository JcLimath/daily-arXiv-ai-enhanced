<div id=toc></div>

# Table of Contents

- [math.CV](#math.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 21]


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [1] [Duality between Bott-Chern and Aeppli Cohomology on Non-Compact Complex Manifolds](https://arxiv.org/abs/2601.03529)
*Xiaojun Wu*

Main category: math.CV

TL;DR: 在非紧复流形上建立Bott-Chern和Aeppli上同调的对偶定理，需要适当的伪凸性条件，在Stein流形上获得完全对偶，并构造反例说明无伪凸性时定理失效。


<details>
  <summary>Details</summary>
Motivation: 研究非紧复流形上Bott-Chern和Aeppli上同调的对偶关系，扩展紧复流形上的已知结果，探索伪凸性条件在非紧情形下的作用。

Method: 在非紧复流形上建立对偶定理，特别关注Stein流形情形，同时构造非紧复曲面上的反例来验证无伪凸性时定理的失效。

Result: 在适当伪凸性条件下建立了Bott-Chern和Aeppli上同调的对偶定理，在Stein流形上获得了完全对偶（扩展了Serre对偶），并证明了无伪凸性时这些结果一般失效。

Conclusion: 伪凸性是非紧复流形上Bott-Chern-Aeppli对偶的关键条件，在Stein流形上可获得完全对偶，但无伪凸性时对偶定理一般失效，通过构造反例证实了这一现象。

Abstract: In this paper we establish duality theorems relating Bott-Chern and Aeppli cohomology, both with and without compact support, on non-compact complex manifolds under suitable pseudoconvexity assumptions. In particular, on Stein manifolds we obtain a full Bott-Chern-Aeppli duality extending Serre duality for Dolbeault cohomology. We also show that these results fail in general without pseudoconvexity assumptions by constructing explicit counterexamples on non-compact complex surfaces.

</details>


### [2] [Normalized Milnor Fibrations for Real Analytic Maps](https://arxiv.org/abs/2601.03538)
*José Luis Cisneros Molina,Aurélio Menegon*

Main category: math.CV

TL;DR: 对于具有孤立临界值的局部满射实解析映射，通过目标空间同胚变换可使映射成为d-正则，从而获得球面上的规范化Milnor纤维化，揭示了实解析与复解析奇点之间更紧密的拓扑平行性。


<details>
  <summary>Details</summary>
Motivation: 研究实解析映射的Milnor纤维化理论，特别是解决实解析映射在球面上规范化纤维化（f/||f||）通常不存在的问题，探索实解析与复解析奇点之间的拓扑平行性。

Method: 对于满足横截性条件的局部满射实解析映射，构造目标空间的同胚变换h，使得复合映射h^{-1}f成为d-正则映射，从而获得球面上的规范化Milnor纤维化。

Result: 证明了对于满足横截性条件的局部满射实解析映射，存在目标空间同胚h使得h^{-1}f成为d-正则，从而规范化映射(h^{-1}f)/||h^{-1}f||在球面上定义光滑局部平凡纤维化，该纤维化等价于管上的Milnor-Le纤维化和球面上的Milnor纤维化。

Conclusion: 实解析映射的规范化纤维化障碍并非本质的，通过适当的目标空间变换可消除，这揭示了实解析与复解析奇点之间比先前认识更紧密的拓扑平行性，且不改变奇点的拓扑类型。

Abstract: Milnor's fibration theorem and its generalizations play a central role in the study of singularities of complex and real analytic maps. In the complex analytic case, the Milnor fibration on the sphere is always given by the normalized map $f/|f|$. In contrast, for real analytic maps the existence of such a normalized Milnor fibration generally fails, even when a Milnor--Le fibration exists on a tube.
  For locally surjective real analytic maps $f:(R^n,0)->(R^k,0)$ with isolated critical value, the existence of a Milnor--Le fibration on a tube is guaranteed under a transversality condition. However, the associated fibration on the sphere need not be given by the normalized map $f/||f||$, unless an additional regularity condition (d-regularity) is imposed.
  In this paper we show that this apparent obstruction is not intrinsic. More precisely, we prove that for any such map satisfying the transversality property, there exists a homeomorphism $h:(R^k,0)->(R^k,0)$ of the target space such that the composition $h^{-1}f$ becomes d-regular. As a consequence, the normalized map $(h^{-1}f)/||h^{-1}f||$ defines a smooth locally trivial fibration on the sphere, which is equivalent to both the Milnor--Le fibration on the tube and the Milnor fibration on the sphere. Our result reveals a closer topological parallel between real and complex analytic singularities than previously recognized, without changing the topological type of the singularity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization](https://arxiv.org/abs/2601.03359)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: 提出多智能体工作流，将主任务描述优化与约束条件解耦，通过量化评分反馈迭代改进提示词，显著提升LLM输出对形式约束的遵从性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成内容相关但不符合形式约束的输出，传统提示优化方法只关注主任务描述的重述，忽略了作为响应验收标准的细粒度约束条件

Method: 提出多智能体工作流，将主任务描述优化与约束条件解耦，使用量化评分作为反馈，迭代重写和改进提示词

Result: 该方法生成的修订提示词在Llama 3.1 8B和Mixtral-8x 7B等模型上产生显著更高的遵从性评分

Conclusion: 通过解耦任务描述与约束条件优化，并利用量化反馈迭代改进，可以有效提升LLM输出对形式约束的遵从性

Abstract: Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.

</details>


### [4] [Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms](https://arxiv.org/abs/2601.03470)
*Michael C. Darling,Alan H. Hesu,Michael A. Mardikes,Brian C. McGuigan,Reed M. Milewicz*

Main category: cs.AI

TL;DR: 提出基于成熟度的认证框架，通过显式测量机制对具身AI系统进行认证，强调结构化评估、量化评分和多目标权衡方法


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统化的具身AI认证方法，需要建立结构化评估框架和量化测量机制来确保系统的可信度

Method: 提出基于成熟度的认证框架，包含结构化评估框架、量化评分机制和多目标权衡导航方法；以不确定性量化作为示例测量机制，通过无人机系统检测案例研究验证可行性

Result: 展示了该认证框架的可行性，通过无人机系统检测案例研究验证了不确定性量化作为测量机制的有效性

Conclusion: 基于成熟度的认证框架为具身AI系统提供了系统化的认证方法，结构化评估、量化测量和多目标权衡导航是实现可信具身AI的关键要素

Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.

</details>


### [5] [CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support](https://arxiv.org/abs/2601.03475)
*Ruiqi Deng,Geoffrey Martin,Tony Wang,Gongbo Zhang,Yi Liu,Chunhua Weng,Yanshan Wang,Justin F Rousseau,Yifan Peng*

Main category: cs.AI

TL;DR: CPGPrompt：一种将叙述性临床指南转换为LLM可执行决策树的自动提示系统，在专科转诊决策上表现优异，但在多类别路径分配上存在领域特异性差异。


<details>
  <summary>Details</summary>
Motivation: 临床实践指南（CPGs）为患者护理提供循证建议，但将其整合到人工智能中面临挑战。现有方法如基于规则的系统存在可解释性差、指南依从性不一致和领域适用性窄等局限性。

Method: 开发CPGPrompt系统，将叙述性临床指南转换为结构化决策树，并利用大型语言模型（LLM）动态导航这些决策树进行患者病例评估。在三个领域（头痛、腰痛和前列腺癌）生成合成病例，分为四类测试不同决策场景。

Result: 二元专科转诊分类在所有领域均表现强劲（F1：0.85-1.00），召回率高（1.00±0.00）。多类别路径分配表现降低，存在领域特异性差异：头痛（F1：0.47）、腰痛（F1：0.72）和前列腺癌（F1：0.77）。性能差异反映了各指南的结构特点。

Conclusion: CPGPrompt系统能够有效将临床指南整合到AI中，在二元决策任务上表现优异。多类别路径分配的性能差异揭示了不同指南结构对AI系统性能的影响，为未来改进提供了方向。

Abstract: Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs).
  Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks.
  The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.

</details>


### [6] [Personalization of Large Foundation Models for Health Interventions](https://arxiv.org/abs/2601.03482)
*Stefan Konigorski,Johannes E. Vedder,Babajide Alamu Owoyele,İbrahim Özkan*

Main category: cs.AI

TL;DR: 大型基础模型（LFMs）在医疗AI中具有潜力，但无法替代N-of-1试验。两者互补：LFMs擅长从群体数据生成假设，N-of-1试验擅长个体因果验证。提出混合框架结合两者优势。


<details>
  <summary>Details</summary>
Motivation: 探讨大型基础模型能否提供真正个性化的治疗建议，识别医疗AI中的矛盾（泛化悖论、隐私-性能悖论等），并解决LFMs在个性化医疗中的局限性。

Method: 提出混合框架：LFMs从多模态群体数据生成干预候选排名和不确定性估计，触发后续N-of-1试验进行个体因果验证。明确预测与因果推理的边界。

Result: LFMs无法替代N-of-1试验，但两者互补。混合框架能利用LFMs的假设生成能力和N-of-1试验的因果验证能力，解决个性化医疗中的悖论问题。

Conclusion: LFMs和N-of-1试验在个性化医疗中具有互补作用。明确预测与因果的边界，并解决AI医疗中的矛盾，对于负责任地整合AI至关重要。混合框架为个性化医疗提供了可行路径。

Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.

</details>


### [7] [STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537)
*Di Wu,Yanyan Zhao,Xin Lu,Mingzhe Li,Bing Qin*

Main category: cs.AI

TL;DR: STAR-S是一个通过自学习循环来增强LLM安全推理能力、防御越狱攻击的框架


<details>
  <summary>Details</summary>
Motivation: 现有基于安全规则推理的方法难以确定何种形式的安全推理能有效防御越狱攻击，且这种推理形式难以显式设计或直接获取

Method: 提出STAR-S框架，将安全规则推理学习整合到自学习循环中：通过安全规则引导的推理和反思来生成数据，然后通过微调增强安全推理能力，形成协同循环

Result: 实验表明STAR-S能有效防御越狱攻击，性能优于基线方法

Conclusion: STAR-S通过自学习循环提升模型对安全规则的理解和推理能力，为LLM安全防御提供了有效解决方案

Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \textbf{STAR-S} (\textbf{S}elf-\textbf{TA}ught \textbf{R}easoning based on \textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.

</details>


### [8] [ReEfBench: Quantifying the Reasoning Efficiency of LLMs](https://arxiv.org/abs/2601.03550)
*Zhizhang Fu,Yuancheng Gu,Chenkai Hu,Hanmeng Liu,Yue Zhang*

Main category: cs.AI

TL;DR: 论文提出神经符号框架评估LLM推理过程，发现扩展token生成并非深度推理必要条件，并揭示混合长短CoT训练会导致过早饱和，蒸馏到小模型无法复制逻辑效能。


<details>
  <summary>Details</summary>
Motivation: 当前CoT评估方法存在局限性，无法区分性能提升是来自真实推理还是仅仅增加冗长度，需要非侵入式、全面的过程中心评估框架来揭示LLM推理的本质。

Method: 提出神经符号框架进行非侵入式、全面的过程中心推理评估，识别四种行为原型并诊断失败模式，研究推理模式、训练策略和模型规模的影响。

Result: 发现扩展token生成并非深度推理的必要条件；混合长短CoT数据训练会导致过早饱和和崩溃；蒸馏到小模型能复制行为长度但无法复制逻辑效能，受限于内在容量限制。

Conclusion: 需要超越传统CoT评估的框架来理解LLM推理，扩展token生成不等于深度推理，训练策略和模型规模对推理能力有重要影响，蒸馏存在固有局限性。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.

</details>


### [9] [Controllable LLM Reasoning via Sparse Autoencoder-Based Steering](https://arxiv.org/abs/2601.03595)
*Yi Fang,Wenjie Wang,Mingfeng Xue,Boyi Deng,Fengli Xu,Dayiheng Liu,Fuli Feng*

Main category: cs.AI

TL;DR: SAE-Steering：利用稀疏自编码器分解推理策略纠缠的隐状态，通过两阶段特征识别管道控制大推理模型的推理策略，提高控制效果和准确性


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）能自主选择推理策略，但这种自主选择常产生低效甚至错误的推理路径。现有方法难以控制细粒度推理策略，因为策略概念在隐状态中纠缠。需要开发控制推理策略的方法以提高可靠性和灵活性。

Method: 使用稀疏自编码器（SAEs）将策略纠缠的隐状态分解为解缠的特征空间。提出SAE-Steering两阶段特征识别管道：1）通过放大策略特定关键词的logits召回特征，过滤99%以上特征；2）按控制效果对剩余特征排序。将识别出的策略特定特征作为控制向量。

Result: SAE-Steering在控制效果上比现有方法高出15%以上。控制推理策略能将LRMs从错误路径重定向到正确路径，实现7%的绝对准确率提升。

Conclusion: 通过稀疏自编码器解缠隐状态并识别策略特定特征，SAE-Steering能有效控制大推理模型的推理策略，提高推理的可靠性和准确性，为解决自主推理策略选择的问题提供了有效方案。

Abstract: Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing methods struggle to control fine-grained reasoning strategies due to conceptual entanglement in LRMs' hidden states. To address this, we leverage Sparse Autoencoders (SAEs) to decompose strategy-entangled hidden states into a disentangled feature space. To identify the few strategy-specific features from the vast pool of SAE features, we propose SAE-Steering, an efficient two-stage feature identification pipeline. SAE-Steering first recalls features that amplify the logits of strategy-specific keywords, filtering out over 99\% of features, and then ranks the remaining features by their control effectiveness. Using the identified strategy-specific features as control vectors, SAE-Steering outperforms existing methods by over 15\% in control effectiveness. Furthermore, controlling reasoning strategies can redirect LRMs from erroneous paths to correct ones, achieving a 7\% absolute accuracy improvement.

</details>


### [10] [How Does the Thinking Step Influence Model Safety? An Entropy-based Safety Reminder for LRMs](https://arxiv.org/abs/2601.03662)
*Su-Hyeon Kim,Hyundong Jin,Yejin Lee,Yo-Sub Han*

Main category: cs.AI

TL;DR: SafeRemind：一种在解码时向大型推理模型思考步骤动态注入安全提醒短语的防御方法，通过熵触发器在决策锁定点进行干预，显著提升模型安全性而不影响推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思考步骤取得显著成功，但这些步骤可能放大不安全行为。现有防御机制因忽视LRM独特的推理动态而无效。研究发现思考步骤中出现安全提醒短语对确保LRM安全至关重要。

Method: 提出SafeRemind解码时防御方法：1）利用熵触发器检测决策锁定点；2）动态向思考步骤注入安全提醒短语；3）无需参数更新，通过干预将有害轨迹重定向至安全结果。

Result: 在5个LRM和6个基准测试上的广泛评估表明，SafeRemind显著提升安全性，改进幅度高达45.5个百分点，同时保持核心推理效用。

Conclusion: SafeRemind通过动态注入安全提醒短语有效应对LRM推理过程中的安全风险，提供了一种无需参数更新的高效防御机制，在安全性和实用性间取得良好平衡。

Abstract: Large Reasoning Models (LRMs) achieve remarkable success through explicit thinking steps, yet the thinking steps introduce a novel risk by potentially amplifying unsafe behaviors. Despite this vulnerability, conventional defense mechanisms remain ineffective as they overlook the unique reasoning dynamics of LRMs. In this work, we find that the emergence of safe-reminding phrases within thinking steps plays a pivotal role in ensuring LRM safety. Motivated by this finding, we propose SafeRemind, a decoding-time defense method that dynamically injects safe-reminding phrases into thinking steps. By leveraging entropy triggers to intervene at decision-locking points, SafeRemind redirects potentially harmful trajectories toward safer outcomes without requiring any parameter updates. Extensive evaluations across five LRMs and six benchmarks demonstrate that SafeRemind substantially enhances safety, achieving improvements of up to 45.5%p while preserving core reasoning utility.

</details>


### [11] [Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction](https://arxiv.org/abs/2601.03672)
*Chen Zhang,Kepu Zhang,Jiatong Zhang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: SandwichR是一种新颖的推理方法，采用"答案-推理-答案"范式，通过一致性感知强化学习策略，在保持CoT推理准确性的同时显著降低延迟，解决了查询纠正中的延迟-准确性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 查询纠正是现代搜索管道的关键入口点，需要在实时延迟约束下保持高准确性。虽然链式思维（CoT）推理能提高准确性，但其延迟过高无法满足实时查询纠正需求。提前输出答案虽然能降低延迟，但在自回归解码下，早期答案独立于后续推理，无法利用推理能力提升准确性。

Method: 提出Sandwich Reasoning（SandwichR）方法，采用"答案-推理-答案"范式：先生成初始纠正，然后进行显式推理过程，最后生成最终精炼纠正。通过一致性感知强化学习策略实现初始答案与后验推理的对齐：1）专用一致性奖励强制初始和最终纠正之间的一致性；2）基于边际的拒绝采样优先处理推理带来最大纠正增益的边界样本。此外，构建了高质量的查询纠正数据集。

Result: 实验结果表明，SandwichR实现了与标准CoT相当的最先进准确性，同时提供了40-70%的延迟降低，解决了在线搜索中的延迟-准确性权衡问题。

Conclusion: SandwichR通过将快速初始答案与后验推理对齐，实现了低延迟查询纠正而不牺牲推理感知的准确性，解决了CoT推理在实时应用中的延迟瓶颈问题。

Abstract: Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.

</details>


### [12] [Personalized Medication Planning via Direct Domain Modeling and LLM-Generated Heuristics](https://arxiv.org/abs/2601.03687)
*Yonatan Vernik,Alexander Tuisov,David Izhaki,Hana Weitman,Gal A. Kaminka,Alexander Shleyfman*

Main category: cs.AI

TL;DR: 使用LLM生成领域特定启发式函数，将个性化用药规划从最多7种药物扩展到至少28种，显著提升覆盖率和规划时间


<details>
  <summary>Details</summary>
Motivation: 个性化用药规划需要为每位患者选择药物并确定给药方案以实现特定医疗目标。先前使用通用领域无关启发式函数的方法最多只能处理7种药物，这在临床实践中远远不够，需要扩展药物数量以接近实际应用

Method: 通过编程方式指定领域（定义初始状态和状态转移过程），使用大型语言模型（LLM）生成针对特定问题的启发式函数，然后与固定的搜索算法（GBFS）结合使用

Result: 该方法在覆盖率和规划时间方面取得了显著改进，将可处理的药物数量扩展到至少28种，使个性化用药规划更接近实际临床应用

Conclusion: 使用LLM自动生成领域特定启发式函数是扩展个性化用药规划规模的有效方法，能够处理更多药物，为临床实践应用迈出了重要一步

Abstract: Personalized medication planning involves selecting medications and determining a dosing schedule to achieve medical goals specific to each individual patient. Previous work successfully demonstrated that automated planners, using general domain-independent heuristics, are able to generate personalized treatments, when the domain and problems are modeled using a general domain description language (\pddlp). Unfortunately, this process was limited in practice to consider no more than seven medications. In clinical terms, this is a non-starter. In this paper, we explore the use of automatically-generated domain- and problem-specific heuristics to be used with general search, as a method of scaling up medication planning to levels allowing closer work with clinicians. Specifically, we specify the domain programmatically (specifying an initial state and a successor generation procedure), and use an LLM to generate a problem specific heuristic that can be used by a fixed search algorithm (GBFS). The results indicate dramatic improvements in coverage and planning time, scaling up the number of medications to at least 28, and bringing medication planning one step closer to practical applications.

</details>


### [13] [EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation](https://arxiv.org/abs/2601.03769)
*Zihang Li,Yuhang Wang,Yikun Zong,Wenhan Yu,Xiaokun Yuan,Runhan Jiang,Zirui Liu,Tong Yang,Arthur Jiang*

Main category: cs.AI

TL;DR: EntroCoT：通过熵基分割和蒙特卡洛评估自动识别和优化低质量思维链监督数据，构建高质量数学推理数据集


<details>
  <summary>Details</summary>
Motivation: 现有思维链微调数据集存在"答案正确但推理错误"问题，即最终答案正确但中间步骤存在幻觉、冗余或逻辑错误，需要自动识别和优化低质量监督数据

Method: 提出EntroCoT统一框架：1) 基于熵的机制在不确定节点分割推理轨迹；2) 蒙特卡洛rollout机制评估每个步骤的边际贡献；3) 准确筛选欺骗性推理样本，构建高质量数据集

Result: 在数学基准测试上的广泛实验表明，使用EntroCoT构建的子集进行微调始终优于全数据集监督的基线方法

Conclusion: EntroCoT能有效识别和优化低质量思维链监督数据，构建的高质量数据集显著提升大语言模型的数学推理能力

Abstract: Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the "answer right but reasoning wrong" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.

</details>


### [14] [ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition](https://arxiv.org/abs/2601.03822)
*Muyang Zhao,Qi Qi,Hao Sun*

Main category: cs.AI

TL;DR: ROI-Reasoning框架通过元认知微调和理性感知强化学习，使LLM在严格全局token约束下进行预算推理，将问题形式化为有序随机多选择背包问题，优化计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理能力，但无法自动判断不同任务所需的计算量。在严格全局token约束下，需要让模型具备元认知能力来预测任务难度、估计投资回报率，并战略性地分配计算资源。

Method: 提出ROI-Reasoning两阶段框架：1) 元认知微调阶段，训练模型在生成前预测推理成本和预期效用，做出明确的解决或跳过决策；2) 理性感知强化学习阶段，在硬token预算下优化序列决策，学习长期分配策略。

Result: 在预算数学推理基准测试中，ROI-Reasoning在严格计算预算下持续提高总体得分，同时显著减少遗憾（regret）。

Conclusion: 将推理时间预算问题形式化为OS-MCKP问题，并通过ROI-Reasoning框架赋予LLM内在的预算感知理性能力，实现了在有限计算资源下的高效推理分配。

Abstract: Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.

</details>


### [15] [Defeasible Conditionals using Answer Set Programming](https://arxiv.org/abs/2601.03840)
*Racquel Dennison,Jesse Heyninck,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种使用答案集编程（ASP）计算理性闭包（RC）的声明式方法，实现了从知识库自动构建最小排序模型并支持查询蕴含检查，证明了编码的正确性，并通过实验验证了比现有命令式实现更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 理性闭包（RC）是KLM框架中用于可废止蕴含推理的重要算法，但现有实现多为命令式方法。本文旨在提供一种基于答案集编程（ASP）的声明式定义，实现更高效、更符合理论基础的RC计算。

Method: 提出了一种使用答案集编程（ASP）的声明式方法来计算理性闭包。该方法能够从给定知识库自动构建最小排序模型，并支持对指定查询进行蕴含检查。作者形式化证明了ASP编码的正确性。

Result: 实验评估表明，基于ASP的实现相比现有的命令式实现（特别是InfOCF求解器）具有更高的计算效率。结果验证了该方法既符合理性闭包的理论基础，又在计算性能上有所提升。

Conclusion: 基于答案集编程的声明式方法为计算理性闭包提供了一种有效的替代方案，不仅保持了理论正确性，还提高了计算效率，为可废止推理的实际应用提供了更好的工具支持。

Abstract: Defeasible entailment is concerned with drawing plausible conclusions from incomplete information. A foundational framework for modelling defeasible entailment is the KLM framework. Introduced by Kraus, Lehmann, and Magidor, the KLM framework outlines several key properties for defeasible entailment. One of the most prominent algorithms within this framework is Rational Closure (RC). This paper presents a declarative definition for computing RC using Answer Set Programming (ASP). Our approach enables the automatic construction of the minimal ranked model from a given knowledge base and supports entailment checking for specified queries. We formally prove the correctness of our ASP encoding and conduct empirical evaluations to compare the performance of our implementation with that of existing imperative implementations, specifically the InfOCF solver. The results demonstrate that our ASP-based approach adheres to RC's theoretical foundations and offers improved computational efficiency.

</details>


### [16] [XAI-LAW: A Logic Programming Tool for Modeling, Explaining, and Learning Legal Decisions](https://arxiv.org/abs/2601.03844)
*Agostino Dovier,Talissa Dreossi,Andrea Formisano,Benedetta Strizzolo*

Main category: cs.AI

TL;DR: 使用ASP对意大利刑法典建模，通过半自动学习司法判例生成法律规则，支持刑事审判推理并提供解释性


<details>
  <summary>Details</summary>
Motivation: 开发支持法律专家在刑事审判阶段进行推理的工具，通过ASP建模刑法条款，从司法判例中学习法律规则，提高决策过程的透明度和可解释性

Method: 将意大利刑法典条款（特别是"人身犯罪"和财产犯罪）编码为ASP程序，利用稳定模型的"支持性"提供解释，集成归纳逻辑编程系统从案例中归纳法律规则

Result: 开发出能够处理矛盾、生成新案件可能决策并提供解释的工具，通过先前判决验证模型有效性，实现法律规则的半自动学习

Conclusion: ASP方法有效支持刑事审判推理，工具提供的自动解释性增强了司法决策的透明度，归纳逻辑编程系统能够从案例中学习法律规则，为法律专家提供有价值的决策支持

Abstract: We propose an approach to model articles of the Italian Criminal Code (ICC), using Answer Set Programming (ASP), and to semi-automatically learn legal rules from examples based on prior judicial decisions. The developed tool is intended to support legal experts during the criminal trial phase by providing reasoning and possible legal outcomes. The methodology involves analyzing and encoding articles of the ICC in ASP, including "crimes against the person" and property offenses. The resulting model is validated on a set of previous verdicts and refined as necessary. During the encoding process, contradictions may arise; these are properly handled by the system, which also generates possible decisions for new cases and provides explanations through a tool that leverages the "supportedness" of stable models. The automatic explainability offered by the tool can also be used to clarify the logic behind judicial decisions, making the decision-making process more interpretable. Furthermore, the tool integrates an inductive logic programming system for ASP, which is employed to generalize legal rules from case examples.

</details>


### [17] [Formally Explaining Decision Tree Models with Answer Set Programming](https://arxiv.org/abs/2601.03845)
*Akihiro Takemura,Masayuki Otani,Katsumi Inoue*

Main category: cs.AI

TL;DR: 使用答案集编程（ASP）为决策树模型生成多种解释类型的方法，相比SAT方法更灵活且支持枚举所有可能解释


<details>
  <summary>Details</summary>
Motivation: 决策树模型（如随机森林和梯度提升决策树）在机器学习中广泛使用，但复杂结构使其难以解释，特别是在需要正式论证的安全关键应用中。现有研究已证明可通过自动推理技术推导逻辑和溯因解释。

Method: 提出基于答案集编程（ASP）的方法，用于生成多种解释类型：充分解释、对比解释、多数解释和树特定解释。相比基于SAT的方法，ASP方法在编码用户偏好方面更灵活，并支持枚举所有可能的解释。

Result: 在多样化数据集上进行了实证评估，展示了该方法相比现有方法的有效性和局限性。

Conclusion: ASP方法为决策树模型提供了一种灵活且全面的解释生成框架，能够支持多种解释类型并枚举所有可能解释，在需要形式化论证的场景中具有应用价值。

Abstract: Decision tree models, including random forests and gradient-boosted decision trees, are widely used in machine learning due to their high predictive performance.  However, their complex structures often make them difficult to interpret, especially in safety-critical applications where model decisions require formal justification.  Recent work has demonstrated that logical and abductive explanations can be derived through automated reasoning techniques.  In this paper, we propose a method for generating various types of explanations, namely, sufficient, contrastive, majority, and tree-specific explanations, using Answer Set Programming (ASP).  Compared to SAT-based approaches, our ASP-based method offers greater flexibility in encoding user preferences and supports enumeration of all possible explanations.  We empirically evaluate the approach on a diverse set of datasets and demonstrate its effectiveness and limitations compared to existing methods.

</details>


### [18] [Investigating the Grounding Bottleneck for a Large-Scale Configuration Problem: Existing Tools and Constraint-Aware Guessing](https://arxiv.org/abs/2601.03850)
*Veronika Semmelrock,Gerhard Friedrich*

Main category: cs.AI

TL;DR: 该论文研究了ASP在大型配置问题中的可扩展性，特别是针对包含超过30,000个组件的电子系统配置，分析了当前ASP技术在处理接地瓶颈方面的潜力和限制。


<details>
  <summary>Details</summary>
Motivation: 研究当前ASP求解技术是否能够扩展到大型配置问题，特别是电子系统配置这类可能包含超过30,000个组件的大规模问题。ASP虽然在许多应用领域实现了"用户指定问题，计算机解决问题"的AI愿景，但在处理大规模配置问题时面临接地瓶颈的挑战。

Method: 1. 以电子系统配置作为基准问题进行研究；2. 重点分析解决接地瓶颈的方法；3. 探索增量求解方法；4. 基于接地分析开发了约束感知猜测方法，显著减少内存需求。

Result: 1. 增量求解方法在实践中证明有效；2. 但即使在增量方法中，内存需求仍然构成显著限制；3. 开发的约束感知猜测方法显著减少了内存需求，推动了ASP在大规模配置问题中的应用极限。

Conclusion: 当前ASP技术在处理大型配置问题时面临接地瓶颈的挑战，内存需求随问题实例规模急剧增加。虽然增量求解方法在实践中有效，但内存限制仍然显著。通过分析接地过程开发的约束感知猜测方法能够显著减少内存需求，为ASP在大规模配置问题中的应用提供了新的可能性。

Abstract: Answer set programming (ASP) aims to realize the AI vision: The user specifies the problem, and the computer solves it. Indeed, ASP has made this vision true in many application domains. However, will current ASP solving techniques scale up for large configuration problems? As a benchmark for such problems, we investigated the configuration of electronic systems, which may comprise more than 30,000 components. We show the potential and limits of current ASP technology, focusing on methods that address the so-called grounding bottleneck, i.e., the sharp increase of memory demands in the size of the problem instances. To push the limits, we investigated the incremental solving approach, which proved effective in practice. However, even in the incremental approach, memory demands impose significant limits. Based on an analysis of grounding, we developed the method constraint-aware guessing, which significantly reduced the memory need.

</details>


### [19] [Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification](https://arxiv.org/abs/2601.03948)
*Rui Sun,Yifan Sun,Sheng Xu,Li Zhao,Jing Li,Daxin Jiang,Chen Hua,Zuo Bai*

Main category: cs.AI

TL;DR: Trade-R1框架通过过程级推理验证，将可验证奖励与随机金融环境连接，解决标准RL在金融决策中的奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: RL在数学和编码等可验证奖励领域表现优异，但金融市场的随机性导致奖励噪声大，标准RL容易退化为奖励黑客，需要新方法解决这一问题

Method: 提出Trade-R1训练框架，通过结构化RAG任务将冗长金融文档的推理评估转化为三角一致性度量，评估检索证据、推理链和决策之间的对齐度，作为噪声市场回报的有效性过滤器。采用两种奖励整合策略：FSR用于稳定对齐信号，DSR用于耦合幅度优化

Result: 在不同国家资产选择实验中，该范式减少了奖励黑客现象，DSR实现了优越的跨市场泛化能力，同时保持了最高的推理一致性

Conclusion: 通过过程级推理验证将可验证奖励与随机环境连接是解决金融决策中RL奖励黑客问题的有效方法，Trade-R1框架为LLM在金融领域的应用提供了新范式

Abstract: Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.

</details>


### [20] [Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models](https://arxiv.org/abs/2601.03969)
*Wei Wu,Liyi Chen,Congxi Xiao,Tianfu Wang,Qimeng Wang,Chengqiang Lu,Yan Gao,Yi Wu,Yao Hu,Hui Xiong*

Main category: cs.AI

TL;DR: 提出DOT方法解决大模型在简单问题上过度推理的问题，通过动态截断冗余token，在保持复杂问题推理能力的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有强化学习增强的大推理模型在简单查询上存在过度冗长的问题，导致部署成本高昂。现有基于显式长度惩罚的高效推理方法存在优化冲突，且未深入探究导致过度思考的生成机制

Method: 提出动态异常截断(DOT)：1) 在训练时选择性抑制冗余token；2) 仅针对完全正确rollout组中的极端尾部响应长度；3) 结合辅助KL正则化和预测性动态采样确保稳定收敛

Result: 在多个模型规模上显著扩展了效率-性能的帕累托前沿。在AIME-24上，推理token使用减少78%，同时准确率相比初始策略有所提升，并超越现有高效推理方法

Conclusion: DOT方法有效解决了模型在简单输入上的长度偏移现象，通过训练时干预选择性抑制冗余token，在保持复杂问题长时程推理能力的同时大幅提升推理效率

Abstract: Large reasoning models enhanced by reinforcement learning with verifiable rewards have achieved significant performance gains by extending their chain-of-thought. However, this paradigm incurs substantial deployment costs as models often exhibit excessive verbosity on simple queries. Existing efficient reasoning methods relying on explicit length penalties often introduce optimization conflicts and leave the generative mechanisms driving overthinking largely unexamined. In this paper, we identify a phenomenon termed length shift where models increasingly generate unnecessary reasoning on trivial inputs during training. To address this, we introduce Dynamic Outlier Truncation (DOT), a training-time intervention that selectively suppresses redundant tokens. This method targets only the extreme tail of response lengths within fully correct rollout groups while preserving long-horizon reasoning capabilities for complex problems. To complement this intervention and ensure stable convergence, we further incorporate auxiliary KL regularization and predictive dynamic sampling. Experimental results across multiple model scales demonstrate that our approach significantly pushes the efficiency-performance Pareto frontier outward. Notably, on the AIME-24, our method reduces inference token usage by 78% while simultaneously increasing accuracy compared to the initial policy and surpassing state-of-the-art efficient reasoning methods.

</details>


### [21] [MobileDreamer: Generative Sketch World Model for GUI Agent](https://arxiv.org/abs/2601.04035)
*Yilin Cao,Yufeng Zhong,Zhixiong Zeng,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Wenji Mao,Wan Guanglu*

Main category: cs.AI

TL;DR: MobileDreamer：基于世界模型的移动GUI智能体前瞻框架，通过文本草图世界模型和滚动想象提升长视野任务性能


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体多为反应式，主要依赖当前屏幕信息进行决策，在长视野任务上表现受限。构建世界模型能够预测动作结果并支持更好的决策，但需要平衡空间感知预测效率和实际部署需求。

Method: 提出MobileDreamer框架，包含文本草图世界模型和GUI智能体滚动想象。文本草图世界模型通过学习过程将数字图像转换为关键任务相关草图来预测后动作状态，采用新颖的顺序不变学习策略保留GUI元素空间信息。滚动想象策略利用世界模型预测能力优化动作选择过程。

Result: 在Android World上的实验表明，MobileDreamer达到最先进性能，任务成功率提升5.25%。世界模型评估进一步验证了文本草图建模能够准确预测关键GUI元素。

Conclusion: MobileDreamer通过高效的世界模型前瞻框架显著提升了移动GUI智能体在长视野任务上的性能，文本草图建模和滚动想象策略的结合为GUI自动化提供了有效的解决方案。

Abstract: Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.

</details>


### [22] [ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows](https://arxiv.org/abs/2601.04060)
*Jinwei Su,Qizhen Lan,Zeyu Wang,Yinghui Xia,Hairu Wen,Yiqun Duan,Xi Xiao,Tianyu Shi,Yang Jingsong,Lewei He*

Main category: cs.AI

TL;DR: ComfySearch：一个基于验证引导的智能体框架，用于在ComfyUI平台上探索组件空间并生成功能性的工作流，显著提高执行通过率和解决方案质量


<details>
  <summary>Details</summary>
Motivation: ComfyUI平台上的模块化工作流虽然提供了定制化创意管道的灵活性，但大量组件和严格的图约束导致工作流构建困难，执行通过率低且质量有限

Method: 提出ComfySearch框架，采用智能体方法探索组件空间，通过验证引导的工作流构建机制生成功能性ComfyUI管道

Result: 实验表明ComfySearch在复杂创意任务上显著优于现有方法，实现了更高的执行通过率、解决方案率和更强的泛化能力

Conclusion: ComfySearch有效解决了ComfyUI工作流构建中的挑战，为模块化AI内容生成提供了更可靠和高质量的解决方案

Abstract: AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.

</details>


### [23] [Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions](https://arxiv.org/abs/2601.04170)
*Abhishek Rath*

Main category: cs.AI

TL;DR: 该研究提出了"智能体漂移"概念，即多智能体LLM系统在长期运行中行为、决策质量和协调性逐渐退化的现象，并开发了量化漂移的指标框架和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统在复杂任务分解和协作问题解决方面表现出强大能力，但其长期行为稳定性尚未得到充分研究。需要理解智能体在长时间交互序列中行为退化的现象，以确保生产级AI系统的可靠部署。

Method: 提出了智能体漂移的理论框架，包括语义漂移、协调漂移和行为漂移三种表现形式。开发了智能体稳定性指数（ASI），一个包含12个维度的复合度量框架，用于量化漂移现象。通过仿真分析和理论建模验证漂移效应，并提出三种缓解策略：情景记忆巩固、漂移感知路由协议和自适应行为锚定。

Result: 研究表明，未受控制的智能体漂移会导致任务完成准确率显著下降和人工干预需求增加。理论分析表明，提出的缓解策略能显著减少漂移相关错误，同时保持系统吞吐量。

Conclusion: 该工作建立了监测、测量和缓解生产级智能AI系统中智能体漂移的基础方法论，对企业部署可靠性和AI安全研究具有直接意义。为理解多智能体LLM系统的长期行为稳定性提供了理论框架和实践工具。

Abstract: Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.

</details>
