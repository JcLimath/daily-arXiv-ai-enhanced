<div id=toc></div>

# Table of Contents

- [physics.flu-dyn](#physics.flu-dyn) [Total: 5]
- [math.CV](#math.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 49]
- [cs.RO](#cs.RO) [Total: 18]
- [cs.LG](#cs.LG) [Total: 45]


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [1] [An Investigation into the Applicability of Friction Velocity Estimation Methods for the Channel with A Deposit Body](https://arxiv.org/abs/2601.08163)
*Jing Zhang,Ruihan Qin,Zhixue Guo,Huantao Xu,Jiahui Zhou,Yun Bai*

Main category: physics.flu-dyn

TL;DR: 该研究通过水槽实验探讨沉积体如何通过改变局部流场影响摩擦特性，评估经典均匀流摩擦模型在沉积体河段的适用性，发现流场非均匀性引起的摩擦变化。


<details>
  <summary>Details</summary>
Motivation: 研究沉积体对摩擦特性的影响机制，特别是通过改变局部流场来影响床面剪切应力，这对于理解非均匀流场中的摩擦估计和泥沙输移具有重要意义。

Method: 采用广义水槽实验，首先基于均匀流条件下u*/√(w'²)=0.85~1.15验证经典模型的可靠性，然后将四种模型应用于沉积体河段估算近床摩擦速度，包括湍流动能法(TKE)、垂向湍流动能法(TKE w')和雷诺剪切应力法(RSS)。

Result: 结果显示：纵向速度梯度与TKE法得到的峰值摩擦速度显著相关；摩擦速度主要受湍流强度影响，收缩和狭窄段类似均匀流，扩张段形成峰值；包含流场脉动的模型能有效捕捉非均匀流场对摩擦特性的影响；低能量状态或沉积体比例大时，沉积体阻力比增加，峰值摩擦速度上升。

Conclusion: 该研究为沉积体非均匀流场中的摩擦估计和泥沙输移提供了理论见解，表明考虑流场脉动的模型能更好地表征非均匀流场对摩擦特性的影响。

Abstract: This study investigates how the deposit body influences friction characteristics by altering local flow fields, which is closely related to bed shear stress. Using generalized flume experiments, the study assesses the applicability of classical uniform flow friction models in deposit body river sections, revealing frictional changes induced by flow field non-uniformity. Initially, based on \( \frac{u_*}{\sqrt{\overline{w'^2}}} = 0.85 \sim 1.15 \) under uniform flow conditions as the judgment basis, the reliability of the classical model is verified. Four models are then applied to estimate near-bed friction velocity in deposit body sections. Results show a significant alignment between the longitudinal velocity gradient and the peak friction velocity derived from the turbulent kinetic energy method (TKE). Dimensional analysis of friction indicators reveals that: (a) friction velocity is primarily influenced by turbulence intensity, with constricted and narrowed sections resembling uniform flow, while the expansion section forms a peak; (b) models incorporating flow field fluctuations (TKE, Vertical Turbulence Kinetic Energy (TKE w'), Reynolds shear stress method (RSS)) effectively capture the impact of non-uniform flow fields on friction characteristics; (c) when energy states are low or when deposit body proportions are large, the deposit body's resistance ratio increases, and peak friction velocity rises. This study provides theoretical insights into friction estimation and sediment transport in non-uniform flow fields of deposit bodies.

</details>


### [2] [Lattice Boltzmann methods for simulating non-Newtonian fluids: A comprehensive review](https://arxiv.org/abs/2601.08206)
*Vedad Dzanic,Qiuxiang Huang,Christopher S. From,Emilie Sauret*

Main category: physics.flu-dyn

TL;DR: 这篇综述论文系统回顾了用于模拟非牛顿流体的格子玻尔兹曼方法，重点介绍了处理剪切依赖性粘度、粘塑性和粘弹性的技术，讨论了验证基准案例，并指出了当前模型的局限性和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 非牛顿流体（如番茄酱、血液、牙膏等）具有复杂的非线性流变特性，传统CFD方法难以准确模拟。格子玻尔兹曼方法因其处理复杂边界条件、易于包含多物理场和计算效率高等优势，在非牛顿流体模拟中显示出巨大潜力。自2011年首次综述以来，该方法已取得显著进展，需要系统总结最新技术发展。

Method: 采用文献综述方法，全面梳理用于非牛顿流体模拟的格子玻尔兹曼技术，重点关注三类主要非牛顿特性：剪切依赖性粘度（剪切稀化/增稠）、粘塑性和粘弹性。同时分析验证这些方法的基准测试案例，并探讨在实际复杂流动问题中的应用。

Result: 系统总结了格子玻尔兹曼方法在非牛顿流体模拟中的最新进展，包括各种数值技术的优缺点、适用范围和验证结果。展示了该方法在模拟实际复杂流动问题中的成功应用，同时识别了当前模型存在的挑战和局限性。

Conclusion: 格子玻尔兹曼方法已成为模拟非牛顿流体流动的有效工具，特别是在处理复杂几何和边界条件方面具有优势。虽然已取得显著进展，但仍存在一些未解决的问题需要进一步研究。未来发展方向包括改进数值稳定性、扩展应用范围以及与其他多物理场模型的耦合。

Abstract: Non-Newtonian fluids encompass a large family of fluids with additional nonlinear material properties, contributing to non-trivial flow behaviour that cannot be captured through a single constant viscosity term. Common non-Newtonian characteristics include shear-thinning, shear-thickening, viscoplasticity, and viscoelasticity, commonly encountered in everyday fluids, such as ketchup, blood, toothpaste, mud, etc., as well as practical applications involving porous media, cosmetics, food processing, and pharmaceuticals. Due to the complex nature of these fluids, accurate computational fluid dynamics simulations are essential for predicting their behaviour under various flow conditions. Recent advancements have highlighted the growing trend of using the lattice Boltzmann method to solve such complex flows, owing to its ability to handle intricate boundary conditions, ease of including additional multiphysics, and providing computationally efficient parallel simulations. Since the initial review over a decade ago [Phillips & Roberts, IMA J. Appl. Math. 76, 790-816 (2011)], significant advancements have been made to the lattice Boltzmann method to simulate non-Newtonian fluids. Here, we present a comprehensive review of different lattice Boltzmann techniques used to solve non-Newtonian fluid systems, specifically dealing with shear-dependent viscosity, viscoplasticity, and viscoelasticity. In addition, we discuss various benchmark cases that validate these approaches and highlight their growing application to realistic and challenging complex flow problems. We further address outstanding issues in current lattice Boltzmann models, as well as future directions for numerical advancement and application.

</details>


### [3] [Bridging Elastic and Active Turbulence](https://arxiv.org/abs/2601.08296)
*Vedad Dzanica,Sumesh P. Thampi,Julia M. Yeomans*

Main category: physics.flu-dyn

TL;DR: 该论文揭示了聚合物溶液弹性湍流与活性物质主动湍流之间的深刻联系，指出两者在宏观连续描述上具有相似性，聚合物流体可视为可变形收缩活性物质的类似物。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索弹性湍流（聚合物溶液）与主动湍流（自驱动活性粒子）之间未被认识的联系。尽管这两种湍流现象来源不同，但都出现在微观结构存在的情况下，研究者希望揭示它们之间的深层相似性。

Method: 研究方法包括理论分析和数值模拟。通过对比两种湍流的连续介质描述，建立类比关系；对Kolmogorov流进行数值模拟，分析聚合物取向场中拓扑缺陷的出现；研究横向不稳定性机制和流动抑制状态的转变。

Result: 研究结果显示：1）弹性湍流与主动湍流的宏观连续描述具有相似性；2）弹性湍流中旅行箭头结构的形成伴随着聚合物取向场中±1/2拓扑缺陷的出现；3）这些相干结构源于各向异性拉伸收缩聚合物产生的类活性梯度驱动的横向不稳定性；4）在足够强的活性下，系统会转变为流动抑制状态，表现为弱聚合物拉伸和有序化。

Conclusion: 该研究建立了弹性湍流与主动湍流之间的深刻联系，表明聚合物流体可视为可变形收缩活性物质的类似物。弹性湍流中的拓扑缺陷形成机制与主动湍流相似，且系统存在向流动抑制状态的转变，这与受限活性向列相中的自发流动转变类似。

Abstract: Remarkably, even under negligible inertia, the addition of microstructural agents can generate chaotic flow fields. Such behavior can arise in polymer solutions, leading to elastic turbulence, or from active, self-driven particles, which generate active turbulence. Here, we demonstrate a close and hitherto unrecognized connection between these two classes of turbulence. Specifically, we reveal that their continuum descriptions are analogous at the macroscopic level, such that polymeric fluids can be interpreted as a deformable analogue of contractile active matter. Moreover, our numerical results for Kolmogorov flow demonstrate that the transition into the well-known traveling arrowhead structures in elastic turbulence is marked by the emergence of $\pm 1/2$ topological defects, long recognized as a defining feature of active turbulence, in the polymer director field. Importantly, these coherent structures originate from a transverse instability driven by activity-like gradients generated by anisotropically stretched, contractile polymers. At sufficiently strong activity, the system undergoes a transition into a flow-suppressed state characterized by weak polymer stretching and ordering, a behavior that can be explained by analogy with the spontaneous-flow transition observed in channel-confined active nematics.

</details>


### [4] [Turbulent spots in hypersonic transitional planar and axisymmetric boundary layers](https://arxiv.org/abs/2601.08795)
*Ankit Bajpai,Jagadeesh Gopalan*

Main category: physics.flu-dyn

TL;DR: 对比研究高超音速（马赫数5.85）环境下平板和轴对称锥体边界层中湍流斑的特性，包括前/后缘速度、流向长度尺度及生成率。


<details>
  <summary>Details</summary>
Motivation: 研究高超音速环境下不同几何形状（平板与轴对称锥体）对过渡边界层中湍流斑特性的影响，为高超音速飞行器边界层转捩预测提供实验数据。

Method: 在相似高超音速自由流环境（马赫数5.85，雷诺数3.0-6.0×10^6/m）下，对平板和轴对称锥体模型进行实验。通过表面热传递测量确定边界层状态并计算转捩边界层的间歇因子。测量湍流斑的前/后缘速度、流向长度尺度和生成率。

Result: 1. 两种模型湍流斑前缘速度均为边界层边缘速度的90%；2. 平板边界层湍流斑后缘速度低于轴对称边界层；3. 平板边界层湍流斑流向长度尺度增长率高于轴对称边界层；4. 两种边界层湍流斑生成率均在1000000-3000000 spots/m/s范围内。

Conclusion: 几何形状显著影响高超音速过渡边界层中湍流斑的特性：轴对称边界层湍流斑后缘速度更高、长度尺度增长率更低，但前缘速度和生成率范围相似。这些差异对高超音速飞行器边界层转捩建模具有重要意义。

Abstract: Experiments were conducted to investigate characteristics of turbulent spots formed in transitional boundary layers developed over a flat plate and an axisymmetric cone placed in similar hypersonic freestream environment of Mach number 5.85. The freestream Reynolds number in the present work varied between $3.0-6.0\times10^6$/m. Heat transfer measurement along the surface of both the test models was used to ascertain the state of boundary layer and to calculate the intermittency associated with transitional boundary layer. Turbulent spots generated in the transitional boundary layer were characterized in terms of their leading-trailing edge velocities, their streamwise length scales and their generation rates on both the test models. Leading edge of the turbulent spots developed over both the test models were found to be convecting at a speed equivalent to 90\% of the boundary layer edge speed. The trailing edge of the spots developed on a planar boundary layer traversed at a lower speed than its axisymmetric counterpart. Streamwise length scales of a turbulent spot developed in a planar boundary layer grew at a higher rate when compared with axisymmetric boundary layer. Turbulent spot generation rates for both planar and axisymmetric boundary layers was found to be in the range of $1000000-3000000$ spots/m/s.

</details>


### [5] [Collapse of statistical equilibrium in large-scale hydroelastic turbulent waves](https://arxiv.org/abs/2601.08799)
*Marlone Vernet,Eric Falcon*

Main category: physics.flu-dyn

TL;DR: 实验研究大尺度水弹性湍流波从统计平衡态自由衰减的过程，发现总能量随时间呈幂律衰减，理论预测与实验数据吻合良好。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡湍流系统在统计平衡态（能量按大尺度模式均分）后的非稳态演化过程，特别是自由衰减行为。目前关于这种平衡态如何出现、衰减、崩溃或其他非稳态演化的关键问题尚未解决。

Method: 实验研究大尺度水弹性湍流波的自由衰减过程。使用空间和时间分辨测量技术，从初始统计平衡态开始观测衰减过程。基于理论初始平衡谱和线性粘性阻尼推导能量衰减定律（无净能量通量）。

Result: 大尺度张力波的总能量随时间呈幂律衰减。理论预测与实验数据在近两个时间量级内吻合良好，适用于统计平衡态的各种初始有效温度。实验确认了耗散机制。

Conclusion: 成功建立了从统计平衡态衰减的理论框架，该方法可推广到其他初始大尺度处于统计平衡态的衰减湍流系统。

Abstract: At scales larger than the forcing scale, some out-of-equilibrium turbulent systems (such as hydrodynamic turbulence, wave turbulence, and nonlinear optics) exhibit a state of statistical equilibrium where energy is equipartitioned among large-scale modes, in line with the Rayleigh-Jeans spectrum. Key open questions now pertain to either the emergence, decay, collapse, or other nonstationary evolutions from this state. Here, we experimentally investigate the free decay of large-scale hydroelastic turbulent waves, initially in a regime of statistical equilibrium. Using space- and time-resolved measurements, we show that the total energy of these large-scale tensional waves decays as a power law in time. We derive an energy decay law from the theoretical initial equilibrium spectrum and the linear viscous damping, as no net energy flux is carried. Our prediction then shows a good agreement with experimental data over nearly two decades in time, for various initial effective temperatures of the statistical equilibrium state. We further identify the dissipation mechanism and confirm it experimentally. Our approach could be applied to other decaying turbulence systems, with the large scales initially in statistical equilibrium.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [6] [Geometric subfamily of locally univalent functions, Blaschke products and quasidisk](https://arxiv.org/abs/2601.07842)
*Molla Basir Ahamed,Rajesh Hossain*

Main category: math.CV

TL;DR: 该论文研究了定义在单位圆盘上的函数族$\mathcal{F}(α)$，该族满足特定微分不等式，并探讨了其几何与分析性质，包括与Blaschke乘积、Schwarzian导数的联系，以及准圆性、范数估计等结果。


<details>
  <summary>Details</summary>
Motivation: 研究满足微分不等式${\rm Re}(1+zf''(z)/f'(z)) > 1 - α/2$的函数族$\mathcal{F}(α)$的几何与分析性质，建立该函数族与准圆、Schwarzian导数等概念的深刻联系。

Method: 通过复分析技术研究函数族$\mathcal{F}(α)$的性质，包括使用微分不等式、Schwarzian导数、准圆理论等方法，并扩展到调和映射情形，考虑其解析部分属于$\mathcal{F}(α)$的情况。

Result: 证明了若$f \in \mathcal{F}(α)$，则$f(\mathbb{D})$是准圆；建立了Schwarzian导数的范数等式$\|S_f\| = 2α(2-α)$；对于调和映射$f = h + \bar{g} \in \mathcal{F}_{\mathcal{H}}(α)$，给出了pre-Schwarzian导数的尖锐估计$\|P_{f}\| \leq 2α+1$。

Conclusion: 函数族$\mathcal{F}(α)$具有丰富的几何与分析性质，包括准圆性、与Blaschke乘积的联系、Schwarzian导数的精确范数估计，以及调和映射情形的尖锐不等式，这些结果深化了对这类函数族的理解。

Abstract: In this article, we consider the family $\mathcal{F}(α)$ defined for $α\in (0, 3]$ by
  \begin{align*}
  {\rm Re}\left(1+\frac{zf''(z)}{f'(z)}\right) > 1 - \fracα{2} \quad \text{for } z \in \mathbb{D}.
  \end{align*}
  Our primary objective is to show that this family possesses significant geometric and analytic properties, including connections with Blaschke products and the Schwarzian derivative, as well as its sharp bounds. Furthermore, we prove that if $f \in \mathcal{F}(α)$, then the image $f(\mathbb{D})$ is a quasidisk. We also show that if $f \in \mathcal{F}(α)$, then $\|S_f\| = 2α(2-α)$. Moreover, we establish the sharp estimate $\|P_{f}\| \leq 2α+1$ for the pre-Schwarzian derivative of harmonic mappings $f = h + \bar{g} \in \mathcal{F}_{\mathcal{H}}(α)$, where the analytic part $h$ belongs to $\mathcal{F}(α)$.

</details>


### [7] [On octonionic Monge-Ampère equation and pluripotential theory associated to octonionic plurisubharmonic functions of two variables](https://arxiv.org/abs/2601.08437)
*Wei Wang*

Main category: math.CV

TL;DR: 该论文将复势理论推广到八元数多元次调和函数，建立了比较原理、拟连续性、相对极值函数和容量理论，并解决了单位球上齐次八元数Monge-Ampère方程的Dirichlet问题。


<details>
  <summary>Details</summary>
Motivation: 将经典的复势理论推广到八元数情形，克服八元数非结合性带来的困难，建立八元数多元次调和函数的系统理论框架。

Method: 利用八元数的弱结合性形式，建立混合八元数Monge-Ampère算子的分部积分公式；通过加权变换公式解决OPSH函数在单位球自同构下的不变性问题；应用Bedford-Taylor方法处理正则性问题。

Result: 证明了连续OPSH函数的比较原理和局部有界OPSH函数的拟连续性；建立了八元数相对极值函数和容量的基本性质；解决了单位球上齐次八元数Monge-Ampère方程的Dirichlet问题，并证明了解的C^{1,1}_{loc}正则性。

Conclusion: 成功将复势理论的核心结果推广到八元数多元次调和函数，克服了非结合性的主要困难，为八元数几何分析建立了理论基础。

Abstract: Several aspects of pluripotential theory are generalized to octonionic plurisubharmonic (OPSH) functions of two variables. We prove the comparison principle for continuous OPSH functions and the quasicontinuity of locally bounded ones. An important tool is a formula of integration by parts for mixed octonionic Monge-Ampère operator. Various useful properties of octonionic relative extremal functions and octonionic capacity are established. The main difficulty is the non-associativity of octonions. However, some weak form of associativity can be used to covercome this difficulty. Another important ingredient in pluripotential theory is the solution to the Dirichlet problem for the homogeneous octonionic Monge-Ampère equation on the unit ball, for which we show the $C_{loc}^{1,1}$-regularity by applying Bedford-Taylor's method. The obstacle to do so is that an OPSH function is usually not OPSH under automorphisms of the unit ball. This issue can be solved by finding a weighted transformation formula of OPSH functions.

</details>


### [8] [Examples of critically cyclic functions in the Dirichlet spaces of the ball](https://arxiv.org/abs/2601.08651)
*Pouriya Torkinejad Ziarati*

Main category: math.CV

TL;DR: 构造了在二重球上Dirichlet空间中的全纯函数，存在临界指数α_c使得函数在D_α中循环当且仅当α≤α_c


<details>
  <summary>Details</summary>
Motivation: 研究Dirichlet空间中的循环性（cyclicity）问题，探索函数在何种参数条件下成为循环向量，特别是临界指数的存在性

Method: 利用Bruna、Ortega、Chaumat和Chollet研究的平滑球代数中的插值集（interpolation sets）概念来构造具体例子

Result: 成功构造了在二重球Dirichlet空间D_2(B_2)中的全纯函数，存在临界指数α_c∈[1/2,2]，使得函数在D_α(B_2)中循环当且仅当α≤α_c

Conclusion: 证明了Dirichlet空间中存在具有临界循环指数的函数，为理解循环性随参数变化提供了具体例证

Abstract: In this work, we construct examples of holomorphic functions in $D_2(\B_2)$, the Dirichlet space on $\B_2$, for which there exists an index $α_c \in [\frac12,2]$ such that the function is cyclic in $D_α(\B_2)$ if and only if $α\leq α_c$. To this end, we use the notion of \emph{interpolation sets} in smooth ball algebras, as studied by Bruna, Ortega, Chaumat, and Chollet.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: 提出混合可解释AI框架，结合前向模糊逻辑与后向SHAP解释，用于资源受限环境下的孕产妇健康风险预测，通过临床医生验证证明能提升可解释性和信任度。


<details>
  <summary>Details</summary>
Motivation: 机器学习在孕产妇健康风险预测中虽有潜力，但在资源受限的临床环境中面临关键障碍：缺乏可解释性和信任度，阻碍临床采用。

Method: 开发混合可解释AI框架，结合前向模糊逻辑规则（ante-hoc）与后向SHAP特征重要性解释（post-hoc）。使用模糊-XGBoost模型在1,014份孕产妇健康记录上训练，并通过14名孟加拉国医疗专业人员的系统性临床反馈进行验证。

Result: 模型达到88.67%准确率（ROC-AUC: 0.9703）。验证研究中71.4%的临床医生偏好混合解释，54.8%表示信任可用于临床。SHAP分析确定医疗可及性为主要预测因子，工程化模糊风险评分排名第三，验证了临床知识整合（r=0.298）。

Conclusion: 结合可解释模糊规则与特征重要性解释能同时提升实用性和信任度，为孕产妇医疗保健中的可解释AI部署提供实用见解。临床医生识别出关键缺失参数：产科史、孕龄和连接障碍。

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [10] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 提出一种无需训练、通用的模型校准方法，利用置信度识别模型未知情况，应用于模型级联和数据清洗，提高AI效率与可靠性


<details>
  <summary>Details</summary>
Motivation: 当模型能够识别自身未知时，会开启多种可能性。关键问题是如何让模型认识到自己不知道。利用模型内部信号计算置信度来反映其无知是一种有前景的方法。先前特定领域研究表明校准可以提供可靠的置信度估计。

Method: 提出一种简单、有效、无需训练的通用方法，适用于视觉和语言模型。方法包括：1) 模型校准；2) 基于校准优势路由的模型级联；3) 基于模型集成的数据清洗。首先建立两个关键经验观察：单个模型内置信度越高对应准确率越高；在验证集上校准的模型在测试集上保持校准。基于校准置信度的可比性，设计级联路由信号，将大模型与小模型级联以提高效率，或将两个规模相当的模型级联以超越任一单独模型。利用多个专家及其校准置信度，设计平衡精度和检测率的数据清洗方法。

Result: 方法在ImageNet和MMLU数据集上验证：1) 模型级联在几乎不损失准确率的情况下显著提高效率；2) 两个规模相当模型的级联能超越任一单独模型；3) 数据清洗方法能有效识别错误标注样本。结果证明让模型识别未知是实现更高效、可靠、可信AI的实用步骤。

Conclusion: 通过模型校准、级联和数据清洗，使模型能够识别自身未知是迈向更高效、可靠、可信AI的重要实践。提出的无需训练通用方法在视觉和语言任务中均有效，为利用模型置信度信息提供了实用框架。

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [11] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 论文提出CADA方法，通过案例增强的审慎对齐改进LLM安全性，避免过度拒绝良性请求，相比详细安全代码规则方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，OpenAI的审慎对齐(DA)使用详细"类代码"安全规则，但在开源LLM中效果不佳，且详细规则会降低模型的有用性，需要更鲁棒的安全对齐方法。

Method: 提出CADA方法：案例增强的审慎对齐，使用强化学习训练模型生成安全推理链，结合简单代码和案例示范，而非仅依赖详细规则枚举。

Result: CADA有效提升无害性，增强对抗攻击的鲁棒性，减少过度拒绝，同时在多样化基准测试中保持实用性，优于仅基于规则的DA方法。

Conclusion: 案例增强推理比详细安全代码规则更有效，CADA为改进LLM安全性同时保持有用性提供了实用替代方案，实现更广泛适应性。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [12] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: 该论文分析了2025年美国与欧盟前沿AI法规在内部部署系统监管方面的三大漏洞：范围模糊性、时点合规评估、信息不对称，并探讨了解决这些问题的可能途径与权衡。


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI监管主要关注面向外部用户的系统部署，但企业内部部署的高风险AI系统（如自动化研发、关键业务流程处理、敏感数据处理）可能逃避监管，存在监管盲区。

Method: 通过分析2025年美国与欧盟的前沿AI法规，识别内部部署系统的监管漏洞，分析漏洞持续存在的原因（可测量性、激励、信息获取等矛盾），并绘制可能的解决方案路径及其权衡。

Result: 识别出三大监管漏洞：1) 范围模糊性使内部系统逃避监管义务；2) 时点合规评估无法捕捉内部系统的持续演化；3) 信息不对称削弱监管意识与监督。这些漏洞源于可测量性、激励结构和信息获取方面的固有矛盾。

Conclusion: 需要有针对性的政策选择来解决内部部署AI系统的监管漏洞，而非偶然性地处理。理解这些模式有助于政策制定者更审慎地设计针对企业内部AI部署的监管框架。

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [13] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: SCASED是一个基于物联网的智能教室系统，集成了自动考勤和面部情绪识别功能，用于监测学生课堂参与度，通过Raspberry Pi摄像头和MobileNetV2模型实现89.5%的情绪分类准确率。


<details>
  <summary>Details</summary>
Motivation: 当前高等教育中智能教室技术主要关注自动考勤，对学生课堂情感和认知参与度关注不足，限制了教师识别学生脱离状态和实时调整教学策略的能力。

Method: 系统采用Raspberry Pi摄像头和OpenCV进行人脸检测，使用微调的MobileNetV2模型分类四种学习相关情绪状态（参与、无聊、困惑、挫折），实现基于会话的考勤管理机制（每节课记录一次考勤，之后进行连续情绪分析），并通过云端仪表板可视化数据。

Result: 使用DAiSEE数据集进行实验评估，情绪分类准确率达到89.5%。结果表明，将考勤数据与情绪分析相结合可以为教师提供更多课堂动态洞察，支持更及时响应的教学实践。

Conclusion: SCASED系统通过集成自动考勤和情绪识别功能，为教师提供了更全面的课堂参与度监测工具，有助于改善教学响应性和课堂动态理解。

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [14] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: 本文提出了一种用于奶牛场能源管理的深度强化学习框架，通过改进PPO算法实现电池储能和热水系统的智能调度，在动态电价环境下降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 奶牛养殖是能源密集型行业，严重依赖电网供电。随着可再生能源的集成增加，可持续能源管理对于减少电网依赖和支持联合国可持续发展目标7（可负担的清洁能源）变得至关重要。然而，可再生能源的间歇性给实时供需平衡带来了挑战。现有强化学习方法通常假设完全了解未来电价或发电量，这在动态环境中不现实，且标准PPO变体依赖固定的裁剪或KL散度阈值，在可变电价下常导致训练不稳定。

Method: 本研究提出了一个用于奶牛场高效负荷调度的深度强化学习框架，重点关注电池储能和热水系统在现实运营约束下的管理。提出了两种改进的PPO变体：1）Forecast Aware PPO，通过基于小时和月份的残差校准方法整合短期需求和可再生能源发电预测；2）PID KL PPO，采用比例-积分-微分控制器自适应调节KL散度，实现稳定的策略更新。方法在真实奶牛场数据上进行训练。

Result: 该方法在真实奶牛场数据上训练，相比标准PPO实现了1%的更低电力成本，相比DQN降低4.8%，相比SAC降低1.5%。在电池调度方面，PPO减少了13.1%的电网输入，展示了在现代奶牛养殖中可持续能源管理的可扩展性和有效性。

Conclusion: 提出的深度强化学习框架通过整合短期预测和自适应KL散度调节，有效解决了奶牛场能源管理中的挑战。该方法在降低运营成本和减少电网依赖方面表现出色，为现代奶牛养殖的可持续能源管理提供了有效的解决方案。

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [15] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: 提出新的后向可达集计算算法，并与前向分析结合，构建神经反馈系统的统一验证框架


<details>
  <summary>Details</summary>
Motivation: 前向可达性分析在神经反馈系统可达-避免属性验证中占主导地位，但现有后向可达性方法可扩展性有限，需要改进后向分析方法

Method: 引入新算法计算后向可达集的过近似和欠近似，并将这些后向算法与现有前向分析技术集成

Result: 开发出能够计算后向可达集近似的新算法，并构建了统一的验证框架

Conclusion: 通过结合前向和后向可达性分析，为神经反馈系统提供了更全面的验证方法，克服了现有后向方法的可扩展性限制

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [16] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: 该论文首次对大型语言模型中的负面指令失败进行了全面的机制研究，发现违反概率与语义压力呈紧密逻辑关系，并揭示了两种不同的失败模式


<details>
  <summary>Details</summary>
Motivation: 负面约束（"不要使用词X"形式的指令）是测试大型语言模型指令遵循能力的基本测试。尽管看似简单，但这些约束经常失败，且失败条件一直未被充分理解。论文旨在首次全面研究负面指令失败的机制原因

Method: 引入语义压力作为模型生成被禁令牌内在概率的量化指标；使用logit lens技术进行分层分析；通过激活修补技术确定因果责任层；分析两种失败模式：启动失败和覆盖失败

Result: 违反概率与语义压力呈紧密逻辑关系(p=σ(-2.40+2.27·P₀))；负面指令在失败中抑制信号较弱（失败中目标概率仅降低5.2个百分点，成功中降低22.8个百分点）；识别出两种失败模式：启动失败（87.5%违反）和覆盖失败（12.5%违反）；确定层23-27对约束效果具有因果责任

Conclusion: 负面约束设计存在根本性矛盾：命名被禁词的行为反而会启动模型生成该词。这揭示了大型语言模型在遵循负面指令时的系统性弱点，为改进指令设计和模型训练提供了机制性见解

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [17] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain是一个用于工具增强型智能体的执行记忆模型，通过构建依赖感知的记忆来管理长时程推理中的中间状态和逻辑关系，解决上下文累积问题。


<details>
  <summary>Details</summary>
Motivation: 工具增强型智能体框架中的复杂推理本质上是长时程的，导致推理轨迹和临时工具产物不断累积，超出大型语言模型的有限工作上下文容量。缺乏显式记忆机制会破坏逻辑连续性并削弱任务对齐，因此记忆不是辅助效率问题，而是维持长时程连贯、目标导向推理的核心组件。

Method: 提出MemoBrain执行记忆模型，为工具增强型智能体构建依赖感知的记忆，捕获关键的中间状态及其逻辑关系。作为推理智能体的协同伙伴运行，在不阻塞执行的情况下组织推理进度，并主动管理工作上下文。具体机制包括：修剪无效步骤、折叠完成的子轨迹、在固定上下文预算下保持紧凑的高显著性推理主干。

Result: 在具有挑战性的长时程基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，相比强基线模型展现出持续的性能改进。

Conclusion: MemoBrain通过显式认知控制推理轨迹而非被动上下文累积，为工具增强型智能体提供了有效的记忆管理机制，解决了长时程推理中的上下文过载问题，实现了更连贯、目标导向的推理过程。

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [18] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: 论文研究了事故后车道变换行为，构建了首个事故后LC数据集，发现其具有更长持续时间、更低插入速度和更高碰撞风险的特点，并开发了基于图注意力机制的新型轨迹预测框架。


<details>
  <summary>Details</summary>
Motivation: 事故发生后，后续车辆需要变换车道绕过障碍物，但目标车道车辆可能拒绝让行，增加了事故后LC的复杂性和碰撞风险。然而，事故后LC的行为特征和运动模式尚不清楚，需要填补这一研究空白。

Method: 1) 从无人机视频中提取车辆轨迹构建事故后LC数据集；2) 通过实证分析比较事故后LC与强制LC、自由LC的差异；3) 开发基于图注意力机制的轨迹预测框架，将让行行为建模为辅助交互感知任务，指导条件变分自编码器和Transformer解码器预测车道变换者轨迹。

Result: 1) 事故后LC相比MLC和DLC具有更长持续时间、更低插入速度和更高碰撞风险；2) 79.4%的事故后LC涉及新跟驰者至少一次不让行行为，远高于DLC(21.7%)和MLC(28.6%)；3) 提出的模型在不同预测时间范围内，平均位移误差和最终位移误差均比现有基线提升超过10%；4) 模型能提供更可靠的碰撞风险分析，降低误报率并提高冲突预测准确性；5) 模型在不同地点收集的额外数据集上验证了可迁移性。

Conclusion: 事故后LC具有独特的风险特征，特别是高频率的不让行行为。提出的交互感知轨迹预测框架能有效捕捉这些特征，显著提升预测性能，并为事故后交通管理提供更可靠的碰撞风险评估工具。

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [19] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: 提出一种在边缘设备上运行的AI伴侣内存系统，通过活动/非活动阶段交替的内存范式，在用户活跃时进行低延迟实时对话，在用户不活跃时进行内存提取和整合，以在有限计算资源下实现低延迟和长期个性化。


<details>
  <summary>Details</summary>
Motivation: 边缘设备的计算资源限制使得开发完全嵌入式的AI伴侣系统面临挑战，现有AI伴侣和内存系统因计算资源不足和延迟问题无法直接用于此类环境。

Method: 提出交替活动/非活动阶段的内存范式：用户活跃阶段使用轻量级检索进行低延迟实时对话；用户不活跃阶段进行计算密集的内存提取、整合和维护。同时引入AI伴侣基准测试，全面评估对话质量和内存能力。

Result: 实验表明，使用非常弱的模型（Qwen2.5-7B-Instruct量化int4）的系统在大多数指标上优于无内存的等效原始LLM，并与具有16k上下文窗口的GPT-3.5表现相当。

Conclusion: 提出的内存范式能够在嵌入式硬件严格约束下最小化延迟，同时保持长期个性化，为边缘设备上的AI伴侣系统提供了可行的解决方案。

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [20] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse是一个用于自主解决最后一公里配送中断的新型智能体框架，采用分层多智能体架构，使用LangGraph编排复杂工作流，并通过真实场景数据集和LLM评估协议进行验证。


<details>
  <summary>Details</summary>
Motivation: 解决最后一公里配送中的复杂中断问题，传统方法难以处理动态变化的现实场景，需要自主、智能的解决方案来提升配送效率和客户满意度。

Method: 采用分层多智能体架构：中央Resolution Supervisor负责战略任务分解，将子任务委托给专门的worker agents进行战术执行。使用LangGraph编排复杂、循环的工作流程。通过分析6000多条真实用户评论，构建了30个复杂中断场景的基准数据集。采用LLM-as-a-Judge评估协议，并包含明确的偏见缓解机制。

Result: 开发了完整的Project Synapse框架，能够自主处理最后一公里配送中断。创建了包含30个复杂场景的基准数据集，并建立了基于LLM的评估协议来验证系统性能。

Conclusion: Project Synapse为最后一公里配送中断提供了有效的自主解决方案，其分层多智能体架构和LangGraph编排能够处理复杂工作流，通过真实场景数据集和偏见缓解的评估协议验证了框架的有效性。

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [21] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核平台的温度和能量感知调度，结合LLM语义特征提取实现零样本部署，显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法存在局限性：基于利用率的启发式方法忽略停顿时间，基于离线分析的方法需要大量配置表且无法运行时适应。需要一种能够动态适应、准确预测热力学和性能状态、支持新工作负载零样本部署的调度方案。

Method: 1) 分层多智能体强化学习框架：两个协作智能体分解指数级动作空间，降低决策延迟至358ms；2) 准确环境模型：使用回归技术预测热力学和性能状态；3) LLM语义特征提取：从OpenMP程序提取13个代码级特征，无需执行；4) Dyna-Q启发框架：结合直接强化学习和基于模型的规划，加速收敛20倍；5) 零样本部署：利用LLM特征和环境模型生成合成训练数据，无需特定工作负载分析样本。

Result: 1) 决策延迟：后续决策358ms，首次决策3.5-8.0s（含一次性LLM特征提取）；2) 收敛速度：比无模型方法快20倍；3) 性能提升：比Linux ondemand governor能效高7.09倍，makespan好4.0倍；4) 首次决策延迟：比基于表的分析方法快8,300倍；5) 平台验证：在NVIDIA Jetson TX2、Jetson Orin NX、RubikPi和Intel Core i7上通过BOTS和PolybenchC基准测试验证。

Conclusion: 提出的基于模型的分层MARL框架结合LLM语义特征提取，实现了高效的热能和能量感知调度，支持新工作负载零样本部署，决策延迟显著降低，适用于动态嵌入式系统的实际部署。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [22] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: HS2C框架利用图同质性进行结构和语义压缩，通过最小化结构熵进行全局层次划分识别同质社区，将冗余背景压缩为社区级共识，提升LLMs在图理解任务中的推理性能和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于上下文窗口，通常采用随机采样丢弃节点/边，这会引入噪声并导致推理不稳定。作者认为图本身包含丰富的结构和语义信息，有效利用这些信息可以释放LLMs在图理解任务中的性能潜力。

Method: 提出HS2C框架：1) 结构压缩：基于结构熵最小化原则进行全局层次划分，识别自然凝聚的同质社区，丢弃随机连接噪声；2) 语义压缩：将检测到的结构同质性传递给LLM，使其能够基于预定义社区类型进行差异化的语义聚合，将冗余背景上下文压缩为简洁的社区级共识，选择性保留与目标节点对齐的语义同质信息。

Result: 在10个节点级基准测试中，使用不同规模和家族的LLMs进行实验，HS2C通过向LLMs提供结构和语义压缩的输入，同时提高了压缩率和下游推理准确率，验证了其优越性和可扩展性。在7个不同的图级基准测试中的扩展进一步巩固了HS2C的任务泛化能力。

Conclusion: HS2C框架通过利用图同质性进行结构和语义压缩，有效解决了现有方法因随机采样引入噪声的问题，显著提升了LLMs在图理解任务中的推理性能和效率，展现了良好的可扩展性和任务泛化能力。

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [23] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 该研究利用世界冠军AI进行自对弈统计分析，发现国际麻将在线单局游戏存在先手优势和子目标计分问题，提出引入补偿分机制和优化牌型计分规则，使传统麻将更适合在线环境。


<details>
  <summary>Details</summary>
Motivation: 国际麻将作为全球流行的传统游戏，在线化后玩家游戏时间碎片化、对手组合不固定，与传统线下固定对手多轮对局不同。现有规则需要调整以确保在线单局游戏的公平性。

Method: 使用世界冠军AI进行自对弈比赛，通过统计分析揭示游戏平衡问题。基于发现的问题，提出规则调整方案，包括引入先手优势补偿分机制和优化不同牌型子目标计分。

Result: 研究发现在线单局游戏存在明显的先手优势问题，以及子目标计分设置不合理。提出的补偿分机制比传统多轮轮换位置方法更适合在线玩家，并实现了修订版在线麻将游戏。

Conclusion: 这是首次利用AI系统数据评估国际麻将游戏平衡性，并开发出更适合在线玩家的修订版本。提出的规则调整使传统游戏更好地适应在线环境，为在线麻将提供了更公平的游戏体验。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [24] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: SANC(E3)是一个理论框架，提出表征单元不是先验给定的，而是在有限激活容量下通过竞争选择、重建和压缩的稳定结果而涌现，受E3能量泛函最小化支配。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统预设固定的原始单元（如token、像素等），绕过了表征单元如何涌现和稳定这一根本问题。本文旨在提出一个理论框架，解释表征单元如何通过自组织过程从经验中形成。

Method: 提出SANC(E3)公理化框架，包含五个核心公理：有限容量、共现关联、相似性竞争、置信度稳定化、重建-压缩-更新权衡。采用伪内存映射I/O机制，使内部回放的格式塔与外部感官输入通过相同公理路径处理。

Result: 从公理推导出12个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以理解为E3最小化下的格式塔完成过程。实现了感知、想象、预测、规划和行动在单一表征和能量过程中的统一。

Conclusion: SANC(E3)为理解智能系统如何从经验中自组织表征单元提供了理论基础，将高级认知功能统一到格式塔完成的基本机制中，为构建更接近人类智能的AI系统提供了新方向。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [25] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench：首个多模态成对上下文完整性基准，用于评估智能体隐私行为，涵盖视觉-文本多模态隐私风险，通过三层评估揭示模型在隐私-效用权衡中的系统性失败


<details>
  <summary>Details</summary>
Motivation: 随着语言模型从被动聊天机器人演变为处理个人数据的主动助手，评估其遵守社会规范（特别是上下文完整性CI）变得至关重要。现有CI基准主要是文本中心且侧重于负面拒绝场景，忽略了多模态隐私风险和隐私与效用的基本权衡。

Method: 提出MPCI-Bench多模态成对上下文完整性基准，包含从相同视觉源派生的正负实例对，分为三个层级：规范性种子判断、上下文丰富的故事推理、可执行的智能体行为轨迹。通过三原则迭代精炼管道确保数据质量。

Result: 对最先进多模态模型的评估显示：1）模型在平衡隐私和效用方面存在系统性失败；2）存在显著的模态泄漏差距，敏感视觉信息比文本信息泄漏更频繁。

Conclusion: MPCI-Bench填补了多模态隐私评估的空白，揭示了当前模型在隐私-效用权衡和模态泄漏方面的缺陷，将开源以促进智能体上下文完整性的未来研究。

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [26] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 论文提出从传统的手工设计数值奖励函数转向基于语言的目标规范，利用大语言模型从自然语言描述合成奖励函数，并探讨了在多智能体强化学习中通过共享语义表示而非显式工程化数值信号实现协调的研究方向。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的奖励工程面临信用分配模糊性、环境非平稳性和交互复杂性组合增长等挑战。传统手工设计数值奖励函数的方法难以应对这些挑战，而大语言模型的发展为语言介导的目标规范提供了新的可能性。

Method: 提出从三个维度实现转变：语义奖励规范（使用自然语言描述目标）、动态奖励适应（在线调整奖励函数）以及改进与人类意图的对齐。借鉴EUREKA和CARD等现有工作，利用LLM从自然语言描述合成奖励函数，并采用Reinforcement Learning from Verifiable Rewards范式。

Result: 语言介导的监督可以作为传统奖励工程的可行替代方案。通过共享语义表示而非显式工程化数值信号，有望在多智能体系统中实现更好的协调。但存在计算开销、幻觉鲁棒性和大规模系统可扩展性等开放挑战。

Conclusion: 论文展望了一个研究方向：在多智能体强化学习中，协调应源于共享的语义表示而非显式工程化的数值信号。这代表了从手工奖励工程向语言介导目标规范的范式转变，但需要解决计算效率、鲁棒性和可扩展性等关键挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [27] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个诊断性基准测试，用于评估LLM在Pearl因果阶梯上的因果判断能力，包含454个专家策划的小故事，通过效用、安全性和明智拒绝三个维度分析模型表现，揭示了前沿模型存在的怀疑陷阱和规模悖论等病理现象。


<details>
  <summary>Details</summary>
Motivation: 需要系统评估大型语言模型在因果推理方面的能力，特别是在Pearl因果阶梯不同层级上的表现，以诊断模型在因果判断中存在的系统性缺陷和病理现象。

Method: 开发了T3基准测试，包含454个专家策划的因果推理小故事，将性能分解为效用（敏感性）、安全性（特异性）和对不确定情况的明智拒绝三个维度，应用于前沿模型进行诊断分析。

Result: 发现了两种病理现象：在L1层级的"怀疑陷阱"（安全调优模型如Claude Haiku拒绝60%的有效链接），以及在L3层级的非单调"规模悖论"（更大的GPT-5.2在模糊反事实问题上比GPT-4-Turbo低55分，主要因过度犹豫而非幻觉）。验证了过程验证协议RCA能够恢复决定性因果判断。

Conclusion: T3基准测试能够有效诊断LLM在因果推理中的系统性缺陷，揭示了安全调优和模型规模扩大可能带来的非预期后果，为改进模型因果判断能力提供了有价值的诊断工具。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [28] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 论文研究大规模动作空间中的智能体系统，提出基于结构化稀疏假设的动作发现算法，证明在稀疏性条件下能够高效识别相关动作集。


<details>
  <summary>Details</summary>
Motivation: 现代智能体系统（如工具增强语言模型）面临数千个可用API或检索操作的超大规模动作空间，但经验表明只有少数动作对特定部署的性能有实质性影响。这种观察促使研究者探索在结构化稀疏假设下的动作发现理论。

Method: 将动作发现问题形式化为块稀疏恢复问题，提出受正交匹配追踪启发的贪婪算法。在不相干性、信号强度和动作覆盖的标准假设下，分析算法性能，证明能够以高概率精确恢复相关动作集。

Result: 证明贪婪算法能够以高概率精确恢复相关动作集，所需样本数量随稀疏水平和潜在维度多项式增长，仅随总动作数量对数增长。提供重拟合参数的估计误差保证，并证明所得决策规则对新潜在状态接近最优。

Conclusion: 稀疏动作发现是大规模动作决策的基本原理，为智能体系统中的动作剪枝提供了理论基础。同时建立信息论下界，证明稀疏性和充分覆盖是问题可处理性的必要条件。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [29] [OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System](https://arxiv.org/abs/2601.08288)
*Yuyang Wu,Hanzhong Cao,Jianhao Chen,Yufei Li*

Main category: cs.AI

TL;DR: OpenMic：基于AutoGen的多智能体系统，将用户提供的生活话题转化为3-5分钟中文单口喜剧表演并生成叙事性喜剧视频


<details>
  <summary>Details</summary>
Motivation: 中文单口喜剧生成需要文化背景的幽默、精确的时机把握、舞台表演提示和隐式的多步推理。现有中文幽默数据集更适合幽默理解和评估，而非长篇单口喜剧生成，导致直接监督与目标任务不匹配。

Method: 1. 构建基于AutoGen的端到端多智能体系统OpenMic；2. 采用多轮迭代循环规划，协调多个专业智能体共同优化幽默性、时机把握和可表演性；3. 使用检索增强生成（RAG）进行素材基础和想法扩展；4. 微调专门的JokeWriter模型以更好地内化单口喜剧特有的铺垫-笑点结构和长程呼应。

Result: OpenMic系统能够将用户提供的生活话题转化为完整的3-5分钟中文单口喜剧表演，并进一步生成叙事性喜剧视频。

Conclusion: 通过多智能体协同、RAG增强和专门模型微调，OpenMic有效解决了中文单口喜剧生成中的文化背景、时机把握、表演提示和数据集不匹配等挑战，实现了从话题到完整表演的端到端生成。

Abstract: Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.

</details>


### [30] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: AtomMem：将记忆管理重构为动态决策问题，通过原子CRUD操作和强化学习实现任务对齐的自适应记忆策略


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆机制主要依赖静态、手工设计的工作流程，限制了性能和泛化能力，需要更灵活、基于学习的记忆框架

Method: 将高层记忆过程解构为原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程，结合监督微调和强化学习训练任务对齐的自主策略

Result: 在3个长上下文基准测试中，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法，训练动态分析显示模型能够发现结构化、任务对齐的记忆管理策略

Conclusion: 基于学习的记忆管理框架相比预定义流程具有关键优势，能够实现自适应、任务特定的记忆行为编排

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [31] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文指出LLM智能体架构将信息传输机制与认知证成机制混为一谈，导致"语义洗钱"问题——缺乏充分依据的命题通过可信接口被系统接受，这构成了盖梯尔问题的架构实现。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体架构存在系统性缺陷，将信息传输机制与认知证成机制混为一谈，导致缺乏充分依据的命题被系统接受。这种架构性失败需要被形式化分析和解决。

Method: 通过形式化分析"语义洗钱"概念，将其定义为一种架构模式：缺乏或仅有弱依据的命题通过跨越架构可信接口而被接受。提出"必然自许可定理"和"依据侵蚀原理"作为理论基础。

Result: 证明了语义洗钱是盖梯尔问题的架构实现：命题获得高认知地位却缺乏真值与证成之间的关联。在标准架构假设下，循环认知证成无法消除，扩展、模型改进和LLM作为评判方案都无法解决这一类型层面的问题。

Conclusion: 基于LLM的智能体架构存在根本性缺陷，语义洗钱问题在类型层面存在且无法通过现有技术手段消除。这揭示了当前架构设计在认知证成方面的系统性失败，需要重新思考智能体架构的基础原理。

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [32] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: TWG 5工作组专注于提升教师AI素养与能动性，开发实施有效策略，使教师具备将AI融入教学实践所需的知识与技能。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术快速发展，但教师在AI素养方面存在不足，需要提升教师对AI的理解与应用能力，以有效将AI工具整合到教学实践中，培养学生对AI概念的深入理解。

Method: 通过课程设计、专业发展项目、实际课堂应用和政策指南等多维度探索，开发系统化的教师AI能力提升策略，包括理论学习和实践应用相结合的方法。

Result: 开发了增强教师AI素养与能动性的综合策略框架，涵盖从课程设计到政策支持的全方位解决方案，为教师提供了系统化的AI教学能力提升路径。

Conclusion: 通过系统化的策略开发，成功构建了提升教师AI素养与能动性的框架，使教师能够自信运用AI工具，并在教学中有效培养学生的AI理解能力。

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [33] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 提出QOR定性模型解决立方体比较测试，通过构建CNGRLO概念邻域图来推理物体旋转


<details>
  <summary>Details</summary>
Motivation: 解决Ekstrom等人(1976)提出的立方体比较测试问题，该测试涉及对旋转立方体的空间推理，需要定性方法来处理旋转推理的复杂性

Method: 构建CNGRLO概念邻域图，将旋转运动与立方体面特征的位置变化和方向变化联系起来，并生成组合表用于旋转推理计算

Result: 开发了QOR定性模型，成功应用于解决立方体比较测试问题，通过概念邻域图和组合表实现了对旋转的有效推理

Conclusion: QOR模型为物体旋转推理提供了有效的定性框架，CNGRLO概念邻域图和组合表方法能够处理复杂的空间旋转推理问题

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [34] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: MoE架构通过稀疏性在训练早期形成稳定的分布式知识存储结构，与密集模型相比具有低熵骨干、早期固化和功能鲁棒性三大特征。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在预训练期间如何影响知识获取过程，以及这一过程与密集架构的差异，以理解稀疏性如何塑造模型的学习动态。

Method: 提出Gated-LPI（门控对数概率增量）神经元级归因度量，分解跨神经元的对数概率增量；对MoE和密集架构进行时间分辨比较，跟踪1.2M训练步（~5.0T tokens）和600K训练步（~2.5T tokens）的检查点。

Result: 发现三个关键模式：1）低熵骨干：MoE中约1%的神经元捕获超过45%的正向更新，形成高效用核心；2）早期固化：MoE在<100K步内锁定稳定重要性分布，而密集模型在整个训练中保持波动；3）功能鲁棒性：掩码最重要的MoE注意力头仅使关系HIT@10下降<10%，而密集模型下降>50%。

Conclusion: 稀疏性从训练早期就培养出内在稳定和分布式的计算骨干，有助于弥合稀疏架构与训练时可解释性之间的差距。

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [35] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: 论文提出生成视角的AI创造力框架，将创造力视为有界信息环境中领域受限生成模型的涌现属性，而非后验评估标签


<details>
  <summary>Details</summary>
Motivation: 当前AI创造力研究主要采用评估框架衡量生成输出的新颖性、多样性或有用性，将创造力视为待评估的属性而非待建模的现象。随着大规模生成系统（特别是多模态架构）在模式重组方面展现日益复杂的能力，需要重新思考机器创造力的本质和界限

Method: 提出生成视角的AI创造力框架，将创造力分解为四个相互作用组件：基于模式的生成、诱导世界模型、上下文基础、任意性，并研究这些组件在多模态生成系统中的表现。关注生成动态与领域特定表征交互的结构和上下文条件

Result: 建立了一个技术框架，将创造力视为AI系统中的涌现现象而非后验评估标签，为研究生成动态与领域特定表征交互如何产生创造性行为提供了概念基础

Conclusion: 通过将创造力重新定义为有界信息环境中领域受限生成模型的涌现属性，该工作为AI创造力研究提供了新的生成视角，强调结构和上下文条件而非评估标准，有助于更深入地理解机器创造力的本质和机制

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [36] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: OSPO使用Shapley-Owen归因进行基于潜在值的奖励重塑，将序列级优势重新分配给语义连贯的文本片段，解决了推荐任务中基于RL的LLM训练中的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的LLM推荐方法（如GRPO）依赖稀疏的序列级奖励，导致信用分配问题，难以确定哪些token驱动成功。当模型需要从非明确语言推断潜在用户意图时，这一问题尤为严重，因为这种推理模式在预训练中很少见。

Method: 提出Owen-Shapley策略优化（OSPO）框架，通过Shapley-Owen归因进行基于潜在值的奖励重塑，将序列级优势重新分配给语义连贯的文本片段（如描述产品属性的短语或捕捉偏好的句子），无需参数化价值模型，直接从任务反馈中学习。

Result: 在Amazon ESCI和H&M Fashion数据集上的实验显示，OSPO相比基线方法获得了一致的性能提升，并且在测试时对训练中未见过的检索器表现出显著的分布外鲁棒性。

Conclusion: OSPO通过基于Shapley-Owen归因的奖励重塑有效解决了LLM推荐中的信用分配问题，无需额外计算成本，在保持最优策略的同时提高了模型性能和对未见检索器的鲁棒性。

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [37] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种结合知识蒸馏、思维链引导和监督微调的方法，将大型语言模型的代码生成能力高效迁移到小型模型，用于资源受限的无人机多SDK控制任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成任务中展现出巨大潜力，但将其应用于资源受限的无人机机载控制时存在矛盾：大模型的高资源消耗与无人机平台的实时性、轻量化要求相冲突。需要一种方法将复杂推理和代码生成能力高效迁移到小型模型。

Method: 1) 构建高质量数据集：覆盖多种主流无人机SDK，包含指令-代码-推理链，并加入反事实负样本进行数据增强；2) 知识蒸馏：以DeepSeek-Coder-V2-Lite为教师模型，采用混合黑盒白盒蒸馏策略生成高质量思维链软标签，结合硬标签的加权交叉熵损失；3) 提示调优工程：针对无人机控制场景优化，提升SDK类型识别和函数调用匹配等核心任务性能。

Result: 蒸馏后的轻量化模型在保持高代码生成准确率的同时，在部署和推理效率方面实现显著提升，有效证明了该方法在实现无人机精准轻量化智能控制方面的可行性和优越性。

Conclusion: 该集成方法成功解决了大型语言模型与无人机资源约束之间的矛盾，通过知识蒸馏等技术将复杂推理能力迁移到小型模型，为资源受限平台的智能控制提供了有效解决方案。

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [38] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 提出RubricHub框架，通过粗到细的评分标准生成方法创建大规模多领域数据集，结合拒绝采样微调和强化学习，在开放生成任务上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有可验证奖励的强化学习在数学等推理密集型领域取得进展，但在开放生成任务中面临缺乏真实标签的挑战。基于评分标准的评估方法存在可扩展性瓶颈和标准粗糙问题，导致监督天花板效应

Method: 提出自动化粗到细评分标准生成框架，结合原则引导合成、多模型聚合和难度演化，创建RubricHub大规模多领域数据集。采用两阶段后训练流程：基于评分标准的拒绝采样微调和强化学习

Result: RubricHub包含约11万样本，覆盖多领域。后训练的Qwen3-14B在HealthBench上达到69.3分，超越GPT-5等前沿专有模型，实现SOTA性能

Conclusion: RubricHub框架通过生成细粒度、高区分度的评分标准，有效解决了开放生成任务的监督瓶颈，为语言模型后训练提供了可扩展的高质量监督信号

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [39] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: YaPO提出了一种基于稀疏自编码器的稀疏导向向量学习方法，用于大语言模型的细粒度对齐控制，相比密集导向向量具有更好的解耦性、可解释性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活干预的密集导向向量方法由于神经元多语义性导致多个潜在因素纠缠，在需要区分密切相关的价值观和行为（如中东文化对齐）的细粒度场景中效果有限且不稳定。

Method: YaPO是一种无参考方法，在稀疏自编码器的潜在空间中学习稀疏导向向量。通过优化稀疏编码，产生解耦、可解释且高效的导向方向。

Result: YaPO收敛更快、性能更强、训练稳定性更好。在文化对齐、幻觉、财富追求、越狱、权力追求等多种对齐相关行为上表现良好，且不损害MMLU通用知识评估。

Conclusion: YaPO为大语言模型的高效、稳定、细粒度对齐提供了通用方案，在可控性和领域适应方面具有广泛应用前景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [40] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: TABGR：一种基于图表示的表格推理方法，通过构建属性表格图（ATG）来保留表格结构，并使用个性化PageRank缓解"迷失在中间"问题，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格推理方法通常将表格线性化为纯文本，这会导致三个关键问题：1）丢失表格结构信息；2）缺乏显式推理路径，影响结果可解释性；3）存在"迷失在中间"问题（信息在长序列中丢失）。

Method: 提出TABGR模型，包含两个核心组件：1）属性表格图（ATG）表示，将表格转换为图结构，显式保留行-列-单元格关系；2）问题引导的个性化PageRank（QG-PPR）机制，根据问题相关性对表格数据进行重排序，缓解"迷失在中间"问题。该方法无需训练。

Result: 在两个常用基准测试上的广泛实验表明，TABGR在准确率上持续优于最先进模型，最高提升达9.7%。

Conclusion: TABGR通过图表示和基于图的推理，有效解决了表格线性化带来的结构丢失、可解释性差和"迷失在中间"问题，为表格推理任务提供了更优的解决方案。

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [41] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: 提出一个多模态可解释的Web应用，用于检测印地语-英语混合文本和表情包中的厌女内容，采用最先进的Transformer模型，并集成SHAP和LIME解释技术。


<details>
  <summary>Details</summary>
Motivation: 数字平台用户激增导致仇恨言论和厌女内容传播，现有AI解决方案对低资源混合语言支持不足且缺乏可解释性，需要透明、可解释的检测系统来打击基于性别的数字暴力。

Method: 1) 文本检测：使用XLM-RoBERTa和mBERT处理约4,193条混合语言评论；2) 多模态检测：结合mBERT+EfficientNet和mBERT+ResNet处理约4,218个表情包；3) 可解释性：集成SHAP和LIME提供特征重要性评分；4) 评估：通过Chatbot Usability Questionnaire和User Experience Questionnaire进行人工评估。

Result: 开发了一个功能完整的Web应用程序，支持文本和表情包中的厌女内容检测，提供模型决策的可解释性分析，并通过人类评估验证了系统可用性。

Conclusion: 该系统为研究者和内容审核者提供了一个有效工具，促进了该领域研究，有助于打击基于性别的数字暴力，确保安全的数字空间，同时展示了多模态可解释AI在敏感内容检测中的实用价值。

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [42] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: SummPilot：基于交互的可定制摘要系统，结合自动摘要效率与个性化需求，通过语义图、实体聚类等组件实现用户定制化摘要生成


<details>
  <summary>Details</summary>
Motivation: 传统自动摘要系统缺乏个性化定制能力，无法满足不同用户对摘要内容、重点和详细程度的具体需求。需要结合自动摘要效率与用户交互，生成符合个体兴趣和要求的个性化摘要。

Method: 提出SummPilot系统，利用大语言模型支持自动和交互式摘要。系统包含语义图可视化、实体聚类、可解释评估等交互组件，用户可通过这些组件理解文档内容并定制摘要。

Result: 演示和用户研究表明SummPilot具有适应性和实用性，能够有效支持可定制摘要生成，满足用户个性化需求。

Conclusion: SummPilot成功解决了自动摘要系统个性化不足的问题，通过交互式设计实现了高效且可定制的摘要生成，为个性化信息处理提供了有效解决方案。

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [43] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: WIT是一个多模态时间序列预测基准，通过专家构建的合理或反事实场景来评估模型是否能基于上下文文本（特别是未来场景）调整预测


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法多为单模态，依赖历史模式外推。虽然大语言模型在多模态预测方面有潜力，但现有基准主要提供回顾性或未对齐的原始上下文，无法确定模型是否真正利用了文本输入。人类专家在实践中会结合历史证据和假设场景，在不同场景下基于相同观测产生不同的预测。

Method: 引入What If TSF (WIT)多模态预测基准，通过提供专家构建的合理或反事实场景，为场景引导的多模态预测提供严格测试平台。基准包含场景引导的预测任务，评估模型是否能基于上下文文本（特别是未来场景）调整预测。

Result: WIT基准已开发完成并公开可用，为评估多模态时间序列预测模型提供了专门测试平台，重点关注模型在场景引导下的预测能力。

Conclusion: WIT基准填补了现有时间序列预测评估的空白，通过场景引导的多模态预测任务，能够更准确地评估模型是否真正利用了文本上下文信息，特别是未来场景对预测的影响。

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [44] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: 提出结合生成式AI和视觉语言模型的三阶段框架，直接从粗略结构草图和文本描述生成立面改造方案，避免详细现状建模


<details>
  <summary>Details</summary>
Motivation: 立面改造比完全拆除更可持续，但现有工作流程需要详细的现状建模，耗时耗力且需要反复修改，需要更高效的方案生成方法

Method: 三阶段框架：1) 微调VLM模型预测需要修改的区域和组件边界框；2) 稳定扩散模型生成新元素详细草图，通过生成式修复管道与原始轮廓合并；3) ControlNet将结果细化为逼真图像

Result: 在数据集和真实工业建筑上的实验表明，该框架能生成保留原始结构同时提升立面细节质量的改造方案，有效绕过详细现状建模需求

Conclusion: 该方法使建筑师能快速探索设计替代方案，迭代早期概念，更清晰地传达改造意图，为立面改造提供高效AI驱动解决方案

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [45] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 提出LPR（学习者定制程序修复）新任务和LSG框架，通过两阶段检索增强方法修复bug并提供解释，优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有智能编程辅导系统大多只修复bug而不提供根本原因，需要一种既能修复代码又能解释bug原因的学习者定制化方法

Method: 提出LSG框架：第一阶段使用修复方案检索框架构建数据库，采用编辑驱动代码检索获取有价值解决方案；第二阶段提出解决方案引导的程序修复方法，在检索方案指导下修复代码并提供解释；还提出迭代检索增强方法，利用生成代码评估结果优化检索方向

Result: 实验结果表明该方法大幅优于一组基线模型，验证了LSG框架在新提出的LPR任务上的有效性

Conclusion: 提出的LPR任务和LSG框架能够有效修复学习者代码bug并提供解释，在编程辅导场景中具有实际应用价值

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [46] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: WaterCopilot是一个基于RAG和工具调用架构的AI虚拟助手，用于解决跨界河流流域水资源管理中的数据碎片化、实时访问受限和信息整合复杂等问题，在Limpopo河流域实现了统一交互平台。


<details>
  <summary>Details</summary>
Motivation: 跨界河流流域可持续水资源管理面临数据碎片化、实时访问受限以及整合多样化信息源复杂性的挑战，需要统一交互平台来弥合这些差距。

Method: 基于检索增强生成(RAG)和工具调用架构，开发WaterCopilot系统，包含两个自定义插件：iwmi-doc-plugin（使用Azure AI Search进行语义搜索）和iwmi-api-plugin（查询实时数据库）。系统支持多语言交互、透明来源引用、自动计算和可视化功能。

Result: 使用RAGAS框架评估，WaterCopilot总体得分0.8043，其中答案相关性0.8571，上下文精确度0.8009。系统成功整合了静态政策文档和实时水文数据，提供环境流量警报、降雨趋势、水库水位、水核算和灌溉数据等动态洞察。

Conclusion: WaterCopilot建立了一个可复制的AI增强框架，用于在数据稀缺的跨界环境中加强水资源治理。虽然存在非英语技术文档处理和API延迟等限制，但该系统展示了AI助手支持复杂河流流域及时知情决策和加强水安全的潜力。

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [47] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: ViDoRe v3是一个全面的多模态RAG基准测试，包含视觉丰富文档上的多类型查询，涵盖10个专业领域数据集，提供26,000文档页和3,099人工验证查询，支持6种语言，用于评估复杂RAG管道的视觉理解、多文档综合和来源定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试主要关注文本数据、单文档理解或孤立评估检索与生成，无法捕捉真实场景中处理视觉元素（表格、图表、图像）、跨文档信息综合和准确来源定位的复杂性，需要更全面的多模态评估框架。

Method: 构建包含10个专业领域数据集的视觉丰富文档语料库（约26,000文档页），创建3,099个人工验证的多类型查询，支持6种语言，通过12,000小时人工标注提供检索相关性、边界框定位和验证参考答案的高质量标注。

Result: 评估显示：视觉检索器优于文本检索器；后期交互模型和文本重排序显著提升性能；混合或纯视觉上下文提高答案生成质量；但当前模型在非文本元素处理、开放式查询和细粒度视觉定位方面仍有困难。

Conclusion: ViDoRe v3为多模态RAG研究提供了全面的评估基准，揭示了当前方法的局限性，特别是视觉理解和细粒度定位方面的挑战，该基准采用商业许可发布以促进相关技术进步。

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [48] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 提出一个可解释的多智能体系统用于模因币跟单交易，通过分解复杂任务并协调专业智能体，在模因币市场中超越传统机器学习模型和单一LLM


<details>
  <summary>Details</summary>
Motivation: 模因币跟单交易面临操纵性机器人盛行、被跟随钱包未来表现不确定、交易执行延迟等问题，而单一LLM在处理复杂多任务时能力有限，且在加密货币领域缺乏足够专业知识

Method: 受资产管理团队结构启发，将复杂任务分解为子任务，协调专业智能体协作解决；采用少样本思维链提示，使每个智能体获取专业模因币交易知识、解释多模态数据并生成可解释决策

Result: 在1000个模因币项目交易数据集上，多智能体系统在识别高质量模因币项目和关键意见领袖钱包方面分别达到73%和70%的精确度；选定的KOL在这些项目中总计产生50万美元利润

Conclusion: 提出的可解释多智能体系统有效解决了模因币跟单交易的挑战，通过任务分解和专业智能体协作，显著提升了交易决策的质量和可解释性

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [49] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: 为本科生设计的强化学习入门教程，通过实例驱动教学，帮助理解理论与代码实现之间的衔接


<details>
  <summary>Details</summary>
Motivation: 解决本科生在学习强化学习时面临的常见挑战，特别是从理论理解到实际代码实现之间的鸿沟，使强化学习对本科生更加可及

Method: 采用清晰、示例驱动的解释方法，通过动手实践示例和易于理解的讲解，帮助学生掌握基础技能

Result: 该教程旨在使学生能够自信地在实际场景中应用强化学习技术，建立理论与实践的桥梁

Conclusion: 通过实例驱动的教学方法，可以有效降低本科生学习强化学习的门槛，帮助他们从理论理解顺利过渡到实际应用

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [50] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: 提出Pced框架，通过并行解码而非注意力机制实现多文档推理，解决RAG中长提示预填充瓶颈与跨文档交互的权衡问题


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成中的权衡问题：长提示拼接支持多文档推理但导致预填充瓶颈，而分离文档KV缓存虽快但破坏跨文档交互

Method: 提出Parallel Context-of-Experts Decoding (Pced)训练免费框架，将证据聚合从注意力机制转移到解码过程，将检索文档视为独立"专家"，通过新颖的检索感知对比解码规则同步专家预测，权衡专家logits与模型先验

Result: 该方法无需构建跨文档共享注意力，即可恢复跨文档推理能力

Conclusion: Pced框架通过解码层面的创新解决了RAG中的关键权衡问题，为多文档推理提供了高效解决方案

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [51] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: PersonaDual框架在单一模型中同时支持通用客观推理和个性化推理，通过上下文自适应切换模式，在保持个性化优势的同时减少干扰


<details>
  <summary>Details</summary>
Motivation: 随着用户期望LLM与其偏好对齐，个性化信息变得有价值，但个性化信息可能是一把双刃剑：既能改善交互，又可能损害客观性和事实正确性，特别是当个性化信息与问题不匹配时

Method: 提出PersonaDual框架，首先通过监督微调学习两种推理模式，然后通过强化学习（使用提出的DualGRPO算法）进一步优化模式选择，实现基于上下文的自适应切换

Result: 在客观和个性化基准测试中，PersonaDual在保持个性化优势的同时减少了干扰，实现了接近无干扰的性能，并更好地利用有益的个性化信号来改进客观问题解决

Conclusion: PersonaDual框架有效解决了个性化信息可能损害客观性的问题，通过自适应模式切换实现了个性化与客观性的平衡

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [52] [MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection](https://arxiv.org/abs/2601.08684)
*Paolo Italiani,David Gimeno-Gomez,Luca Ragazzi,Gianluca Moro,Paolo Rosso*

Main category: cs.AI

TL;DR: MemeWeaver：用于检测性别歧视和厌女症的多模态框架，通过图推理机制捕捉在线仇恨的关系性质


<details>
  <summary>Details</summary>
Motivation: 女性因性别遭受在线骚扰的可能性是男性的两倍。现有多模态内容审核方法大多忽视了这种现象背后的社会动态，即施害者在志同道合的社区中强化偏见和群体认同。基于图的方法虽然有望捕捉此类互动，但现有解决方案仍受限于启发式图构建、浅层模态融合和实例级推理。

Method: 提出MemeWeaver，一种端到端可训练的多模态框架，通过新颖的meme间图推理机制检测性别歧视和厌女症。系统评估了多种视觉-文本融合策略，并展示了该方法在MAMI和EXIST基准测试中持续优于最先进的基线方法，同时实现更快的训练收敛。

Result: 在MAMI和EXIST基准测试中，MemeWeaver持续优于最先进的基线方法，并实现更快的训练收敛。进一步分析表明，学习到的图结构捕捉了语义上有意义的模式，为在线仇恨的关系性质提供了有价值的见解。

Conclusion: MemeWeaver通过图推理机制有效捕捉在线仇恨的社会动态，为检测性别歧视和厌女症提供了更全面的多模态框架，超越了传统的实例级方法。

Abstract: Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.

</details>


### [53] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: 提出OIP-SCE评估方法，通过检查临床对话中所有必需义务是否按正确顺序完成并提供清晰证据，解决现有评估方法忽略对话全过程合规性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI评估方法大多忽略了合规性依赖于完整对话过程这一事实，导致技术进展与医疗实际需求之间存在差距，需要一种能够评估完整对话流程合规性的方法。

Method: 提出Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE)方法，该方法检查每个必需的临床义务是否按正确顺序完成，并提供清晰的证据供临床医生审查，将复杂规则转化为可实践、可审计的评估框架。

Result: 在两个案例研究（呼吸病史、福利验证）中展示了该方法如何将政策转化为共享、可操作的步骤，通过阶段级证据使复杂规则变得实用且可审计。

Conclusion: OIP-SCE为临床医生提供了控制检查内容的能力，为工程师提供了清晰的实施规范，创建了一个统一、可审计的评估界面，使AI能力与临床工作流程对齐，支持常规安全使用。

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [54] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: 该论文提出了一种无需真实标签即可评估模型解释质量的新方法AXE，能够检测Rashomon集合中模型解释的对抗性公平漂白，并识别受保护属性的使用。


<details>
  <summary>Details</summary>
Motivation: 在Rashomon集合中，多个模型性能相似但内部机制不同，现有解释评估方法依赖与真实解释的对比，这会掩盖模型间的行为差异，且无法检测对抗性公平漂白问题。

Method: 提出了解释评估的三个原则，并开发了AXE方法（无需真实标签的解释评估），通过分析解释本身的质量来评估特征重要性解释，而非依赖真实标签对比。

Result: AXE能够以100%的成功率检测对抗性公平漂白，识别受保护属性是否被用于预测，相比基于模型敏感性或真实标签对比的现有方法更具优势。

Conclusion: AXE提供了一种无需真实标签的模型解释评估框架，能够揭示Rashomon集合中模型的行为差异，帮助选择更合适的模型，并有效检测解释中的公平漂白问题。

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [55] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago是一种新颖的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择刚好超出当前能力范围的中间目标来引导学习，从而解决长时域环境中模仿学习的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时域环境中经常失败，因为完美复制演示不现实，小错误会灾难性累积。现有方法要么仅用演示初始化策略，要么用于奖励塑形，无法有效处理智能体能力与专家轨迹之间的差距。

Method: Cago（能力感知目标采样）动态跟踪智能体在专家轨迹上的能力，使用这个信号选择刚好超出智能体当前能力范围的中间步骤（目标）来引导学习。这创建了一个自适应课程，使智能体能够稳步向解决完整任务前进。

Result: 实验结果表明，Cago在一系列稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，始终优于现有的模仿学习基线方法。

Conclusion: Cago通过动态能力感知的目标采样机制，有效解决了模仿学习在长时域环境中的脆弱性问题，提供了一种更鲁棒的学习方法。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [56] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 论文指出AI系统主要被定位为提升生产力的智能工具，但娱乐正成为AI的重要应用场景，而现有评估框架无法衡量AI娱乐内容的社会文化价值，需要新的"厚重娱乐"评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域过度关注生产力提升的叙事，忽视了娱乐作为AI重要应用场景的兴起。现有评估框架存在严重不对称性：只关注文化危害，缺乏衡量文化益处的框架，无法应对AI娱乐内容对社会的影响。

Method: 通过分析AI娱乐应用现状数据，批判现有评估实践的局限性，借鉴人文学科理论，提出"厚重娱乐"评估框架，该框架关注娱乐在意义建构、身份形成和社会连接中的作用。

Result: 发现AI娱乐应用已广泛普及（尤其在年轻人中），可能成为AI公司的主要商业模式；现有评估仅关注文化危害，缺乏衡量文化益处的系统方法；提出"厚重娱乐"作为替代评估框架。

Conclusion: AI可能最终更多是关于娱乐而非智能，正如社交媒体更多是关于连接而非社交。需要建立新的评估框架来理解和衡量AI娱乐内容的社会文化价值，而不仅仅是防范危害。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [57] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种通过将模型生成的投票预测与真实议会投票记录对齐来构建政治偏见基准的通用方法，并在荷兰、挪威和西班牙三个国家案例中应用，发现先进LLM普遍存在左倾或中间倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在数字平台和决策系统中深度嵌入，对其政治偏见的担忧日益增长。尽管已有大量研究关注性别和种族等社会偏见，但对政治偏见的系统性研究仍然有限，尽管其具有直接的社会影响。

Method: 提出构建政治偏见基准的通用方法：将模型生成的投票预测与经过验证的议会投票记录对齐。在三个国家案例中实例化该方法：PoliBiasNL（荷兰）、PoliBiasNO（挪威）和PoliBiasES（西班牙）。提出将LLM和政党的意识形态可视化到共享的二维CHES空间的方法，通过将基于投票的位置与CHES维度链接，实现模型与现实政治行为者之间的直接可解释比较。

Result: 实验揭示了细粒度的意识形态区分：最先进的LLM一致显示出左倾或中间倾向，同时对右翼保守政党存在明显的负面偏见。

Conclusion: 这些发现强调了基于真实议会行为的透明、跨国评估对于理解和审计现代LLM中政治偏见的价值。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [Contact-aware Path Planning for Autonomous Neuroendovascular Navigation](https://arxiv.org/abs/2601.07945)
*Aabha Tamhankar,Ron Alterovitz,Ajit S. Puri,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 提出一种用于神经血管导航的确定性、时间高效的接触感知路径规划器，利用术前和术中血管图像信息，通过智能预测和利用与解剖结构的相互作用来导航预弯曲被动工具。


<details>
  <summary>Details</summary>
Motivation: 神经血管导航需要精确控制预弯曲被动工具在复杂血管结构中的运动，传统方法在考虑工具与血管壁接触相互作用方面存在不足，需要开发能够智能预测和利用这些相互作用的路径规划算法。

Method: 算法基于术前和术中血管图像信息，推导运动学模型，采用基于采样的规划器进行树扩展，利用简化的运动基元，通过智能预测工具与解剖结构的相互作用来实现接触感知的路径规划。

Result: 在最坏情况下，算法在22.8秒内达到100%收敛率，跟踪误差小于0.64毫米（亚毫米级），在代表约94%患者的解剖体模上验证有效，能够快速计算可行路径且精度损失可忽略。

Conclusion: 提出的接触感知路径规划器能够高效、精确地导航预弯曲被动工具通过复杂血管结构，为神经血管介入手术提供了有前景的解决方案，具有临床应用的潜力。

Abstract: We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.

</details>


### [59] [Fiducial Exoskeletons: Image-Centric Robot State Estimation](https://arxiv.org/abs/2601.08034)
*Cameron Smith,Basile Van Hoorick,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: Fiducial Exoskeletons：一种基于图像的3D机器人状态估计方法，通过单张RGB图像估计机器人各连杆的6D位姿，替代传统繁琐的标定流程


<details>
  <summary>Details</summary>
Motivation: 传统机器人状态估计方法依赖高精度执行器和耗时的手眼标定流程，而现代基于学习的机器人控制越来越多地使用RGB观测在低成本硬件上训练和部署。需要简化机器人-相机外参估计过程，使其更鲁棒且易于实现。

Method: 1. 将机器人状态估计重构为从单张RGB图像估计每个连杆的6D位姿：机器人-相机基座变换直接作为估计的基连杆位姿获取，关节状态通过轻量级全局优化恢复，强制与观测到的连杆位姿保持运动学一致性（可选使用编码器读数预热启动）。2. 引入"基准外骨骼"：每个连杆上安装带有基准标记的3D打印支架，具有已知的标记-连杆几何关系，使每连杆6D位姿估计变得鲁棒且简单（即使无需学习）。

Result: 该方法在低成本机械臂上得到验证，能够从单张图像获得鲁棒的相机-机器人外参、每连杆SE(3)位姿和关节角度状态，即使机器人未通电也能实现鲁棒状态估计。显著简化了设置过程，同时提高了标定精度、状态准确性和下游3D控制性能。

Conclusion: Fiducial Exoskeletons提供了一种硬件-算法协同设计的图像式机器人状态估计方案，通过基准标记外骨骼简化了传统复杂的标定流程，使低成本硬件上的机器人状态估计更加鲁棒和易于实现。作者开源了代码和可打印的硬件设计以促进进一步研究。

Abstract: We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.
  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.
  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.

</details>


### [60] [Efficient Incremental SLAM via Information-Guided and Selective Optimization](https://arxiv.org/abs/2601.08110)
*Reza Arablouei*

Main category: cs.RO

TL;DR: 提出了一种高效的增量SLAM后端方法，结合信息引导门控和选择性部分优化，在保持批量优化精度的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统增量SLAM方法在实时动态数据丰富环境中面临计算效率与估计精度之间的权衡挑战，需要一种既能保持全局一致性又能高效计算的方法

Method: 结合信息引导门控（IGG）和选择性部分优化（SPO）。IGG基于信息矩阵对数行列式评估新测量的信息增益，仅在显著增益时触发全局优化；SPO执行多轮高斯牛顿更新但限制于受新测量影响最大的变量子集，动态调整活动集直至收敛

Result: 在基准SLAM数据集上的实验表明，该方法始终匹配批量求解器的估计精度，同时相比传统增量方法实现显著计算节省，提供精度与效率的平衡

Conclusion: 提出的方法在保持完整高斯牛顿收敛保证的同时，通过信息理论准则和选择性优化实现了计算效率的显著提升，为实时动态数据丰富环境提供了鲁棒且可扩展的解决方案

Abstract: We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.

</details>


### [61] [A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles](https://arxiv.org/abs/2601.08143)
*Takuya Kato,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了一种用于极端环境移动机器人的抓取器，能够同时抓取和识别地形形状，采用针阵列结构适应不规则地形


<details>
  <summary>Details</summary>
Motivation: 多肢攀爬机器人在崎岖地形（如悬崖、洞穴壁）上有效，但在未知自然环境中可能因误抓表面或失去可抓取点而摔倒或卡住，需要能自适应不规则地形并准确测量地形形状的抓取器

Method: 开发了一种采用针阵列结构的抓取器，能够抓取凸凹地形并同时测量地形形状，通过原型机评估了其抓取和地形识别性能

Result: 提出的针阵列设计不仅适用于不规则地形的自适应抓取，还能有效进行3D地形测绘

Conclusion: 该抓取器解决了极端环境下移动机器人面临的抓取和地形识别问题，通过针阵列结构实现了对不规则地形的自适应抓取和精确形状测量

Abstract: This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.

</details>


### [62] [Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching](https://arxiv.org/abs/2601.08161)
*Jing Tao,Banglei Guan,Yang Shang,Shunkun Liang,Qifeng Yu*

Main category: cs.RO

TL;DR: 提出一种用于大规模飞行导航的鲁棒高精度定位方法，解决复杂背景干扰导致的定位失败和传统滑动窗口匹配计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模飞行导航中复杂背景干扰导致的定位失败问题，以及传统滑动窗口匹配技术计算效率低下的局限性。

Method: 采用三层框架：1) 通过光照均衡和结构信息提取进行降维；2) 粗到精候选点选择策略减少滑动窗口计算成本；3) 为候选点生成自适应模板，通过改进的模板匹配和相关系数极值拟合实现亚像素精度。

Result: 实验结果表明该方法能有效提取和定位复杂大规模环境中的对角标记，适用于导航任务中的视场测量。

Conclusion: 该方法实现了鲁棒、高精度的定位，特别适合复杂大规模环境下的飞行导航应用。

Abstract: This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.

</details>


### [63] [FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models](https://arxiv.org/abs/2601.08246)
*Yifan Han,Pengfei Yi,Junyan Li,Hanqing Wang,Gaojing Zhang,Qi Peng Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出基于预训练扩散模型的数据高效框架，从人类视频演示中提取语义先验，实现无需特定硬件抓取数据集的多指灵巧抓取合成


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取合成面临高维度和运动多样性挑战，现有方法依赖大量硬件特定的抓取数据集，阻碍了新灵巧手设计的可扩展性

Method: 从人类视频演示中提取时间对齐的细粒度抓取可操作性，与深度图像的3D场景几何融合，通过运动学感知重定向模块映射到不同灵巧手

Result: 系统产生稳定、功能适当的多接触抓取，在常见物体和工具上可靠成功，对未见物体实例、姿态变化和不同手具现化表现出强泛化能力

Conclusion: 通过人类演示和预训练生成模型驱动的语义可操作性提取，为可扩展、硬件无关的灵巧操作开辟了新路径，单深度模态结合基础模型语义即可实现高性能抓取合成

Abstract: Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.

</details>


### [64] [ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation](https://arxiv.org/abs/2601.08325)
*Zhenyang Liu,Yongchong Gu,Yikai Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: ActiveVLA：一种赋予机器人主动感知能力的视觉-语言-动作框架，通过粗到细的两阶段方法实现高精度细粒度操作


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作研究大多依赖静态腕部摄像头，缺乏主动感知能力，无法在任务执行过程中自适应选择最优视角或分辨率，限制了在长时程任务和细粒度操作场景中的性能

Method: 采用粗到细的两阶段范式：1)关键区域定位：将3D输入投影到多视角2D投影，识别关键3D区域；2)主动感知优化：基于定位的关键区域，使用主动视角选择策略选择最优视角，最大化非模态相关性和多样性，最小化遮挡，并应用3D放大提高关键区域分辨率

Result: ActiveVLA在三个仿真基准测试中实现了精确的3D操作，并优于最先进的基线方法。此外，该框架能够无缝迁移到真实世界场景，使机器人能够在复杂环境中学习高精度任务

Conclusion: ActiveVLA通过引入主动感知能力，有效解决了现有视觉-语言-动作框架在细粒度操作中的局限性，为机器人高精度操作提供了新范式

Abstract: Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.

</details>


### [65] [Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments](https://arxiv.org/abs/2601.08405)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.RO

TL;DR: 提出了一种基于微调CodeT5模型与AirSim无人机模拟器的自然语言控制方法，实现多任务无人机操作的自动化代码生成


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的快速发展，降低无人机操作门槛，通过自然语言交互简化复杂无人机任务的编程控制

Method: 将微调的CodeT5模型与基于Unreal Engine的AirSim无人机模拟器集成，使用ChatGPT生成的自然语言-程序代码对和开发者编写的无人机代码作为训练数据，实现自然语言到可执行代码的自动翻译

Result: 实验结果表明该方法在模拟环境中展现出优越的任务执行效率和命令理解能力

Conclusion: 该方法显著降低了无人机操作门槛，未来计划以模块化方式扩展模型功能，增强对复杂场景的适应性，推动无人机技术在真实环境中的应用

Abstract: Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.

</details>


### [66] [Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech](https://arxiv.org/abs/2601.08422)
*Taerim Yoon,Dongho Kang,Jin Cheng,Fatemeh Zargarbashi,Yijiang Huang,Minsung Ahn,Stelian Coros,Sungjoon Choi*

Main category: cs.RO

TL;DR: 提出人机交互框架，让四足机器人通过少量人类演示数据学习理解社交线索和导航行为，支持手势和语音多模态输入，在物理仿真中重建交互场景，实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 让四足机器人能够理解人类社交线索并产生适当行为，但通过物理交互学习需要大量人类提供的数据，给用户带来沉重负担，需要数据高效的学习方法。

Method: 提出人在回路框架，使用基于物理的仿真重建交互场景并聚合数据以缓解有限演示数据带来的分布偏移；采用渐进式目标提示策略，在训练过程中自适应地提供适当命令和导航目标；支持手势和语音多模态自然输入。

Result: 在六个真实世界敏捷导航场景（包括跳跃或避开障碍物）中评估，所提方法在几乎所有试验中都取得成功，总演示数据少于1小时的情况下达到97.15%的任务成功率。

Conclusion: 该框架能够以数据高效的方式让机器人学习导航行为，通过多模态自然输入控制，实现准确导航和人类输入与机器人行为之间的强对齐。

Abstract: In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.

</details>


### [67] [Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?](https://arxiv.org/abs/2601.08434)
*Long Zhang,Yuchen Xia*

Main category: cs.RO

TL;DR: 提出语义与策略双驱动混合决策框架，融合大语言模型与深度强化学习，解决自动驾驶在开放世界中的持续学习和联合决策问题。


<details>
  <summary>Details</summary>
Motivation: 传统模块化自动驾驶设计在需要持续环境理解和逻辑推理的开放世界场景中存在局限，而仅依赖大语言模型增强具身智能驾驶无法实现联合决策，限制了持续学习能力。

Method: 提出语义与策略双驱动混合决策框架，结合大语言模型进行语义理解和认知表征，利用深度强化学习进行实时策略优化，实现持续学习和联合决策。

Result: 通过车道变换规划任务的案例研究验证了框架的性能优越性，展示了在完成自动驾驶任务中的有效性。

Conclusion: 该混合决策框架为具身智能驾驶提供了有效的解决方案，并指出了未来研究方向以指导后续工作。

Abstract: The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.

</details>


### [68] [Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees](https://arxiv.org/abs/2601.08454)
*Alessandro Adami,Sebastian Zudaire,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 提出了一种基于行为树的自主Real2Sim框架，通过视觉语言模型推理生成任务特定的物理交互行为树，自动估计仿真所需物理参数，无需预定义任务模板或专家设计的探索例程。


<details>
  <summary>Details</summary>
Motivation: 传统Real2Sim流程依赖手动测量或固定的预编程探索例程，限制了其对不同任务和用户意图的适应性。需要一种能够根据具体仿真目标自主获取所需物理参数的方法。

Method: 给定高层用户请求、不完整仿真描述和场景RGB观测，使用视觉语言模型进行多模态推理识别相关对象、推断所需物理参数，并生成由基本机器人动作组成的结构化行为树。在扭矩控制的Franka Emika Panda上执行该行为，通过顺应性接触丰富的交互进行参数估计，最终自动构建物理感知仿真。

Result: 在真实机械臂上的实验结果表明，该方法能够在多种场景（包括遮挡对象和不完整先验模型）中准确估计物体质量、表面高度和摩擦相关参数，实现了可解释、意图驱动的自主Real2Sim流程。

Conclusion: 该方法通过将高层推理与物理基础的机器人交互相结合，实现了可解释、意图驱动的自主Real2Sim流程，为真实到仿真的参数估计提供了灵活且自适应的解决方案。

Abstract: Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.

</details>


### [69] [AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization](https://arxiv.org/abs/2601.08491)
*Mohamed Afouene Melki,Mohammad Shehab,Mohamed-Slim Alouini*

Main category: cs.RO

TL;DR: 该论文提出了一种可持续的水下物联网解决方案，通过自主水下航行器同时实现信息上行和声能传输，使设备能够无限期运行。采用深度强化学习算法优化年龄信息和公平性指标。


<details>
  <summary>Details</summary>
Motivation: 传统水下物联网设备依赖电池供电，存在寿命有限和废弃后环境危害的问题。需要一种可持续的解决方案来延长设备运行时间并减少环境影响。

Method: 提出通过自主水下航行器同时进行信息上行和声能传输的方法。采用年龄信息和Jain公平指数作为性能指标。开发了两种深度强化学习算法：高性能的频分双工方案和中等性能的时分双工方案。

Result: 提出的FDD和TDD解决方案显著降低了平均年龄信息，提高了能量收集效率，并改善了数据收集的公平性，相比基线方法有显著提升。

Conclusion: 该研究为水下物联网提供了一种可持续的解决方案，通过AUV同时实现信息传输和能量补充，使设备能够无限期运行，解决了传统电池供电设备的局限性。

Abstract: Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.

</details>


### [70] [Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation](https://arxiv.org/abs/2601.08514)
*Davide Risi,Vincenzo Petrone,Antonio Langella,Lorenzo Pagliara,Enrico Ferrentino,Pasquale Chiacchio*

Main category: cs.RO

TL;DR: 提出了一种用于ROS2的模块化架构，将参考值获取、验证和插值逻辑与控制律解耦，通过专用Reference Generator组件处理参考值，提高控制器代码复用性。


<details>
  <summary>Details</summary>
Motivation: 现有ROS2控制器中重复实现参考值处理逻辑，导致代码冗余，跨机器人平台复用性差，需要一种解耦参考值处理与控制律的架构。

Method: 设计Reference Generator组件，接收外部节点（如规划器）的单点或轨迹参考值，通过ros2_control链式机制以控制器采样周期写入单点参考值；实现两种参考生成器（关节空间和笛卡尔空间），并开发PD+重力补偿、笛卡尔位姿、导纳控制器。

Result: 在仿真和真实机器人（Universal Robots和Franka Emika）上验证：1）所有测试场景中参考值可靠跟踪；2）参考生成器减少链式控制器中的重复代码，支持复杂控制器管道构建和复用；3）控制器实现专注于控制律。

Conclusion: 提出的模块化架构成功解耦参考值处理与控制律，提高了ROS2控制器的代码复用性和可维护性，适用于多种机器人平台和控制器类型。

Abstract: This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.

</details>


### [71] [Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps](https://arxiv.org/abs/2601.08520)
*Krzysztof Zielinski,Dominik Belter*

Main category: cs.RO

TL;DR: 提出基于关键帧的NDT地图构建系统，利用RGB-D传感器数据更新局部NDT地图，采用视点依赖的2D结构存储单元，通过位姿图存储局部地图并支持闭环校正，最终合并局部地图生成全局地图。


<details>
  <summary>Details</summary>
Motivation: 传统地图构建方法未能充分利用RGB-D相机的特性和不确定性模型，需要一种能够根据相机距离自适应调整精度、支持闭环校正并能有效合并局部地图的建图系统。

Method: 1. 基于关键帧的NDT地图构建：使用RGB-D数据更新局部NDT地图；2. 视点依赖的2D存储结构：根据RGB-D相机特性设计存储单元；3. 位姿图集成：将局部地图存储在位姿图中支持闭环校正；4. 地图合并与滤波：通过专门流程合并局部地图生成全局地图。

Result: 与Octomap和NDT-OM方法进行比较，展示了所提方法的优势，并提供了实际应用示例验证了系统的有效性。

Conclusion: 提出的关键帧NDT建图系统能够更好地利用RGB-D相机特性，实现距离相关的精度自适应，支持闭环校正和全局地图生成，为机器人导航和环境建模提供了有效解决方案。

Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.

</details>


### [72] [QP-Based Control of an Underactuated Aerial Manipulator under Constraints](https://arxiv.org/abs/2601.08523)
*Nesserine Laribi,Mohammed Rida Mokhtari,Abdelaziz Benallegue,Abdelhafid El-Hadri,Mehdi Benallegue*

Main category: cs.RO

TL;DR: 提出一种约束感知控制框架，用于欠驱动空中机械臂，实现精确末端执行器轨迹跟踪，同时显式考虑安全和可行性约束。


<details>
  <summary>Details</summary>
Motivation: 解决欠驱动空中机械臂在复杂环境下进行精确末端执行器轨迹跟踪时面临的安全约束、可行性限制以及鲁棒性问题。

Method: 将控制问题表述为二次规划，计算满足欠驱动、执行器限制和系统约束的动态一致广义加速度；在力矩层面引入基于被动性的积分作用以增强鲁棒性。

Result: 通过高保真物理仿真验证，在参数扰动、粘性关节摩擦和真实传感状态估计条件下，实现了精确跟踪、平滑控制输入和可靠约束满足。

Conclusion: 所提出的约束感知控制框架能够有效处理欠驱动空中机械臂的复杂控制问题，在真实操作条件下保证安全约束的同时实现精确轨迹跟踪。

Abstract: This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.

</details>


### [73] [VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory](https://arxiv.org/abs/2601.08665)
*Shaoan Wang,Yuanfei Luo,Xingyu Chen,Aocheng Luo,Dongyue Li,Chang Liu,Sheng Chen,Yangang Zhang,Junzhi Yu*

Main category: cs.RO

TL;DR: VLingNav：一种基于语言驱动认知的VLA模型，通过自适应思维链机制和视觉辅助语言记忆模块，解决了具身导航中显式推理和长期记忆的挑战，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在具身导航中主要依赖从观察到动作的被动映射，缺乏复杂长时程导航任务所需的显式推理能力和持久记忆，限制了其在动态环境和长期任务中的表现。

Method: 1. 引入自适应思维链机制，基于人类认知双过程理论，动态触发显式推理，实现快速直觉执行与慢速审慎规划的灵活切换；2. 开发视觉辅助语言记忆模块，构建持久跨模态语义记忆，支持历史观察回忆和动态环境运动趋势推断；3. 构建Nav-AdaCoT-2.9M数据集（目前最大的具身导航推理标注数据集）；4. 采用在线专家指导强化学习训练策略。

Result: VLingNav在广泛的具身导航基准测试中达到最先进的性能，能够以零样本方式迁移到真实机器人平台，执行多种导航任务，展现出强大的跨领域和跨任务泛化能力。

Conclusion: VLingNav通过语言驱动认知框架有效解决了具身导航中的推理和记忆挑战，其自适应思维链机制和跨模态语义记忆模块为VLA模型在复杂导航任务中的应用提供了新范式，实现了从模拟到真实环境的无缝迁移。

Abstract: VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.

</details>


### [74] [Real-Time Localization Framework for Autonomous Basketball Robots](https://arxiv.org/abs/2601.08713)
*Naren Medarametla,Sreejon Mondal*

Main category: cs.RO

TL;DR: 提出一种混合定位算法，结合经典技术与基于学习的方法，仅利用球场地面视觉数据进行篮球场自定位


<details>
  <summary>Details</summary>
Motivation: 定位是自主机器人的基本能力，在Robocon 2025中，准确可靠的定位对于提高射击精度、避免与其他机器人碰撞以及高效导航比赛场地至关重要

Method: 提出混合定位算法，集成经典技术与基于学习的方法，仅依赖球场地面的视觉数据进行自定位

Result: 未在摘要中明确说明具体实验结果，但暗示该方法旨在实现篮球场上的自定位能力

Conclusion: 通过结合经典与学习方法的混合定位算法，能够仅利用视觉数据实现篮球场上的有效自定位

Abstract: Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.

</details>


### [75] [Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot](https://arxiv.org/abs/2601.08819)
*Roshni Kaushik,Reid Simmons*

Main category: cs.RO

TL;DR: 研究探索老年人对机器人运动教练不同节奏的言语和非言语反馈的反应，发现改变一种模态的节奏会影响两种模态的感知，为设计适合老年人群体的反馈频率提供依据。


<details>
  <summary>Details</summary>
Motivation: 人们以不同方式响应反馈和指导，机器人需要个性化交互并利用言语和非言语沟通线索。本研究旨在了解老年人如何响应机器人运动教练不同节奏的言语和非言语反馈。

Method: 通过在线研究，让老年人参与者评估机器人以不同节奏提供反馈的视频，分别针对言语和非言语两种模态。

Result: 结果表明，改变一种模态的节奏会影响对该模态和另一种模态的感知。不同节奏的反馈在老年人群体中产生不同的认知效果。

Conclusion: 研究结果可用于更好地设计机器人教练在与老年人群体的运动会话中的反馈频率，实现更有效的个性化交互。

Abstract: People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Hierarchical Sparse Plus Low Rank Compression of LLM](https://arxiv.org/abs/2601.07839)
*Pawan Kumar,Aditi Gupta*

Main category: cs.LG

TL;DR: HSS压缩方法：两阶段方案，先移除最大权重到稀疏矩阵，再对残差矩阵应用递归分层稀疏可分离低秩分解，实现LLM的高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型对内存和计算资源需求巨大，需要原则性压缩方法以便部署和持续训练。

Method: 1. 第一阶段：移除最大幅值权重到稀疏矩阵S；2. 第二阶段：对密集残差矩阵应用递归分层稀疏可分离低秩分解；3. 采用递归降秩策略和反向Cuthill-Mckee排列，使高权重向对角线对齐；4. 硬件友好设计，矩阵向量乘法简化为一次稀疏乘法和一系列薄矩阵乘法。

Result: 在LLaMA-7B上实验，仅压缩自注意力投影（1.6B参数中的Q、K、V矩阵）即可实现显著内存节省，同时保持可比的困惑度。使用30%稀疏预算和512外秩时，sHSS-RCM困惑度为1.64，优于密集基线和经典稀疏+SVD变体。

Conclusion: HSS压缩方案能有效减少LLM的内存和计算需求，同时保持模型性能，为模型部署和持续训练提供了可行的压缩解决方案。

Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a two-stage scheme that (i) removes the largest-magnitude weights into a sparse matrix S and (ii) applies a recursive Hierarchically Sparse Separable (HSS) low-rank factorisation to the dense residual matrix. A recursive rank-reducing strategy and a reverse Cuthill-Mckee (RCM) permutation are introduced to align high weights towards the diagonal with the block-diagonal hierarchy, maximising off-diagonal compressibility (because they are touched only once). HSS is hardware-friendly: its matrix-vector multiply reduces to one sparse and a sequence of thin-matrix multiplications and can be trained end-to-end with standard optimisers.
  Experiments on LLaMA-7B show that targeting only the self-attention projections (1.6 B parameters of Q, K, and V matrices out of a total 7B parameters) suffices to yield large memory savings while retaining comparable state-of-the-art perplexity scores on test samples of the WikiText dataset. For example, with a 30\% sparsity budget and an outer rank of 512, sHSS-RCM achieves a perplexity of 1.64, outperforming dense baselines and classical sparse-plus-SVD variants, while also achieving significant memory savings.

</details>


### [77] [Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification](https://arxiv.org/abs/2601.07858)
*Nina Peire,Yupei Li,Björn Schuller*

Main category: cs.LG

TL;DR: 正则化持续学习方法在EEG情感分类中表现有限，因其稳定性-可塑性权衡失衡，过度关注防止遗忘而忽视适应新被试


<details>
  <summary>Details</summary>
Motivation: EEG情感分类中跨被试泛化面临挑战，持续学习（CL）是潜在解决方案，但现有正则化CL方法（如EWC、SI、MAS）在该问题上的适用性尚未充分探索

Method: 通过理论和实证分析，在DREAMER和SEED数据集上评估正则化CL方法，研究其在被试增量序列中的表现，分析参数重要性估计、梯度干扰、约束累积和顺序敏感性等问题

Result: 正则化CL方法在EEG情感分类中表现有限：重要性估计在噪声数据和协变量偏移下不可靠；重要参数的梯度更新干扰新被试适应；累积约束过度限制模型；性能对顺序敏感；前向迁移无显著改善

Conclusion: 正则化持续学习方法不适合EEG情感分类的跨被试泛化，因为EEG信号高变异性导致过去被试对未来的价值有限，且稳定性-可塑性权衡存在根本性错位

Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p > 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.

</details>


### [78] [HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2601.07870)
*Michal Jan Wlodarczyk,Danzel Serrano,Przemyslaw Musialski*

Main category: cs.LG

TL;DR: 提出HOSC激活函数：tanh(βsin(ω₀x))，通过β参数显式控制Lipschitz界为βω₀，解决传统周期激活的梯度不稳定问题，在INR应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统周期激活函数（如sine）虽然能通过振荡结构保留高频信息，但存在梯度不稳定和对多尺度行为控制有限的问题。需要一种既能保持周期性又能提供更好梯度控制的激活函数。

Method: 提出HOSC激活函数：HOSC(x) = tanh(βsin(ω₀x))，其中β参数显式控制激活函数的Lipschitz界为βω₀。这提供了直接调节梯度幅度的机制，同时保留了周期载波特性。

Result: 在图像、音频、视频、NeRFs和SDFs等多个领域使用标准化训练协议进行综合实证研究。与SIREN、FINER等方法比较，HOSC在某些场景提供显著优势，在其他场景达到竞争性水平。

Conclusion: HOSC是INR应用中实用的周期激活函数，提供了领域特定的超参数选择指导。通过显式控制Lipschitz界，解决了传统周期激活的梯度不稳定问题，同时保持了高频信息保留能力。

Abstract: Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .

</details>


### [79] [Multiplicative Orthogonal Sequential Editing for Language Models](https://arxiv.org/abs/2601.07873)
*Hao-Xiang Xu,Jun-Yu Ma,Ziqi Peng,Yuhao Sun,Zhen-Hua Ling,Jia-Chen Gu*

Main category: cs.LG

TL;DR: MOSE提出了一种新的知识编辑范式：乘性正交序列编辑，通过正交矩阵乘法而非传统加法更新，保持参数矩阵数值稳定性，提升序列编辑性能并保留模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要采用加法范式（在原始参数矩阵上添加更新矩阵），这会损害矩阵的数值稳定性指标（如条件数和范数），特别是在序列编辑场景中，导致编辑性能下降和通用能力损失。虽然后续方法有所改进，但仍未脱离加法框架，未能从根本上解决数值稳定性问题。

Method: 从统计和数学角度分析发现，正交矩阵乘法不会改变矩阵的数值稳定性。基于此，提出乘性编辑范式MOSE：首先推导乘性形式的矩阵更新，然后将新知识编码到正交矩阵中，将该正交矩阵与原始参数矩阵相乘。这种方法保持了编辑后矩阵的数值稳定性不变。

Result: 在三种不同LLM上的实验表明，MOSE能有效限制编辑参数矩阵的偏差并保持其数值稳定性。与现有方法相比，MOSE在序列编辑性能上提升了12.08%，同时在下游任务中保留了95.73%的通用能力。

Conclusion: MOSE通过乘性正交编辑范式，从根本上解决了传统加法编辑方法损害数值稳定性的问题，在保持模型通用能力的同时显著提升了序列编辑性能，为知识编辑领域提供了新的有效解决方案。

Abstract: Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.

</details>


### [80] [E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis](https://arxiv.org/abs/2601.07877)
*Fei Ma,Han Lin,Yifan Xie,Hongwei Ren,Xiaoyu Shen,Wenbo Ding,Qi Tian*

Main category: cs.LG

TL;DR: E^2-LLM是首个用于EEG信号可解释情感分析的多模态大语言模型框架，通过整合预训练EEG编码器和Qwen LLM，采用多阶段训练策略，在情感分类和复杂推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有EEG情感识别方法面临三大挑战：高被试间变异性、标记数据有限、缺乏可解释性推理。虽然多模态大语言模型在情感分析方面有进展，但尚未适应神经信号的独特时空特性。

Method: 提出E^2-LLM框架，整合预训练EEG编码器和基于Qwen的大语言模型，通过可学习投影层连接。采用多阶段训练流程：情感判别预训练、跨模态对齐、带思维链推理的指令微调。设计全面的评估协议，涵盖基本情感预测、多任务推理和零样本场景理解。

Result: 在七种情感类别的数据集上实验表明，E^2-LLM在情感分类上表现优异，更大变体显示出更高的可靠性和对复杂推理场景的优越零样本泛化能力。

Conclusion: 该工作建立了生理信号与LLM推理能力结合的新范式，表明模型缩放不仅能提高识别准确性，还能增强情感计算中的可解释情感理解。

Abstract: Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.

</details>


### [81] [Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models](https://arxiv.org/abs/2601.07878)
*Deyu Cao,Yixin Yin,Samin Aref*

Main category: cs.LG

TL;DR: 提出一种基于切片Wasserstein损失的分布感知校准方法，用于超低位后训练量化，通过分布对齐提升量化模型性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署时存在资源效率低下的经济与环境成本问题，量化是提高效率的关键技术，但4位以下压缩会扭曲激活分布并降低性能

Method: 引入切片Wasserstein损失函数进行分布感知校准，在随机线性投影下对齐全精度和量化模型的输出分布，与标准均方误差损失互补，不增加推理计算开销

Result: 在OmniQuant和TesseraQ两个前沿方法上验证，在多个超低位设置下持续改善困惑度和下游任务准确率，为OmniQuant恢复4.12-20.37%的LLaMA-2-7B精度损失，为TesseraQ恢复3.63-7.63%的相对精度退化

Conclusion: 分布对齐为前沿量化方法提供了简单有效的性能提升，可推动超低位量化技术的发展，方法已在GitHub开源

Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant's lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ's accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.

</details>


### [82] [KVzap: Fast, Adaptive, and Faithful KV Cache Pruning](https://arxiv.org/abs/2601.07891)
*Simon Jegou,Maximilian Jeblick*

Main category: cs.LG

TL;DR: KVzap是一种快速、输入自适应的KV缓存压缩方法，在预填充和解码阶段都能工作，实现了2-4倍的KV缓存压缩且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 随着transformer语言模型上下文长度的增长，KV缓存已成为推理的关键瓶颈。现有的KV缓存剪枝方法由于速度-精度权衡问题尚未被主流推理引擎采用

Method: KVzap是KVzip的快速、输入自适应近似方法，能在预填充和解码阶段工作，通过压缩KV缓存来减少内存占用

Result: 在Qwen3-8B、Llama-3.1-8B-Instruct和Qwen3-32B模型上，KVzap实现了2-4倍的KV缓存压缩，精度损失可忽略，在KVpress排行榜上达到最先进性能

Conclusion: KVzap提供了一种有效的KV缓存压缩解决方案，解决了现有方法的速度-精度权衡问题，有望在实际推理引擎中得到应用

Abstract: Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.

</details>


### [83] [Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification](https://arxiv.org/abs/2601.07892)
*Hong Huang,Decheng Wu,Qiangqiang Hu,Guanghua Yu,Jinhai Yang,Jianchen Zhu,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Sherry提出了一种硬件高效的三值量化框架，通过3:4细粒度稀疏实现1.25位宽度的规则化打包，解决了现有三值量化方法在硬件对齐和推理速度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署受到内存和计算需求的严重限制。现有三值量化方法存在硬件对齐问题：2位对齐打包会导致显著的位浪费，而1.67位不规则打包会降低推理速度。

Method: Sherry框架包含两个关键技术：1) 3:4细粒度稀疏，将四个权重打包到五个比特中，实现1.25位宽度的规则化打包，恢复2的幂次对齐；2) Arenas机制，通过退火残差突触解决稀疏三值训练中的权重陷阱问题，保持训练过程中的表示多样性。

Result: 在LLaMA-3.2模型上的五个基准测试表明，Sherry在保持最先进三值量化性能的同时显著减小模型尺寸。在Intel i7-14700HX CPU上，1B模型相比SOTA基线实现零精度损失，同时提供25%的位节省和10%的速度提升。

Conclusion: Sherry通过创新的硬件对齐三值量化方法，有效解决了边缘设备上LLM部署的内存和计算瓶颈，在保持模型性能的同时实现了显著的存储节省和推理加速。

Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .

</details>


### [84] [Revealing the Attention Floating Mechanism in Masked Diffusion Models](https://arxiv.org/abs/2601.07894)
*Xin Dai,Pengcheng Huang,Zhenghao Liu,Shuo Wang,Yukun Yan,Chaojun Xiao,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.LG

TL;DR: 论文研究了掩码扩散模型中的注意力机制，发现"注意力浮动"现象：与自回归模型不同，MDMs的注意力锚点动态分散且随去噪步骤和层数变化，形成浅层结构感知、深层内容聚焦的机制，这解释了MDMs在上下文学习中的优势。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）在性能上逐渐接近自回归模型（ARMs），但其内部注意力机制尚未得到充分探索。本文旨在深入分析MDMs中的注意力行为，揭示其独特的工作机制。

Method: 通过分析MDMs中的注意力行为，发现"注意力浮动"现象，并进一步揭示其浅层结构感知、深层内容聚焦的注意力机制。对注意力模式进行实证研究，解释MDMs在上下文学习中的能力。

Result: MDMs表现出动态、分散的注意力锚点，随去噪步骤和层数变化。浅层利用浮动标记构建全局结构框架，深层更专注于捕捉语义内容。这种独特的注意力模式使MDMs在知识密集型任务中的性能比ARMs提高一倍。

Conclusion: 注意力浮动现象为MDMs强大的上下文学习能力提供了机制性解释，揭示了其与ARMs不同的注意力工作机制。浅层结构感知、深层内容聚焦的机制是MDMs性能优势的关键因素。

Abstract: Masked diffusion models (MDMs), which leverage bidirectional attention and a denoising process, are narrowing the performance gap with autoregressive models (ARMs). However, their internal attention mechanisms remain under-explored. This paper investigates the attention behaviors in MDMs, revealing the phenomenon of Attention Floating. Unlike ARMs, where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. Further analysis reveals its Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers utilize floating tokens to build a global structural framework, while deeper layers allocate more capability toward capturing semantic content. Empirically, this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks. All codes and datasets are available at https://github.com/NEUIR/Attention-Floating.

</details>


### [85] [Large Language Models and Algorithm Execution: Application to an Arithmetic Function](https://arxiv.org/abs/2601.07898)
*Farah Ben Slama,Frédéric Armetta*

Main category: cs.LG

TL;DR: LLM-DAL训练方法通过专门的监督训练和推理分解，显著提升大语言模型执行复杂算法推理和泛化的能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备统计学习和泛化能力，但在内化处理数据方面存在局限，特别是在自主执行算法方面表现不足。论文旨在探索通过专门的监督训练来扩展模型执行算法的能力。

Method: 提出LLM-DAL（大语言模型-分解算法学习）训练模型，专注于推理分解的专门监督训练，通过精心设计的训练方法引导模型的学习过程。

Result: 研究表明，当训练方法被适当设计以引导模型学习过程时，大语言模型执行复杂算法推理和泛化的能力可以得到显著提升。

Conclusion: 通过LLM-DAL这种专注于推理分解的专门监督训练方法，可以有效扩展大语言模型的算法执行能力，克服其在自主执行算法方面的局限性。

Abstract: Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.

</details>


### [86] [Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning](https://arxiv.org/abs/2601.07903)
*Jianqi Zhang,Jingyao Wang,Wenwen Qiang,Fanjiang Xu,Changwen Zheng*

Main category: cs.LG

TL;DR: 提出LVICL方法，通过向量注入的上下文学习提升大语言模型在时间序列预测中的性能，同时冻结所有LLM参数以减少计算开销


<details>
  <summary>Details</summary>
Motivation: 大语言模型直接应用于时间序列预测存在预训练语料与时间序列数据差异大、预测质量难以保证的问题，而微调LLMs又会产生大量计算开销，因此需要同时解决预测性能和计算开销的双重挑战

Method: 提出LVICL（向量注入的上下文学习）方法：1）使用LLM和可学习的上下文向量适配器从多个示例中自适应提取包含压缩示例信息的上下文向量；2）在前向传播过程中将该向量注入到LLM的每一层以提升预测性能

Result: 广泛实验证明了该方法的有效性，相比传统ICL方法，向量注入ICL不增加提示长度，且能自适应地从示例中推导上下文向量，抑制对预测有害的成分

Conclusion: LVICL方法能够在冻结所有LLM参数的情况下，通过向量注入的上下文学习有效提升大语言模型在时间序列预测中的性能，同时减少计算开销

Abstract: The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [87] [Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation](https://arxiv.org/abs/2601.07935)
*Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: Med-MoE-LoRA：一种结合MoE和LoRA的新框架，用于解决医学领域LLM适应中的稳定性-可塑性困境和任务干扰问题，通过非对称专家分布和知识保护插件实现高效多任务领域适应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型向领域专业化发展面临两大挑战：1) 稳定性-可塑性困境 - 模型需要获取复杂临床知识同时避免灾难性遗忘通用知识；2) 任务干扰 - 医学诊断、报告总结、药物相互作用预测等不同子任务在有限低秩参数空间中相互竞争。

Method: 提出Med-MoE-LoRA框架，集成混合专家(MoE)与低秩适应(LoRA)。采用非对称专家分布，深层配备更高密度的LoRA专家以捕获复杂语义抽象。引入"知识保护插件"（受LoRA MoE启发）隔离和保护通用推理能力。通过软合并、自适应路由和秩级解耦实现多任务适应。

Result: 实验结果表明，该方法在多个临床NLP任务上持续优于标准LoRA和传统MoE架构，同时减少了任务干扰并保留了模型的通用认知能力。

Conclusion: Med-MoE-LoRA通过创新的MoE-LoRA集成和非对称专家设计，有效解决了医学领域LLM适应的关键挑战，在保持通用知识的同时提升了医学任务性能，为领域特定模型适应提供了高效解决方案。

Abstract: The rapid evolution of Large Language Models (LLMs) has shifted focus from general-purpose capabilities to domain-specific expertise. However, adapting LLMs to specialized fields such as medicine presents two challenge: (1) the "Stability-Plasticity Dilemma", where the model must acquire complex clinical knowledge without suffering from catastrophic forgetting of general world knowledge; and (2) "Task Interference", where disparate sub-tasks, such as medical diagnosis, report summarization, and drug-drug interaction prediction, compete for limited low-rank parameter space. In this paper, we propose Med-MoE-LoRA, a novel framework that integrates Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation, especially for medical scenarios. Drawing inspiration from recent advances, our framework employs an asymmetric expert distribution where deeper layers are equipped with a higher density of LoRA experts to capture complex semantic abstractions. We further introduce a "Knowledge-Preservation Plugin", inspired by LoRA MoE, to isolate and protect general-purpose reasoning. By utilizing soft merging with adaptive routing and rank-wise decoupling, Med-MoE-LoRA achieves superior performance in medical benchmarks while reducing interference. Experimental results demonstrate that our approach consistently outperforms standard LoRA and conventional MoE architectures across multiple clinical NLP tasks while retaining the model's general cognitive capabilities.

</details>


### [88] [Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields](https://arxiv.org/abs/2601.07946)
*AmirPouya Hemmasian,Amir Barati Farimani*

Main category: cs.LG

TL;DR: DiffCoder：结合扩散模型与卷积编码器的流场重建框架，在强压缩下比传统VAE更好地保持流场的高阶统计特性


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）在强压缩流场数据时难以保持高阶统计结构，需要一种能同时保证压缩效率和统计保真度的新方法

Method: 提出DiffCoder框架，将概率扩散模型与传统卷积ResNet编码器耦合进行端到端训练。编码器压缩流场到潜在表示，扩散模型学习基于压缩状态的生成先验来重建流场

Result: 在Kolmogorov流场数据集上，强压缩时DiffCoder显著改善谱精度，而VAE严重退化。两者L2重建误差相当，但DiffCoder更好地保持了流场的分布结构。中等压缩时，足够大的VAE仍有竞争力

Conclusion: 扩散模型的生成解码为复杂流场的紧凑、统计一致表示提供了有前景的路径，尤其在信息瓶颈严重时优势明显

Abstract: Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.

</details>


### [89] [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)
*Amir Eskandari,Aman Anand,Elyas Rashno,Farhana Zulkernine*

Main category: cs.LG

TL;DR: InfGraND：一种基于节点结构影响力的GNN到MLP知识蒸馏框架，通过识别关键节点和预计算多跳邻域特征来提升MLP性能


<details>
  <summary>Details</summary>
Motivation: GNN依赖聚合和更新操作，在低延迟推理或资源受限场景中面临挑战。MLP计算效率高但监督训练性能不佳。现有知识蒸馏方法要么均匀转移知识，要么依赖图无关指标，忽略了节点对图结构重要性的根本问题。

Method: 提出InfGraND框架：1）识别并优先处理结构上有影响力的节点来指导蒸馏过程；2）通过一次性多跳邻域特征预计算将结构感知嵌入MLP中，避免推理时开销；3）在GNN教师和MLP学生之间进行影响力引导的知识蒸馏。

Result: 在七个同配性图基准数据集上的转导和归纳设置中进行严格评估，InfGraND始终优于先前的GNN到MLP知识蒸馏方法，证明了其在现实世界延迟关键应用中的实用性。

Conclusion: InfGraND通过关注节点对图结构的重要性，有效提升了MLP在图数据上的性能，为低延迟推理场景提供了实用的解决方案，同时避免了推理时的计算开销。

Abstract: Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally efficient alternative. Yet, training an MLP in a supervised setting often leads to suboptimal performance. Knowledge Distillation (KD) from a GNN teacher to an MLP student has emerged to bridge this gap. However, most KD methods either transfer knowledge uniformly across all nodes or rely on graph-agnostic indicators such as prediction uncertainty. We argue this overlooks a more fundamental, graph-centric inquiry: "How important is a node to the structure of the graph?" We introduce a framework, InfGraND, an Influence-guided Graph KNowledge Distillation from GNN to MLP that addresses this by identifying and prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph. Additionally, InfGraND embeds structural awareness in MLPs through one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead. Our rigorous evaluation in transductive and inductive settings across seven homophilic graph benchmark datasets shows InfGraND consistently outperforms prior GNN to MLP KD methods, demonstrating its practicality for numerous latency-critical applications in real-world settings.

</details>


### [90] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 该论文研究了黎曼流形上零阶优化问题，针对测地不完备的黎曼度量，通过构造结构保持的完备度量，建立了内在的零阶估计器收敛性分析框架。


<details>
  <summary>Details</summary>
Motivation: 传统黎曼优化方法通常假设流形是测地完备的，但在实际应用中许多重要的黎曼流形（如某些矩阵流形、形状空间等）具有测地不完备的度量。这导致现有优化理论无法直接应用，需要开发能够处理不完备度量的优化框架。

Method: 1. 构造结构保持的完备度量：对于给定的测地不完备度量g，构造一个新的测地完备度量g'，使得在原度量下的驻点在新度量下仍然是驻点。2. 内在零阶估计器分析：重新审视经典对称两点零阶估计器，从纯粹内在的几何角度分析其均方误差，不依赖于任何环境嵌入。3. 收敛性保证：基于内在分析，建立使用该内在估计器的随机梯度下降的收敛性保证。

Result: 1. 理论结果：在适当条件下，新度量g'下的ε-驻点对应于原度量g下的ε-驻点，从而在测地不完备情况下达到了与完备情况相同的最佳已知复杂度。2. 实验验证：在合成问题上的实验证实了理论发现，在实际网格优化任务中的实验表明，即使在缺乏测地完备性的情况下，该框架仍能保持稳定的收敛性。

Conclusion: 该论文提出了一个处理测地不完备黎曼流形上零阶优化问题的系统框架，通过构造结构保持的完备度量和内在的零阶估计器分析，成功地将优化理论扩展到不完备度量空间，为实际应用中的复杂几何优化问题提供了理论基础和实用工具。

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [91] [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: 提出基于查找表(LUT)编译的KAN模型，用于资源受限IoT设备的DoS攻击检测，在保持高准确率的同时显著降低推理延迟


<details>
  <summary>Details</summary>
Motivation: DoS攻击对IoT生态系统构成严重威胁，但在资源受限的边缘设备上部署有效的入侵检测系统具有挑战性。KANs虽然参数更少，但运行时B样条计算引入显著计算开销，不适合延迟敏感的IoT应用

Method: 提出查找表(LUT)编译流水线，用预计算的量化表和线性插值替代昂贵的样条计算；构建轻量级KAN模型(50K参数，0.19MB)；在CICIDS2017 DoS数据集上评估，并全面分析LUT分辨率、量化方案和越界策略

Result: 轻量级KAN模型达到99.0%准确率；LUT编译后(L=8)保持98.96%准确率(F1退化<0.0004)；在批量大小256时获得68倍加速，批量大小1时超过5000倍加速，仅2倍内存开销；建立了准确率-延迟-内存权衡的帕累托前沿

Conclusion: LUT编译的KANs能够在仅CPU的IoT网关上实现实时DoS检测，具有确定性推理延迟和最小资源占用，为资源受限IoT环境提供了可行的入侵检测解决方案

Abstract: Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate spline functions on edges rather than fixed activations on nodes, achieving competitive accuracy with fewer parameters. However, runtime B-spline evaluation introduces significant computational overhead unsuitable for latency-critical IoT applications. We propose a lookup table (LUT) compilation pipeline that replaces expensive spline computations with precomputed quantized tables and linear interpolation, dramatically reducing inference latency while preserving detection quality. Our lightweight KAN model (50K parameters, 0.19~MB) achieves 99.0\% accuracy on the CICIDS2017 DoS dataset. After LUT compilation with resolution $L=8$, the model maintains 98.96\% accuracy (F1 degradation $<0.0004$) while achieving $\mathbf{68\times}$ speedup at batch size 256 and over $\mathbf{5000\times}$ speedup at batch size 1, with only $2\times$ memory overhead. We provide comprehensive evaluation across LUT resolutions, quantization schemes, and out-of-bounds policies, establishing clear Pareto frontiers for accuracy-latency-memory trade-offs. Our results demonstrate that LUT-compiled KANs enable real-time DoS detection on CPU-only IoT gateways with deterministic inference latency and minimal resource footprint.

</details>


### [92] [Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment](https://arxiv.org/abs/2601.08089)
*Qitao Tan,Xiaoying Song,Ningxi Cheng,Ninghao Liu,Xiaoming Zhai,Lingzi Hong,Yanzhi Wang,Zhen Xiang,Geng Yuan*

Main category: cs.LG

TL;DR: Q-realign：一种基于后训练量化的后处理防御方法，通过将量化重构为压缩和安全性的双重目标过程，在保持任务性能的同时大幅减少微调LLM的不安全行为


<details>
  <summary>Details</summary>
Motivation: 公共大语言模型通常在预训练阶段进行安全对齐，但部署所需的任务特定微调往往会破坏这种对齐并引入安全风险。现有防御方法要么将安全恢复嵌入微调过程，要么依赖微调先验进行后处理校正，导致安全恢复与训练紧密耦合，计算开销大且工作流程复杂。

Method: 提出Q-realign方法，基于后训练量化，通过表征结构分析指导。将量化重构为压缩和安全性的双重目标过程，使安全对齐与微调解耦，并自然融入现代部署流程。该方法利用量化过程中的精度降低来恢复模型的安全对齐。

Result: 在多个模型和数据集上的实验表明，该方法能显著减少不安全行为同时保持任务性能，内存使用和GPU时间大幅降低。特别地，该方法能在单张RTX 4090上40分钟内恢复7B LLM微调后的安全对齐。

Conclusion: Q-realign提供了一种实用、即插即用的安全感知部署解决方案，通过后训练量化实现安全对齐恢复，避免了与微调过程的紧密耦合，降低了计算开销和流程复杂性。

Abstract: Public large language models (LLMs) are typically safety-aligned during pretraining, yet task-specific fine-tuning required for deployment often erodes this alignment and introduces safety risks. Existing defenses either embed safety recovery into fine-tuning or rely on fine-tuning-derived priors for post-hoc correction, leaving safety recovery tightly coupled with training and incurring high computational overhead and a complex workflow. To address these challenges, we propose \texttt{Q-realign}, a post-hoc defense method based on post-training quantization, guided by an analysis of representational structure. By reframing quantization as a dual-objective procedure for compression and safety, \texttt{Q-realign} decouples safety alignment from fine-tuning and naturally piggybacks into modern deployment pipelines. Experiments across multiple models and datasets demonstrate that our method substantially reduces unsafe behaviors while preserving task performance, with significant reductions in memory usage and GPU hours. Notably, our approach can recover the safety alignment of a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes. Overall, our work provides a practical, turnkey solution for safety-aware deployment.

</details>


### [93] [Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition](https://arxiv.org/abs/2601.08094)
*Zheng Zhou,Isabella McEvoy,Camilo E. Valderrama*

Main category: cs.LG

TL;DR: 提出融合局部通道级和全局试次级描述符的双分支Transformer框架，用于提升跨被试EEG情绪识别的泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决被试独立EEG情绪识别中的跨被试变异性和从短时噪声记录中学习鲁棒表示的挑战

Method: 融合局部通道级描述符（差分熵+图论特征）和全局试次级描述符（时域、频域、复杂度特征），采用双分支Transformer架构，结合注意力融合和域对抗正则化，并基于强度阈值过滤样本

Result: 在SEED-VII数据集上，采用留一被试交叉验证，该方法在7类被试独立情绪识别中达到约40%的平均准确率，优于单视图和经典基线方法

Conclusion: 融合局部和全局描述符的框架能有效提升跨被试EEG情绪识别的泛化性能，为解决被试间变异性问题提供了有效方案

Abstract: Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.

</details>


### [94] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: STO-RL：利用LLM生成时序子目标序列的离线强化学习框架，通过势能奖励塑形解决稀疏奖励长时程任务


<details>
  <summary>Details</summary>
Motivation: 传统离线RL在处理稀疏奖励的长时程任务时效果不佳，现有目标条件和分层方法忽略了子目标间的时序依赖关系，依赖不精确的奖励塑形，导致策略次优。

Method: 提出STO-RL框架：1) 利用LLM生成时序有序的子目标序列和状态到子目标阶段的映射；2) 基于此时序结构应用势能奖励塑形，将稀疏的终端奖励转化为密集、时序一致的信号；3) 使用增强的数据集进行离线训练。

Result: 在四个离散和连续稀疏奖励基准测试中，STO-RL始终优于最先进的离线目标条件和分层RL基线，实现更快的收敛速度、更高的成功率和更短的轨迹。消融研究证实了其对不完美或噪声LLM生成子目标序列的鲁棒性。

Conclusion: LLM引导的子目标时序结构与理论基础的奖励塑形相结合，为长时程离线RL提供了实用且可扩展的解决方案，能够有效处理稀疏奖励任务中的时序依赖问题。

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [95] [Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions](https://arxiv.org/abs/2601.08121)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 研究XGBoost中列抽样对合成比率特征能力的影响：当数据存在比率结构时，列抽样会降低模型性能，但加入显式比率特征可消除此影响。


<details>
  <summary>Details</summary>
Motivation: 许多实际问题中的信号需要组合多个原始测量值才能显现（如比率和速率）。在梯度提升树中，这种组合不是显式操作，模型需要通过协调的分裂来合成。研究XGBoost中的列抽样是否使这种合成变得更困难。

Method: 使用两种具有抵消结构的数据生成过程：两个原始特征共享强干扰因子，而目标取决于较小的差异因子。通过log比率抵消干扰并提取信号。在XGBoost中变化colsample_bylevel和colsample_bynode参数（s∈{0.4,0.6,0.8,0.9}），重点研究轻度抽样（s≥0.8）。对照组包含显式工程化的比率特征。

Result: 在仅使用原始特征的设置中，列抽样降低了测试PR-AUC。主要过程中，当两个参数都设为0.4时，相对下降达到54%。当包含工程化比率特征时，这种影响基本消失。基于路径的共使用度量在性能恶化的相同单元格中下降。

Conclusion: 如果数据可能存在比率类结构，应避免使用列抽样或包含预期的比率特征。列抽样会阻碍模型合成比率特征的能力，从而降低性能。

Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s >= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.

</details>


### [96] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: 提出统一框架RFM，将噪声期望和梯度期望两种扩散策略训练方法统一为更一般的后验均值估计问题，通过引入Langevin Stein算子降低重要性采样方差，提升在线强化学习中流策略的训练效率。


<details>
  <summary>Details</summary>
Motivation: 扩散和流策略在在线强化学习中因表达能力强而受到关注，但缺乏直接目标分布样本导致训练困难。现有噪声期望和梯度期望两种方法关系不明确，需要统一框架来理解其联系并提升训练效率。

Method: 提出反向流匹配(RFM)框架，采用反向推断视角，将训练目标形式化为给定中间噪声样本的后验均值估计问题。引入Langevin Stein算子构造零均值控制变量，推导出能有效降低重要性采样方差的通用估计器类别。

Result: 证明现有噪声期望和梯度期望方法是RFM框架的特例，该统一视角实现了两个关键进展：将玻尔兹曼分布目标能力从扩散策略扩展到流策略，并能结合Q值和Q梯度信息推导出最优最小方差估计器。

Conclusion: RFM为训练无直接目标样本的扩散和流模型提供了统一框架，在连续控制基准测试中相比扩散策略基线表现出改进性能，提升了训练效率和稳定性。

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [97] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: 提出Resistance Curvature Flow (RCF)框架，利用电路物理中的有效电阻概念替代传统基于最优传输的曲率流，实现100倍以上计算加速，并开发了DGSL-RCF图优化算法，在多种学习任务中显著提升表示质量和下游性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于欧氏距离的静态图构建方法难以捕捉数据流形的内在曲率特征，而现有的Ollivier-Ricci曲率流(OCF)虽然能进行动态拓扑优化，但其核心依赖最优传输(Wasserstein距离)导致计算复杂度极高，限制了在大规模数据集和深度学习框架中的应用。

Method: 提出Resistance Curvature Flow (RCF)几何演化框架，利用电路物理中的有效电阻概念，将昂贵的曲率优化转化为高效的矩阵运算。基于此框架设计了图优化算法DGSL-RCF，通过曲率梯度引导边权重的重新分布，消除拓扑噪声并增强局部聚类结构。

Result: RCF实现了超过100倍的计算加速，同时保持了与OCF相当的几何优化能力。在深度度量学习、流形学习和图结构学习等实验中，DGSL-RCF显著提升了表示质量和下游任务性能。

Conclusion: RCF框架成功解决了传统曲率流计算复杂度高的问题，为大规模数据集的几何表示学习提供了高效解决方案，同时阐明了其在流形增强和噪声抑制方面的机制，以及与深度学习模型的兼容性。

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [98] [TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations](https://arxiv.org/abs/2601.08181)
*Aviral Gupta,Armaan Sethi,Dhruv Kumar*

Main category: cs.LG

TL;DR: 该论文通过探测实验分析了表格基础模型内部表示的信息编码，发现模型在隐藏层中存储了线性回归系数、复杂表达式的中间值等结构化信息，揭示了模型如何逐步处理输入特征并生成最终预测。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型虽然在多个领域表现出色，但其内部表示和学习的概念仍不明确。这种缺乏可解释性的问题使得研究这些模型如何处理和转换输入特征变得重要，以增强模型的透明度和可信度。

Method: 通过一系列探测实验分析模型隐藏表示中的信息编码：测试线性回归系数的存在性、复杂表达式的中间值以及早期层中的最终答案。这些实验允许推理模型内部执行的计算过程。

Result: 结果表明表格基础模型的表示中存储了有意义的结构化信息。观察到清晰的信号对应模型预测过程中涉及的中间量和最终量，揭示了模型如何细化输入以及最终输出如何形成。

Conclusion: 研究发现表格基础模型编码了具体且可解释的信息，这有助于更深入地理解这些模型的内部机制，使决策过程更加透明和可信，向提高模型可解释性迈出了重要一步。

Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.

</details>


### [99] [One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation](https://arxiv.org/abs/2601.08216)
*Zahir Alsulaimawi*

Main category: cs.LG

TL;DR: 本文提出了一种用于联邦岭回归的单次通信协议，通过客户端计算并传输局部充分统计量（Gram矩阵和矩向量），服务器通过单次矩阵求逆重构全局解，避免了迭代通信。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习协议需要客户端与中央服务器之间反复同步，收敛速度受学习率、数据异质性和客户端采样影响。本文探讨迭代通信对于分布式线性回归是否必要，旨在减少通信开销。

Method: 将联邦岭回归建模为分布式均衡问题：每个客户端计算局部充分统计量（Gram矩阵和矩向量）并单次传输；服务器通过单次矩阵求逆重构全局解。对于高维场景，提出随机投影技术将通信复杂度从O(d²)降至O(m²)。建立差分隐私保证，噪声单次注入每个客户端。

Result: 在客户端特征矩阵满足覆盖条件下，单次聚合可精确恢复集中式岭解；对于违反覆盖条件的异质分布，推导了依赖于聚合Gram矩阵谱性质的非渐近误差界。实验表明单次融合匹配FedAvg精度，同时减少高达38倍通信量。

Conclusion: 迭代通信对于分布式线性回归非必需；单次通信协议在精确恢复、通信效率、隐私保护方面具有优势，适用于核方法和随机特征模型，但不适用于一般非线性架构。

Abstract: Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\mathcal{O}(Rd)$ in iterative methods to $\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\mathcal{O}(m^2)$ where $m \ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.

</details>


### [100] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 论文提出了一种基于智能体的矩阵降秩方法，使用LLM生成秩-1 SVD更新，VLM接受/拒绝更新并决定停止时机，无需固定阈值。


<details>
  <summary>Details</summary>
Motivation: 探索智能体系统能否替代传统数值算法进行矩阵分解，实现无需固定阈值的自适应降秩过程。

Method: 采用双智能体架构：1) 求解器LLM生成秩-1 SVD更新；2) VLM评估更新并决定停止时机。通过上下文学习和行列置换提高稳定性。

Result: 在Digits、CIFAR-10和合成矩阵上评估，合成噪声案例中最佳配置与目标仅差1.75 RMSE。所有设置下均取得竞争性结果。

Conclusion: 完全智能体化、无阈值的降秩方法是传统数值算法的可行替代方案。

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [101] [Hyperbolic Heterogeneous Graph Transformer](https://arxiv.org/abs/2601.08251)
*Jongmin Park,Seunghoon Han,Hyewon Lee,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出双曲异构图变换器HypHGT，直接在双曲空间中学习异构图表示，通过基于变换器的架构和关系特定的双曲注意力机制，有效捕获局部和全局依赖关系，在节点分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双曲异构图学习方法存在两个主要问题：1）严重依赖切空间操作，导致频繁转换中的映射失真；2）消息传递架构主要关注局部邻域信息，难以捕获全局层次结构和不同类型节点间的长程依赖关系。

Method: 提出双曲异构图变换器HypHGT，完全在双曲空间中学习异构图表示。采用基于变换器的架构自然捕获局部和全局依赖，设计关系特定的双曲注意力机制，具有线性时间复杂度，能高效计算并保留不同关系类型的异构信息。

Result: 在节点分类任务上的综合实验表明，HypHGT始终优于最先进的方法，同时显著减少了训练时间和内存使用，证明了其有效性和效率。

Conclusion: HypHGT通过直接在双曲空间中操作，避免了切空间操作带来的失真问题，并通过变换器架构和关系特定的双曲注意力机制，有效捕获了异构图中的复杂结构特性和语义信息，为异构图学习提供了更高效和有效的解决方案。

Abstract: In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.

</details>


### [102] [On Evaluation of Unsupervised Feature Selection for Pattern Classification](https://arxiv.org/abs/2601.08257)
*Gyu-Il Kim,Dae-Won Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: 该论文重新评估无监督特征选择方法的评价范式，指出基于单标签分类的评估存在局限性，提出采用多标签分类框架进行更公平可靠的比较。


<details>
  <summary>Details</summary>
Motivation: 现有无监督特征选择方法大多使用单标签数据集进行评估，但单标签是从多标签数据中任意选择的，这导致方法优劣排名可能因所选标签不同而变化，基于单标签准确率的评估无法真实反映方法的判别能力。

Method: 采用多标签分类框架重新评估无监督特征选择方法，在21个多标签数据集上对多种代表性方法进行实验，比较它们在多标签设置下的性能表现。

Result: 实验结果表明，在多标签评估设置下，各方法的性能排名与单标签设置下报告的排名存在显著差异，说明多标签评估能提供更公平可靠的比较。

Conclusion: 基于单标签准确率的评估范式存在缺陷，多标签评估设置能够更公平可靠地比较无监督特征选择方法的真实判别能力，建议采用多标签框架进行方法评估。

Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.

</details>


### [103] [Demystifying the Slash Pattern in Attention: The Role of RoPE](https://arxiv.org/abs/2601.08297)
*Yuan Cheng,Fengzhuo Zhang,Yunlong Hou,Cunxiao Du,Chao Du,Tianyu Pang,Aixin Sun,Zhuoran Yang*

Main category: cs.LG

TL;DR: 论文揭示了大型语言模型中斜线注意力模式（Slash-Dominant Heads）的内在形成机制，通过实证和理论分析表明这些模式源于查询/键向量的低秩特性与RoPE位置编码中高频成分的相互作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中普遍存在的斜线注意力模式（注意力分数集中在Δ次对角线上）在跨令牌信息传递中起关键作用，但其形成原因尚不明确。本文旨在从实证和理论角度揭示这些斜线主导头（SDHs）的内在形成机制。

Method: 采用双轨研究方法：1）实证分析开源LLMs，发现SDHs的普遍性和泛化性；2）分析查询、键向量和RoPE位置编码的相互作用，识别SDHs的两个特征条件；3）理论建模，在浅层Transformer中形式化这些条件，通过梯度下降训练动态证明SDHs的必然出现。

Result: 发现SDHs是模型内在特性，能泛化到分布外提示。实证识别出SDHs的两个关键条件：查询/键向量几乎为秩一矩阵，RoPE由中高频成分主导。理论证明在这些条件下，梯度下降训练必然产生SDHs，且具有泛化能力。

Conclusion: 斜线注意力模式的形成源于查询/键向量的低秩特性与RoPE位置编码中高频成分的系统性相互作用。这种机制是Transformer架构的内在特性，而非特定训练数据的产物，为理解LLMs的注意力机制提供了理论基础。

Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.

</details>


### [104] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT：一个可控的多预算推理框架，通过输入触发分离的推理模式，避免统一应用过长推理链带来的不必要计算成本


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(LRMs)通过长形式思维链推理获得强性能，但在推理时统一应用过长推理链会产生大量且通常不必要的计算成本。先前工作尝试从输入推断适当的推理预算，但这种方法在最坏情况下不可靠，因为估计最小所需推理努力本质上很困难，且它们在训练期间隐式固定了推理成本与准确性的权衡，限制了在不同部署场景下的灵活性。

Method: ORBIT采用多阶段强化学习来发现每个推理努力水平下的帕累托最优推理行为，然后通过策略蒸馏将这些行为融合到单个统一模型中，实现通过输入触发的可控多预算推理。

Result: 实验表明ORBIT实现了：(1) 跨多个模式的可控推理行为；(2) 每个模式内具有竞争力的推理密度；(3) 将这些前沿策略集成到单个统一学生模型中，同时保持清晰的模式分离和每个模式的高性能。

Conclusion: ORBIT框架解决了现有推理模型在计算成本与准确性权衡方面的局限性，提供了灵活可控的多预算推理方案，能够在不同部署场景下优化推理效率。

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [105] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: 该研究评估了通用和放射组学专用AutoML框架在放射组学分类任务中的性能、效率和可访问性，发现Simplatab在性能与可访问性之间达到最佳平衡，但现有框架仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: AutoML框架可以降低放射组学预测模型开发的技术门槛，但其在解决放射组学特定挑战方面的有效性尚不明确。本研究旨在评估通用和放射组学专用AutoML框架在多样化放射组学分类任务中的表现，以明确放射组学领域的发展需求。

Method: 使用10个公共/私有放射组学数据集（涵盖CT/MRI多种成像模态、不同规模、解剖部位和终点），测试了6个通用框架和5个放射组学专用框架。采用预定义参数和标准化交叉验证，评估指标包括AUC、运行时间，以及软件状态、可访问性和可解释性等定性方面。

Result: Simplatab（放射组学专用工具，无代码界面）获得最高平均测试AUC（81.81%），运行时间适中（约1小时）。LightAutoML（通用框架）执行最快，性能具有竞争力（平均AUC 78.74%，仅需6分钟）。大多数放射组学专用框架因过时、编程要求高或计算效率低而被排除在性能分析之外。通用框架表现出更高的可访问性和易实施性。

Conclusion: Simplatab为放射组学分类问题提供了性能、效率和可访问性的有效平衡。然而，现有AutoML框架仍存在显著不足，包括缺乏可访问的生存分析支持，以及特征可重复性和协调性整合有限。未来研究应着重使AutoML解决方案更好地应对这些放射组学特定挑战。

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [106] [Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings](https://arxiv.org/abs/2601.08358)
*Hilde I. Hummel,Sandjai Bhulai,Rob D. van der Mei,Burooj Ghani*

Main category: cs.LG

TL;DR: 首次对水下声学目标识别进行迁移学习的实证比较研究，评估多种预训练音频模型，发现线性探测能有效抑制录音特定特征并分离船型特征，实现低计算成本的自动UATR。


<details>
  <summary>Details</summary>
Motivation: 船舶人为噪声增加导致水下声污染，威胁海洋生态系统，需要监测船舶辐射噪声。被动声学监测系统产生大量数据，手动分析不现实，需要基于机器学习的自动化方法。现有监督学习方法受限于标记数据稀缺，迁移学习为解决此限制提供了有前景的替代方案。

Method: 首次对UATR进行迁移学习的实证比较研究，评估多个来自不同音频领域的预训练音频模型。冻结预训练模型权重，通过分类、聚类和相似性评估分析生成的嵌入表示。使用线性探测方法从嵌入中分离船型特征。

Result: 分析显示嵌入空间的几何结构主要由录音特定特征主导。然而，简单的线性探测能有效抑制这些录音特定信息，并从嵌入中分离出船型特征。线性探测使预训练音频模型能够以低计算成本实现有效的自动UATR，显著减少对大量高质量标记船舶录音的需求。

Conclusion: 迁移学习为水下声学目标识别提供了有效的解决方案，线性探测方法能有效利用预训练音频模型，在标记数据有限的情况下实现高性能的船舶类型识别，为海洋声学监测提供了实用的自动化工具。

Abstract: Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.

</details>


### [107] [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)
*Tian Xie,Haoming Luo,Haoyu Tang,Yiwen Hu,Jason Klein Liu,Qingnan Ren,Yang Wang,Wayne Xin Zhao,Rui Yan,Bing Su,Chong Luo,Baining Guo*

Main category: cs.LG

TL;DR: SSO（Spectral Sphere Optimizer）是一种新的优化器，通过强制权重及其更新的模块级谱约束，实现完全μP对齐的优化过程，在多种架构上优于AdamW和Muon。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Muon仅部分满足μP（Maximal Update Parametrization）理论要求，它们控制更新但允许权重漂移，缺乏对权重本身的谱约束，限制了大规模模型训练的稳定性和收敛性。

Method: 提出Spectral Sphere Optimizer（SSO），通过推导谱球面上的最速下降方向，对权重及其更新施加严格的模块级谱约束，实现完全μP对齐的优化过程，并在Megatron中实现为高效的并行算法。

Result: 在Dense 1.7B、MoE 8B-A1B和200层DeepNet等多种架构上进行大规模预训练，SSO始终优于AdamW和Muon，并展现出显著的实际稳定性优势，包括改进的MoE路由器负载均衡、抑制异常值和严格有界的激活。

Conclusion: SSO通过强制权重和更新的谱约束，实现了完全μP对齐的优化，为大规模模型训练提供了更稳定、高效的优化策略，解决了现有优化器仅部分满足理论约束的问题。

Abstract: Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.

</details>


### [108] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: Taxon是一个用于电商平台税务编码预测的语义对齐专家引导框架，通过多模态特征门控专家混合架构和LLM蒸馏的语义一致性模型，结合多源训练数据，在阿里巴巴税务系统中实现高精度部署。


<details>
  <summary>Details</summary>
Motivation: 税务编码预测是电商平台自动开票和合规管理的关键但未充分探索的任务。产品需要准确映射到国家标准定义的多层次分类体系中，错误会导致财务不一致和监管风险。现有方法在处理真实业务中的噪声监督和语义对齐方面存在不足。

Method: Taxon框架包含：(1) 特征门控专家混合架构，自适应路由多模态特征到分类体系不同层级；(2) 从大语言模型蒸馏的语义一致性模型，作为领域专家验证产品标题与官方税务定义的对齐；(3) 多源训练管道，结合税务数据库、发票验证日志和商家注册数据，提供结构和语义监督；(4) 完整层次路径重建程序提升结构一致性。

Result: 在专有TaxCode数据集和公共基准测试中，Taxon实现了最先进的性能，优于强基线。完整层次路径重建显著提高了结构一致性，获得最高的整体F1分数。Taxon已在阿里巴巴税务服务系统中部署，平均每天处理超过50万次税务编码查询，业务高峰期超过500万次请求，提高了准确性、可解释性和鲁棒性。

Conclusion: Taxon通过语义对齐和专家引导的框架，有效解决了电商平台税务编码预测中的噪声监督和语义对齐问题。该框架在实际生产环境中表现出色，为大规模电商平台的自动化税务合规管理提供了实用解决方案。

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [109] [Coverage Improvement and Fast Convergence of On-policy Preference Learning](https://arxiv.org/abs/2601.08421)
*Juno Kim,Jihun Yun,Jason D. Lee,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 在线策略偏好学习算法（如在线DPO）在语言模型对齐中显著优于离线方法，本文通过分析采样策略覆盖度的演化提供理论解释，提出"覆盖度改进原则"，证明了在线DPO的指数收敛性，并提出基于优先G-最优设计的混合采样器实现两轮收敛。


<details>
  <summary>Details</summary>
Motivation: 在线策略偏好学习算法（如在线DPO）在语言模型对齐中表现出比离线方法更好的性能，但缺乏理论解释。本文旨在从理论上解释这一现象，分析采样策略覆盖度在在线训练中的演化机制。

Method: 1. 提出并理论证明"覆盖度改进原则"：在足够大的批次大小下，每次更新都会移动到目标区域附近，使覆盖度均匀改善；2. 在具有Bradley-Terry偏好和线性softmax策略类的上下文赌博机设置中，分析在线DPO的收敛性；3. 提出基于优先G-最优设计的混合采样器，消除对覆盖度的依赖；4. 为一般函数类设置开发原则性的在线策略奖励蒸馏方案。

Result: 1. 证明了在线DPO在批次大小超过广义覆盖度阈值时，迭代次数呈指数收敛；2. 证明了任何限制于初始策略离线采样的学习器都面临较慢的极小极大速率，导致总样本复杂度的明显分离；3. 提出的混合采样器能在两轮内保证收敛；4. 在一般函数类设置下，在线策略奖励蒸馏算法在替代的偏差基覆盖度概念下获得更快的无噪声速率；5. 实验证实在线DPO和提出的奖励蒸馏算法优于离线对应方法，且性能增益稳定单调。

Conclusion: 在线策略偏好学习算法在语言模型对齐中的优越性能源于采样策略覆盖度的系统性改善，本文提出的理论框架和算法设计为在线对齐方法提供了坚实的理论基础和实用工具，混合采样器设计进一步提升了收敛效率。

Abstract: Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.

</details>


### [110] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: DiffMM：基于编码器-扩散框架的稀疏轨迹地图匹配方法，通过一步扩散过程实现高效准确匹配


<details>
  <summary>Details</summary>
Motivation: 现有基于隐马尔可夫模型或编码器-解码器框架的地图匹配方法在处理噪声或稀疏采样的GPS轨迹时面临显著挑战，需要更有效的解决方案

Method: 提出DiffMM框架：1) 道路段感知轨迹编码器，通过注意力机制将输入轨迹及其周围候选道路段嵌入共享潜在空间；2) 一步扩散方法，利用轨迹和候选道路段的联合嵌入作为条件上下文，通过捷径模型实现地图匹配

Result: 在大规模轨迹数据集上的实验表明，该方法在准确性和效率方面均优于现有最先进的地图匹配方法，特别是在稀疏轨迹和复杂路网拓扑情况下表现突出

Conclusion: DiffMM通过编码器-扩散框架有效解决了稀疏轨迹地图匹配问题，为交通调度和交通流分析等应用提供了更准确高效的解决方案

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [111] [Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care](https://arxiv.org/abs/2601.08503)
*Aditya Kumar,Simon Rauch,Mario Cypko,Marcel Naik,Matthieu-P Schapranow,Aadil Rashid,Fabian Halleck,Bilgin Osmanodja,Roland Roller,Lars Pape,Klemens Budde,Mario Schiffer,Oliver Amft*

Main category: cs.LG

TL;DR: TFN是一种多模态、任务无关的嵌入模型，用于整合不规则时间序列和非结构化临床叙事，在肾移植后护理中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 临床环境中存在异构数据源（不规则时间序列和丰富叙事文档），需要能够整合这些多模态数据的通用模型来改善临床预测任务。

Method: 开发了Temporal Fusion Nexus (TFN)模型，这是一个多模态、任务无关的嵌入模型，专门设计用于整合不规则时间序列和非结构化临床叙事数据。

Result: 在3382名患者的肾移植后护理队列中，TFN在移植物丢失（AUC 0.96 vs 0.94）、移植物排斥（AUC 0.84 vs 0.74）和死亡率预测（AUC 0.86）方面优于现有最佳模型，比单模态基线提升约5-10% AUC。

Conclusion: TFN通过整合临床文本显著提升预测性能，具有可解释的潜在因子，且可推广到其他具有异构数据源和不规则纵向数据的临床任务中。

Abstract: We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.

</details>


### [112] [Your Group-Relative Advantage Is Biased](https://arxiv.org/abs/2601.08521)
*Fengkai Yang,Zherui Chen,Xiaohan Wang,Xiaodong Lu,Jiajun Chai,Guojun Yin,Wei Lin,Shuai Ma,Fuzhen Zhuang,Deqing Wang,Yaodong Yang,Jianxin Li,Yikun Ban*

Main category: cs.LG

TL;DR: 本文揭示了基于群体的强化学习（RLVR）中群体相对优势估计存在固有偏差的问题，并提出历史感知自适应难度加权（HA-DW）方法进行校正，在五个数学推理基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于群体的RLVR方法（如GRPO及其变体）广泛用于大语言模型推理任务的后训练，但其核心的群体相对优势估计器的理论特性尚不明确。研究发现该估计器存在固有偏差，导致对困难提示的优势低估和对简单提示的优势高估，造成探索与利用不平衡。

Method: 提出历史感知自适应难度加权（HA-DW）方法，这是一种自适应重加权方案，基于演化难度锚点和训练动态调整优势估计。该方法通过校正偏差来改善群体相对优势估计，可集成到GRPO及其变体中。

Result: 理论分析和在五个数学推理基准上的实验表明，HA-DW在集成到GRPO及其变体后能持续提升性能。结果证实校正偏差优势估计对于鲁棒高效的RLVR训练至关重要。

Conclusion: 基于群体的RL方法中的群体相对优势估计器存在固有偏差，HA-DW通过自适应难度加权有效校正这一偏差，为RLVR训练提供了更稳健和高效的解决方案。

Abstract: Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

</details>


### [113] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M²FMoE：一种通过多分辨率多视图频率建模学习常规和极端模式的极端自适应预测模型，在具有极端模式的水文数据集上优于现有方法且无需极端事件标签。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理极端事件时性能显著下降，主要原因是它们擅长建模主导的常规模式，但难以捕捉极端事件的高方差、不规则动态和稀疏但高影响的特性。虽然一些方法引入辅助信号来改进性能，但仍无法捕捉极端事件的复杂时间动态。

Method: M²FMoE包含三个核心模块：1）多视图频率混合专家模块：在傅里叶和小波域为不同频谱带分配专家，通过跨视图共享带分割器对齐频率分区，实现专家间协作以捕捉主导和罕见波动；2）多分辨率自适应融合模块：从粗到细分辨率层次聚合频率特征，增强对短期变化和突变的敏感性；3）时间门控集成模块：动态平衡长期趋势和短期频率感知特征，提高对常规和极端时间模式的适应性。

Result: 在具有极端模式的真实世界水文数据集上的实验表明，M²FMoE在不需极端事件标签的情况下，优于最先进的基线方法。

Conclusion: M²FMoE通过多分辨率多视图频率建模有效解决了极端事件预测的挑战，能够同时学习常规和极端模式，在无需极端事件标注的情况下显著提升了预测性能，为具有极端事件的时间序列预测提供了有效的解决方案。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


### [114] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: 该论文研究具有安全约束的马尔可夫决策过程的最优策略学习问题，提出两种基于乐观面对不确定性原则的在线强化学习算法，并分析其有限样本性能和遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，安全约束至关重要，特别是在实际应用中。现有算法在学习阶段往往无法保证安全约束，这限制了强化学习在安全关键领域的应用。因此，需要设计能够在学习阶段以高概率保证安全约束的在线强化学习算法。

Method: 1. 在可达-避免框架下形式化安全约束的MDP问题；2. 提出基于乐观面对不确定性原则的基础算法；3. 在基础算法基础上，提出利用熵正则化的主要算法；4. 对两种算法进行有限样本分析，推导遗憾界。

Result: 1. 证明了两种算法在学习阶段能够以任意高概率保证安全约束；2. 推导了两种算法的遗憾界；3. 证明了熵正则化的引入能够改善遗憾界，并显著控制基于OFU的安全RL算法固有的幕间变异性。

Conclusion: 该研究成功设计了能够在学习阶段保证安全约束的在线强化学习算法。熵正则化的引入不仅改善了算法的遗憾性能，还显著降低了幕间变异性，为安全关键领域的强化学习应用提供了理论基础和实用算法。

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [115] [TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations](https://arxiv.org/abs/2601.08659)
*Hamid Gadirov,Martijn Westra,Steffen Frey*

Main category: cs.LG

TL;DR: 比较2D和3D卷积自编码器在参数化卡门涡街模拟数据中的异常检测性能，发现3D模型利用时空上下文能更好地检测异常运动模式并减少冗余检测


<details>
  <summary>Details</summary>
Motivation: 高维、时间相关的模拟数据中的异常检测具有挑战性，因为存在复杂的空间和时间动态。研究旨在探索重建基异常检测方法在参数化卡门涡街模拟的集成数据中的应用。

Method: 使用卷积自编码器进行重建基异常检测，比较2D自编码器（处理单个时间帧）和3D自编码器（处理短时间堆栈）。评估体积时间相关数据，分析重建误差与质量空间分布的关系。

Result: 2D模型能识别单个时间步中的局部空间不规则性，而3D模型利用时空上下文检测异常运动模式，并减少跨时间的冗余检测。重建误差受质量空间分布强烈影响，高度集中区域比分散配置产生更大误差。

Conclusion: 在动态模拟中进行鲁棒异常检测时，时间上下文至关重要。3D卷积自编码器通过利用时空信息，在检测异常运动模式和减少冗余检测方面优于2D方法。

Abstract: Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.

</details>


### [116] [Soft Partition-based KAPI-ELM for Multi-Scale PDEs](https://arxiv.org/abs/2601.08719)
*Vikas Dwivedi,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: 提出KAPI-ELM方法，通过软分区自适应核解决多尺度PDE问题，避免谱偏差和手动调参，在八个基准测试中达到或超过现有PINN和TFC方法的精度，仅需单次线性求解。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息机器学习方法在处理高度振荡、多尺度或奇异摄动PDE时面临谱偏差、反向传播成本高、需要手动调整核或傅里叶频率等问题，需要更高效稳定的解决方案。

Method: 提出软分区核自适应物理信息极限学习机(KAPI-ELM)，使用平滑分区长度联合控制配置中心和核宽度，实现连续粗到细分辨率，无需傅里叶特征、随机采样或硬域界面；采用基于符号距离的加权方法稳定不规则几何上的最小二乘学习。

Result: 在八个基准测试（包括振荡ODE、高频泊松方程、不规则形状域和刚性奇异摄动对流扩散问题）中，该方法达到或超过最先进的PINN和TFC变体的精度，同时仅需单次线性求解。

Conclusion: 软分区核自适应为多尺度PDE提供了一种快速、无架构的方法，在物理信息建模方面具有广泛潜力，虽然目前仅在线性稳态PDE上验证，但展示了良好的扩展前景。

Abstract: Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi

</details>


### [117] [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,Thushari Silva,A. Mahasinghe*

Main category: cs.LG

TL;DR: 该研究提出了一种基于量子玻尔兹曼机（QBM）和经典玻尔兹曼机（CBM）比较的可解释AI框架，通过在二值化降维MNIST数据集上的实验，发现QBM在分类准确率和特征归因可解释性方面均优于CBM。


<details>
  <summary>Details</summary>
Motivation: AI系统在分类任务中表现出色，但缺乏可解释性是一个重大挑战，特别是在医疗和金融等高风险领域。需要开发能够提供决策透明度的可解释AI框架。

Method: 提出基于QBM和CBM比较的可解释AI框架。使用PCA对MNIST数据集进行二值化和降维预处理。QBM采用混合量子-经典电路和强纠缠层，CBM作为经典基线使用对比散度。使用梯度显著性图（QBM）和SHAP（CBM）进行特征归因评估。

Result: QBM在分类准确率上显著优于CBM（83.5% vs. 54%）。QBM的特征归因分布更集中（熵值1.27 vs. 1.39），能更清晰地识别预测背后的关键特征。量子-经典混合模型在准确率和可解释性方面均有提升。

Conclusion: 量子-经典混合模型能够同时提高分类准确率和可解释性，为实现更可信和可解释的AI系统提供了有前景的方向。

Abstract: Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of "active ingredient" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.

</details>


### [118] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出Uniqueness-Aware Reinforcement Learning方法，通过奖励独特的高层解题策略来解决RL训练LLM时的探索崩溃问题，在保持pass@1的同时提升pass@k和策略多样性。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在训练大语言模型进行复杂推理任务时存在探索崩溃问题：策略过早集中于少数主导推理模式，虽然提高了pass@1（单次采样成功率），但限制了rollout层面的多样性和pass@k（k次采样成功率）的提升。

Method: 提出Uniqueness-Aware Reinforcement Learning方法，使用LLM作为评判员对同一问题的不同rollout进行聚类，根据高层解题策略（忽略表面变化）分组，然后按聚类大小的倒数重新加权策略优势，使正确但新颖的策略获得比冗余策略更高的奖励。

Result: 在数学、物理和医学推理基准测试中，该方法在保持pass@1不降低的同时，显著提高了大采样预算下的pass@k，增加了pass@k曲线下面积（AUC@K），并维持了探索性，发现了更多样化的解题策略。

Conclusion: 通过显式奖励独特的高层解题策略，Uniqueness-Aware RL有效解决了RL训练LLM时的探索崩溃问题，实现了在保持单次采样性能的同时提升多样性和多次采样成功率。

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>


### [119] [Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling](https://arxiv.org/abs/2601.08777)
*Yang Cai,Weiqiang Zheng*

Main category: cs.LG

TL;DR: 该论文提出了一个通过测试时缩放实现通用对齐的理论框架，证明了最优收敛率为k/(k+1)，并指出现有对齐方法（如NLHF）由于缺乏输出多样性而无法充分利用测试时缩放的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要服务具有异质且可能冲突偏好的用户，这是个性化和可信AI的核心挑战。现有对齐方法在测试时缩放方面存在不足，无法充分利用多候选响应带来的优势。

Method: 1) 形式化(k,f(k))-鲁棒对齐和渐近通用对齐(U-alignment)概念；2) 证明最优收敛率为f(k)=k/(k+1)；3) 提出对称多玩家对齐游戏框架，证明其对称纳什均衡策略能达到最优对齐；4) 提供自博弈学习动态的理论收敛保证。

Result: 1) 存在单输出策略族，其k样本乘积策略能以f(k)=k/(k+1)的速率实现U-alignment，且这是最优收敛率；2) NLHF等现有方法由于输出多样性不足，在k>1时最多只能达到略高于1/2的胜率；3) 对称多玩家对齐游戏的均衡策略能保持输出多样性并达到最优测试时缩放速率。

Conclusion: 测试时缩放是实现通用对齐的有效途径，但需要保持输出多样性。提出的对称多玩家对齐游戏框架能实现最优收敛率，为个性化AI对齐提供了理论基础和实用方法。

Abstract: Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\to 1$ as $k\to\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\frac{k}{k+1}$, and no method can achieve a faster rate in general.
  We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.

</details>


### [120] [Fast and explainable clustering in the Manhattan and Tanimoto distance](https://arxiv.org/abs/2601.08781)
*Stefan Güttel,Kaustubh Roy*

Main category: cs.LG

TL;DR: CLASSIX聚类算法扩展至多种距离度量（曼哈顿距离、Tanimoto距离），通过数据向量范数排序和三角形不等式优化搜索，在化学指纹基准测试中性能显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 原始CLASSIX算法仅支持欧氏距离，限制了其在需要其他距离度量（如化学指纹分析中的Tanimoto距离）的应用场景。需要扩展算法以支持更广泛的距离度量，同时保持其快速和可解释的特性。

Method: 1. 使用数据向量的适当范数（而非主成分）作为排序标准；2. 结合三角形不等式实现搜索终止；3. 对于Tanimoto距离，采用可证明更尖锐的交集不等式进一步提升性能；4. 支持曼哈顿距离和Tanimoto距离等多种距离度量。

Result: 在真实世界化学指纹基准测试中：1. CLASSIX Tanimoto比Taylor-Butina算法快约30倍；2. 比DBSCAN快约80倍；3. 在两种情况下都计算出了更高质量的聚类结果。

Conclusion: 扩展后的CLASSIX算法成功支持了多种距离度量，在保持算法快速性和可解释性的同时，显著提升了在特定应用领域（如化学指纹分析）的性能表现，证明了其通用性和实用性。

Abstract: The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.

</details>
